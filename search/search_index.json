{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tremor \u00b6 Tremor is an event processing system originally designed for the needs of platform engineering and infrastructure. Tremor has been running in production since October 2018, processes 10 terabytes of data per day, or 10 billion messages per minute and 10 million metrics per second. Tremor achieves this with 10x footprint reduction in bare metal infrastructure and cloud based deployments in 6 ( and counting ) systems at Wayfair, whilst reducing memory usage by 10x and delivering better quality of service under conditions when our network is saturated at peak eCommerce trading lifecycles. Tremor reduces cost, reduces complexity and consolidates and simplifies our operational environment to spark SRE joy, to reduce workload on our NOC and to drop our operating costs. As a secondary benefit, tremor is relatively low latency and relatively high throughput however this is an explicit non-goal of the project. Tremor runs 24x7 365 days per year and is implemented in the Rust programming language. Click for an Architectural overview or Canned History of the project. Other interesting topics are: The tremor-script language The tremor-query language Artefacts namely: Onramps Offramps Codecs Pre- and Postprocessors Operational information about Monitoring Configuration and the Configuration Walkthrough The tremor CLI The tremor API Workshop and various use case examples Development related information Benchmarks A Quickstart Guide Notes about Testing and Debugging This is not an exhaustive list and for the curious it might be worth to explore the docs folder on their own.","title":"Home"},{"location":"#tremor","text":"Tremor is an event processing system originally designed for the needs of platform engineering and infrastructure. Tremor has been running in production since October 2018, processes 10 terabytes of data per day, or 10 billion messages per minute and 10 million metrics per second. Tremor achieves this with 10x footprint reduction in bare metal infrastructure and cloud based deployments in 6 ( and counting ) systems at Wayfair, whilst reducing memory usage by 10x and delivering better quality of service under conditions when our network is saturated at peak eCommerce trading lifecycles. Tremor reduces cost, reduces complexity and consolidates and simplifies our operational environment to spark SRE joy, to reduce workload on our NOC and to drop our operating costs. As a secondary benefit, tremor is relatively low latency and relatively high throughput however this is an explicit non-goal of the project. Tremor runs 24x7 365 days per year and is implemented in the Rust programming language. Click for an Architectural overview or Canned History of the project. Other interesting topics are: The tremor-script language The tremor-query language Artefacts namely: Onramps Offramps Codecs Pre- and Postprocessors Operational information about Monitoring Configuration and the Configuration Walkthrough The tremor CLI The tremor API Workshop and various use case examples Development related information Benchmarks A Quickstart Guide Notes about Testing and Debugging This is not an exhaustive list and for the curious it might be worth to explore the docs folder on their own.","title":"Tremor"},{"location":"CII/","text":"CNCF Core Infrastructure Initiative \u00b6 This document normatively details tremor project best-practices which is founded on the Cloud Native Compute Foundation's Core Infrastructure Initiative's Best Practices. Basics \u00b6 Identification \u00b6 What is the human-readable name of the project? Tremor Event Processing System What is a brief description of the project? Tremor What is the URL for the project (as a whole)? https://www.tremor.rs/ What is the URL for the version control repository (it may be the same as the project URL)? https://github.com/tremor-rs/tremor What programming language(s) are used to implement the project? Rust Erlang Python What is the Common Platform Enumeration (CPE) name for the project (if it has one)? N/A FIXME We should follow the CPE process at NIST -- https://nvd.nist.gov/products/cpe Basic project website content \u00b6 The project website MUST succinctly describe what the software does (what problem does it solve?). Met The project website MUST provide information on how to: obtain, provide feedback (as bug reports or enhancements), and contribute to the software. Met. https://github.com/tremor-rs/tremor-runtime/issues The information on how to contribute MUST explain the contribution process (e.g., are pull requests used?) (URL required) Met. https://docs.tremor.rs/Contributing The information on how to contribute SHOULD include the requirements for acceptable contributions (e.g., a reference to any required coding standard). (URL required) Met. https://docs.tremor.rs/Contributing FLOSS license \u00b6 What license(s) is the project released under? ASL-2.0 The software produced by the project MUST be released as FLOSS. Met. https://github.com/tremor-rs/tremor-runtime/tree/main/LICENSE It is SUGGESTED that any required license(s) for the software produced by the project be approved by the Open Source Initiative (OSI). Met. The ASL-2.0 license is approved by the Open Source Initiative (OSI). The project MUST post the license(s) of its results in a standard location in their source repository. (URL required) Met. The license (ASL-2.0) in in the file LICENSE, see https://github.com/tremor-rs/tremor-runtime/tree/main/LICENSE Documentation \u00b6 The project MUST provide basic documentation for the software produced by the project. Met. https://docs.tremor.rs/ The project MUST provide reference documentation that describes the external interface (both input and output) of the software produced by the project. Met. https://docs.tremor.rs/api/ Other \u00b6 The project sites (website, repository, and download URLs) MUST support HTTPS using TLS. Met. The project MUST have one or more mechanisms for discussion (including proposed changes and issues) that are searchable, allow messages and topics to be addressed by URL, enable new people to participate in some of the discussions, and do not require client-side installation of proprietary software. Met. The project SHOULD provide documentation in English and be able to accept bug reports and comments about code in English. Met. Change Control \u00b6 Public version-controlled source repository \u00b6 The project MUST have a version-controlled source repository that is publicly readable and has a URL. Met. https://github.com/tremor-rs/tremor-runtime.git The project's source repository MUST track what changes were made, who made the changes, and when the changes were made. Met. https://github.com/tremor-rs/tremor-runtime.git To enable collaborative review, the project's source repository MUST include interim versions for review between releases; it MUST NOT include only final releases. Met. https://github.com/tremor-rs/tremor-runtime.git It is SUGGESTED that common distributed version control software be used (e.g., git) for the project's source repository. Met. https://github.com/tremor-rs/tremor-runtime.git Unique version numbering \u00b6 The project results MUST have a unique version identifier for each release intended to be used by users. Met. The project follows Semantic Versioning (SemVer) format It is SUGGESTED that projects identify each release within their version control system. For example, it is SUGGESTED that those using git identify each release using git tags. Met. https://github.com/tremor-rs/tremor-runtime/tree/main/docs/development/process/release.md The project MUST provide, in each release, release notes that are a human-readable summary of major changes in that release to help users determine if they should upgrade and what the upgrade impact will be. The release notes MUST NOT be the raw output of a version control log (e.g., the \"git log\" command results are not release notes). Projects whose results are not intended for reuse in multiple locations (such as the software for a single website or service) AND employ continuous delivery MAY select \"N/A\". (URL required) Met. https://github.com/tremor-rs/tremor-runtime/tree/main/CHANGELOG.md The release notes MUST identify every publicly known vulnerability with a CVE assignment or similar that is fixed in each new release, unless users typically cannot practically update the software themselves. If there are no release notes or there have been no publicly known vulnerabilities, choose \"not applicable\" (N/A). Met. Reporting \u00b6 Bug-reporting process \u00b6 The project MUST provide a process for users to submit bug reports (e.g., using an issue tracker or a mailing list). (URL required) Met. https://docs.tremor.rs/Contributing The project SHOULD use an issue tracker for tracking individual issues. Met. https://github.com/tremor-rs/tremor-runtime/issues The project MUST acknowledge a majority of bug reports submitted in the last 2-12 months (inclusive); the response need not include a fix. Met. The project SHOULD respond to a majority (>50%) of enhancement requests in the last 2-12 months (inclusive). Met. It is up to contributors to submit enhancement requests via the RFC process. https://github.com/tremor-rs/tremor-rfcs. The project MUST have a publicly available archive for reports and responses for later searching. Met. All requests, reports and responses are gated via the issue tracking system. https://github.com/tremor-rs/tremor-runtime/issues Vulnerability report process \u00b6 The project MUST publish the process for reporting vulnerabilities on the project site. (URL required) Met. https://docs.tremor.rs/Security If private vulnerability reports are supported, the project MUST include how to send the information in a way that is kept private. (URL required) Met. https://docs.tremor.rs/Security The project's initial response time for any vulnerability report received in the last 6 months MUST be less than or equal to 14 days. Met. Within 48 hours. Quality \u00b6 Working build system? \u00b6 If the software produced by the project requires building for use, the project MUST provide a working build system that can automatically rebuild the software from source code. Met. It is SUGGESTED that common tools be used for building the software. Met. The project SHOULD be buildable using only FLOSS tools. Met. The project primarily uses FLOSS tooling with the exception of EQC which is not required to build the project. Automated test suite \u00b6 The project MUST use at least one automated test suite that is publicly released as FLOSS (this test suite may be maintained as a separate FLOSS project). Met. https://github.com/tremor-rs/tremor-runtime/actions A test suite SHOULD be invocable in a standard way for that language. Met. https://docs.tremor.rs/development/testing It is SUGGESTED that the test suite cover most (or ideally all) the code branches, input fields, and functionality. Met. The project uses a combination of unit, functional, benchmark, integration and specialized ( EQC ) tests It is SUGGESTED that the project implement continuous integration (where new or changed code is frequently integrated into a central code repository and automated tests are run on the result). Met. https://github.com/tremor-rs/tremor-runtime/actions and TBD-eqc-runner-public-url New functionality testing \u00b6 The project MUST have a general policy (formal or not) that as major new functionality is added to the software produced by the project, tests of that functionality should be added to an automated test suite. Met. https://github.com/tremor-rs/tremor-rfcs The project MUST have evidence that the test policy for adding tests has been adhered to in the most recent major changes to the software produced by the project. Met. It is SUGGESTED that this policy on adding tests (see test policy) be documented in the instructions for change proposals. Met. Warning flags \u00b6 The project MUST enable one or more compiler warning flags, a \"safe\" language mode, or use a separate \"linter\" tool to look for code quality errors or common simple mistakes, if there is at least one FLOSS tool that can implement this criterion in the selected language. Met. The project enables all warnings and enforces strict / pedantic checks on code style, format. These are constraints and limitations are enforced by the build and continuous integration systems. The project MUST address warnings. Met. It is SUGGESTED that projects be maximally strict with warnings in the software produced by the project, where practical. Met. Security \u00b6 The project MUST have at least one primary developer who knows how to design secure software. (See \u2018details\u2019 for the exact requirements.) Met. At least one of the project's primary developers MUST know of common kinds of errors that lead to vulnerabilities in this kind of software, as well as at least one method to counter or mitigate each of them. Met. Use basic good cryptographic practices \u00b6 The software produced by the project MUST use, by default, only cryptographic protocols and algorithms that are publicly published and reviewed by experts (if cryptographic protocols and algorithms are used). Met. The project does not provide bespoke cryptographic protocols, algorithms or methods of its own. The project calls on specifically designed externally provided cryptographic methods. If the software produced by the project is an application or library, and its primary purpose is not to implement cryptography, then it SHOULD only call on software specifically designed to implement cryptographic functions; it SHOULD NOT re-implement its own. Met. The project does not provide bespoke cryptographic protocols, algorithms or methods of its own. The project calls o specifically designed externally provided cryptographic methods. All functionality in the software produced by the project that depends on cryptography MUST be implementable using FLOSS. Met. The security mechanisms within the software produced by the project MUST use default keylengths that at least meet the NIST minimum requirements through the year 2030 (as stated in 2012). It MUST be possible to configure the software so that smaller keylengths are completely disabled. Met. The default security mechanisms within the software produced by the project MUST NOT depend on broken cryptographic algorithms (e.g., MD4, MD5, single DES, RC4, Dual_EC_DRBG), or use cipher modes that are inappropriate to the context, unless they are necessary to implement an interoperable protocol (where the protocol implemented is the most recent version of that standard broadly supported by the network ecosystem, that ecosystem requires the use of such an algorithm or mode, and that ecosystem does not offer any more secure alternative). The documentation MUST describe any relevant security risks and any known mitigations if these broken algorithms or modes are necessary for an interoperable protocol. Met. The security mechanisms within the software produced by the project SHOULD implement perfect forward secrecy for key agreement protocols so a session key derived from a set of long-term keys cannot be compromised if one of the long-term keys is compromised in the future. Met. If the software produced by the project causes the storing of passwords for authentication of external users, the passwords MUST be stored as iterated hashes with a per-user salt by using a key stretching (iterated) algorithm (e.g., PBKDF2, Bcrypt or Scrypt). Met. The security mechanisms within the software produced by the project MUST generate all cryptographic keys and nonces using a cryptographically secure random number generator, and MUST NOT do so using generators that are cryptographically insecure. Secured delivery against min-in-the-middle (MITM) attacks \u00b6 The project MUST use a delivery mechanism that counters MITM attacks. Using https or ssh+scp is acceptable. Met. A cryptographic hash (e.g., a sha1sum) MUST NOT be retrieved over http and used without checking for a cryptographic signature. Met. Publicly known vulnerabilities fixed \u00b6 There MUST be no unpatched vulnerabilities of medium or higher severity that have been publicly known for more than 60 days. Met. Projects SHOULD fix all critical vulnerabilities rapidly after they are reported. Met. Other security issues \u00b6 The public repositories MUST NOT leak a valid private credential (e.g., a working password or private key) that is intended to limit public access. Met. Analysis \u00b6 Static code analysis \u00b6 At least one static code analysis tool (beyond compiler warnings and \"safe\" language modes) MUST be applied to any proposed major production release of the software before its release, if there is at least one FLOSS tool that implements this criterion in the selected language. Met. It is SUGGESTED that at least one of the static analysis tools used for the static_analysis criterion include rules or approaches to look for common vulnerabilities in the analyzed language or environment. Met. All medium and higher severity exploitable vulnerabilities discovered with static code analysis MUST be fixed in a timely way after they are confirmed. Met. It is SUGGESTED that static source code analysis occur on every commit or at least daily. Met. Dynamic code analysis \u00b6 It is SUGGESTED that at least one dynamic analysis tool be applied to any proposed major production release of the software before its release. Met. It is SUGGESTED that if the software produced by the project includes software written using a memory-unsafe language (e.g., C or C++), then at least one dynamic tool (e.g., a fuzzer or web application scanner) be routinely used in combination with a mechanism to detect memory safety problems such as buffer overwrites. If the project does not produce software written in a memory-unsafe language, choose \"not applicable\" (N/A). Met. It is SUGGESTED that the software produced by the project include many run-time assertions that are checked during dynamic analysis. Met. All medium and higher severity exploitable vulnerabilities discovered with dynamic code analysis MUST be fixed in a timely way after they are confirmed. Met.","title":"Core Infrastructure Initiative"},{"location":"CII/#cncf-core-infrastructure-initiative","text":"This document normatively details tremor project best-practices which is founded on the Cloud Native Compute Foundation's Core Infrastructure Initiative's Best Practices.","title":"CNCF Core Infrastructure Initiative"},{"location":"CII/#basics","text":"","title":"Basics"},{"location":"CII/#identification","text":"What is the human-readable name of the project? Tremor Event Processing System What is a brief description of the project? Tremor What is the URL for the project (as a whole)? https://www.tremor.rs/ What is the URL for the version control repository (it may be the same as the project URL)? https://github.com/tremor-rs/tremor What programming language(s) are used to implement the project? Rust Erlang Python What is the Common Platform Enumeration (CPE) name for the project (if it has one)? N/A FIXME We should follow the CPE process at NIST -- https://nvd.nist.gov/products/cpe","title":"Identification"},{"location":"CII/#basic-project-website-content","text":"The project website MUST succinctly describe what the software does (what problem does it solve?). Met The project website MUST provide information on how to: obtain, provide feedback (as bug reports or enhancements), and contribute to the software. Met. https://github.com/tremor-rs/tremor-runtime/issues The information on how to contribute MUST explain the contribution process (e.g., are pull requests used?) (URL required) Met. https://docs.tremor.rs/Contributing The information on how to contribute SHOULD include the requirements for acceptable contributions (e.g., a reference to any required coding standard). (URL required) Met. https://docs.tremor.rs/Contributing","title":"Basic project website content"},{"location":"CII/#floss-license","text":"What license(s) is the project released under? ASL-2.0 The software produced by the project MUST be released as FLOSS. Met. https://github.com/tremor-rs/tremor-runtime/tree/main/LICENSE It is SUGGESTED that any required license(s) for the software produced by the project be approved by the Open Source Initiative (OSI). Met. The ASL-2.0 license is approved by the Open Source Initiative (OSI). The project MUST post the license(s) of its results in a standard location in their source repository. (URL required) Met. The license (ASL-2.0) in in the file LICENSE, see https://github.com/tremor-rs/tremor-runtime/tree/main/LICENSE","title":"FLOSS license"},{"location":"CII/#documentation","text":"The project MUST provide basic documentation for the software produced by the project. Met. https://docs.tremor.rs/ The project MUST provide reference documentation that describes the external interface (both input and output) of the software produced by the project. Met. https://docs.tremor.rs/api/","title":"Documentation"},{"location":"CII/#other","text":"The project sites (website, repository, and download URLs) MUST support HTTPS using TLS. Met. The project MUST have one or more mechanisms for discussion (including proposed changes and issues) that are searchable, allow messages and topics to be addressed by URL, enable new people to participate in some of the discussions, and do not require client-side installation of proprietary software. Met. The project SHOULD provide documentation in English and be able to accept bug reports and comments about code in English. Met.","title":"Other"},{"location":"CII/#change-control","text":"","title":"Change Control"},{"location":"CII/#public-version-controlled-source-repository","text":"The project MUST have a version-controlled source repository that is publicly readable and has a URL. Met. https://github.com/tremor-rs/tremor-runtime.git The project's source repository MUST track what changes were made, who made the changes, and when the changes were made. Met. https://github.com/tremor-rs/tremor-runtime.git To enable collaborative review, the project's source repository MUST include interim versions for review between releases; it MUST NOT include only final releases. Met. https://github.com/tremor-rs/tremor-runtime.git It is SUGGESTED that common distributed version control software be used (e.g., git) for the project's source repository. Met. https://github.com/tremor-rs/tremor-runtime.git","title":"Public version-controlled source repository"},{"location":"CII/#unique-version-numbering","text":"The project results MUST have a unique version identifier for each release intended to be used by users. Met. The project follows Semantic Versioning (SemVer) format It is SUGGESTED that projects identify each release within their version control system. For example, it is SUGGESTED that those using git identify each release using git tags. Met. https://github.com/tremor-rs/tremor-runtime/tree/main/docs/development/process/release.md The project MUST provide, in each release, release notes that are a human-readable summary of major changes in that release to help users determine if they should upgrade and what the upgrade impact will be. The release notes MUST NOT be the raw output of a version control log (e.g., the \"git log\" command results are not release notes). Projects whose results are not intended for reuse in multiple locations (such as the software for a single website or service) AND employ continuous delivery MAY select \"N/A\". (URL required) Met. https://github.com/tremor-rs/tremor-runtime/tree/main/CHANGELOG.md The release notes MUST identify every publicly known vulnerability with a CVE assignment or similar that is fixed in each new release, unless users typically cannot practically update the software themselves. If there are no release notes or there have been no publicly known vulnerabilities, choose \"not applicable\" (N/A). Met.","title":"Unique version numbering"},{"location":"CII/#reporting","text":"","title":"Reporting"},{"location":"CII/#bug-reporting-process","text":"The project MUST provide a process for users to submit bug reports (e.g., using an issue tracker or a mailing list). (URL required) Met. https://docs.tremor.rs/Contributing The project SHOULD use an issue tracker for tracking individual issues. Met. https://github.com/tremor-rs/tremor-runtime/issues The project MUST acknowledge a majority of bug reports submitted in the last 2-12 months (inclusive); the response need not include a fix. Met. The project SHOULD respond to a majority (>50%) of enhancement requests in the last 2-12 months (inclusive). Met. It is up to contributors to submit enhancement requests via the RFC process. https://github.com/tremor-rs/tremor-rfcs. The project MUST have a publicly available archive for reports and responses for later searching. Met. All requests, reports and responses are gated via the issue tracking system. https://github.com/tremor-rs/tremor-runtime/issues","title":"Bug-reporting process"},{"location":"CII/#vulnerability-report-process","text":"The project MUST publish the process for reporting vulnerabilities on the project site. (URL required) Met. https://docs.tremor.rs/Security If private vulnerability reports are supported, the project MUST include how to send the information in a way that is kept private. (URL required) Met. https://docs.tremor.rs/Security The project's initial response time for any vulnerability report received in the last 6 months MUST be less than or equal to 14 days. Met. Within 48 hours.","title":"Vulnerability report process"},{"location":"CII/#quality","text":"","title":"Quality"},{"location":"CII/#working-build-system","text":"If the software produced by the project requires building for use, the project MUST provide a working build system that can automatically rebuild the software from source code. Met. It is SUGGESTED that common tools be used for building the software. Met. The project SHOULD be buildable using only FLOSS tools. Met. The project primarily uses FLOSS tooling with the exception of EQC which is not required to build the project.","title":"Working build system?"},{"location":"CII/#automated-test-suite","text":"The project MUST use at least one automated test suite that is publicly released as FLOSS (this test suite may be maintained as a separate FLOSS project). Met. https://github.com/tremor-rs/tremor-runtime/actions A test suite SHOULD be invocable in a standard way for that language. Met. https://docs.tremor.rs/development/testing It is SUGGESTED that the test suite cover most (or ideally all) the code branches, input fields, and functionality. Met. The project uses a combination of unit, functional, benchmark, integration and specialized ( EQC ) tests It is SUGGESTED that the project implement continuous integration (where new or changed code is frequently integrated into a central code repository and automated tests are run on the result). Met. https://github.com/tremor-rs/tremor-runtime/actions and TBD-eqc-runner-public-url","title":"Automated test suite"},{"location":"CII/#new-functionality-testing","text":"The project MUST have a general policy (formal or not) that as major new functionality is added to the software produced by the project, tests of that functionality should be added to an automated test suite. Met. https://github.com/tremor-rs/tremor-rfcs The project MUST have evidence that the test policy for adding tests has been adhered to in the most recent major changes to the software produced by the project. Met. It is SUGGESTED that this policy on adding tests (see test policy) be documented in the instructions for change proposals. Met.","title":"New functionality testing"},{"location":"CII/#warning-flags","text":"The project MUST enable one or more compiler warning flags, a \"safe\" language mode, or use a separate \"linter\" tool to look for code quality errors or common simple mistakes, if there is at least one FLOSS tool that can implement this criterion in the selected language. Met. The project enables all warnings and enforces strict / pedantic checks on code style, format. These are constraints and limitations are enforced by the build and continuous integration systems. The project MUST address warnings. Met. It is SUGGESTED that projects be maximally strict with warnings in the software produced by the project, where practical. Met.","title":"Warning flags"},{"location":"CII/#security","text":"The project MUST have at least one primary developer who knows how to design secure software. (See \u2018details\u2019 for the exact requirements.) Met. At least one of the project's primary developers MUST know of common kinds of errors that lead to vulnerabilities in this kind of software, as well as at least one method to counter or mitigate each of them. Met.","title":"Security"},{"location":"CII/#use-basic-good-cryptographic-practices","text":"The software produced by the project MUST use, by default, only cryptographic protocols and algorithms that are publicly published and reviewed by experts (if cryptographic protocols and algorithms are used). Met. The project does not provide bespoke cryptographic protocols, algorithms or methods of its own. The project calls on specifically designed externally provided cryptographic methods. If the software produced by the project is an application or library, and its primary purpose is not to implement cryptography, then it SHOULD only call on software specifically designed to implement cryptographic functions; it SHOULD NOT re-implement its own. Met. The project does not provide bespoke cryptographic protocols, algorithms or methods of its own. The project calls o specifically designed externally provided cryptographic methods. All functionality in the software produced by the project that depends on cryptography MUST be implementable using FLOSS. Met. The security mechanisms within the software produced by the project MUST use default keylengths that at least meet the NIST minimum requirements through the year 2030 (as stated in 2012). It MUST be possible to configure the software so that smaller keylengths are completely disabled. Met. The default security mechanisms within the software produced by the project MUST NOT depend on broken cryptographic algorithms (e.g., MD4, MD5, single DES, RC4, Dual_EC_DRBG), or use cipher modes that are inappropriate to the context, unless they are necessary to implement an interoperable protocol (where the protocol implemented is the most recent version of that standard broadly supported by the network ecosystem, that ecosystem requires the use of such an algorithm or mode, and that ecosystem does not offer any more secure alternative). The documentation MUST describe any relevant security risks and any known mitigations if these broken algorithms or modes are necessary for an interoperable protocol. Met. The security mechanisms within the software produced by the project SHOULD implement perfect forward secrecy for key agreement protocols so a session key derived from a set of long-term keys cannot be compromised if one of the long-term keys is compromised in the future. Met. If the software produced by the project causes the storing of passwords for authentication of external users, the passwords MUST be stored as iterated hashes with a per-user salt by using a key stretching (iterated) algorithm (e.g., PBKDF2, Bcrypt or Scrypt). Met. The security mechanisms within the software produced by the project MUST generate all cryptographic keys and nonces using a cryptographically secure random number generator, and MUST NOT do so using generators that are cryptographically insecure.","title":"Use basic good cryptographic practices"},{"location":"CII/#secured-delivery-against-min-in-the-middle-mitm-attacks","text":"The project MUST use a delivery mechanism that counters MITM attacks. Using https or ssh+scp is acceptable. Met. A cryptographic hash (e.g., a sha1sum) MUST NOT be retrieved over http and used without checking for a cryptographic signature. Met.","title":"Secured delivery against min-in-the-middle (MITM) attacks"},{"location":"CII/#publicly-known-vulnerabilities-fixed","text":"There MUST be no unpatched vulnerabilities of medium or higher severity that have been publicly known for more than 60 days. Met. Projects SHOULD fix all critical vulnerabilities rapidly after they are reported. Met.","title":"Publicly known vulnerabilities fixed"},{"location":"CII/#other-security-issues","text":"The public repositories MUST NOT leak a valid private credential (e.g., a working password or private key) that is intended to limit public access. Met.","title":"Other security issues"},{"location":"CII/#analysis","text":"","title":"Analysis"},{"location":"CII/#static-code-analysis","text":"At least one static code analysis tool (beyond compiler warnings and \"safe\" language modes) MUST be applied to any proposed major production release of the software before its release, if there is at least one FLOSS tool that implements this criterion in the selected language. Met. It is SUGGESTED that at least one of the static analysis tools used for the static_analysis criterion include rules or approaches to look for common vulnerabilities in the analyzed language or environment. Met. All medium and higher severity exploitable vulnerabilities discovered with static code analysis MUST be fixed in a timely way after they are confirmed. Met. It is SUGGESTED that static source code analysis occur on every commit or at least daily. Met.","title":"Static code analysis"},{"location":"CII/#dynamic-code-analysis","text":"It is SUGGESTED that at least one dynamic analysis tool be applied to any proposed major production release of the software before its release. Met. It is SUGGESTED that if the software produced by the project includes software written using a memory-unsafe language (e.g., C or C++), then at least one dynamic tool (e.g., a fuzzer or web application scanner) be routinely used in combination with a mechanism to detect memory safety problems such as buffer overwrites. If the project does not produce software written in a memory-unsafe language, choose \"not applicable\" (N/A). Met. It is SUGGESTED that the software produced by the project include many run-time assertions that are checked during dynamic analysis. Met. All medium and higher severity exploitable vulnerabilities discovered with dynamic code analysis MUST be fixed in a timely way after they are confirmed. Met.","title":"Dynamic code analysis"},{"location":"CodeOfConduct/","text":"The Tremor Code of Conduct \u00b6 A version of this document can be found online . Conduct \u00b6 Contact : opensource@wayfair.com We are committed to providing a friendly, safe and welcoming environment for all, regardless of level of experience, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, nationality, or other similar characteristic. On tremor-chat, please avoid using overtly sexual nicknames or other nicknames that might detract from a friendly, safe and welcoming environment for all. Please be kind and courteous. There's no need to be mean or rude. Respect that people have differences of opinion and that every design or implementation choice carries a trade-off and numerous costs. There is seldom a right answer. Please keep unstructured critique to a minimum. If you have solid ideas you want to experiment with, make a fork and see how it works. We will exclude you from interaction if you insult, demean or harass anyone. That is not welcome behavior. We interpret the term \"harassment\" as including the definition in the Citizen Code of Conduct ; if you have any lack of clarity about what might be included in that concept, please read their definition. In particular, we don't tolerate behavior that excludes people in socially marginalized groups. Private harassment is also unacceptable. No matter who you are, if you feel you have been or are being harassed or made uncomfortable by a community member, please contact one of the channel ops or any of the Tremor moderation team immediately. Whether you're a regular contributor or a newcomer, we care about making this community a safe place for you and we've got your back. Likewise any spamming, trolling, flaming, baiting or other attention-stealing behavior is not welcome. Moderation \u00b6 These are the policies for upholding our community's standards of conduct. If you feel that a thread needs moderation, please contact the Tremor moderation team . Remarks that violate the Tremor standards of conduct, including hateful, hurtful, oppressive, or exclusionary remarks, are not allowed. (Cursing is allowed, but never targeting another user, and never in a hateful manner.) Remarks that moderators find inappropriate, whether listed in the code of conduct or not, are also not allowed. Moderators will first respond to such remarks with a warning. If the warning is unheeded, the user will be \"kicked,\" i.e., kicked out of the communication channel to cool off. If the user comes back and continues to make trouble, they will be banned, i.e., indefinitely excluded. Moderators may choose at their discretion to un-ban the user if it was a first offense and they offer the offended party a genuine apology. If a moderator bans someone and you think it was unjustified, please take it up with that moderator, or with a different moderator, in private . Complaints about bans in-channel are not allowed. Moderators are held to a higher standard than other community members. If a moderator creates an inappropriate situation, they should expect less leeway than others. In the Tremor community we strive to go the extra step to look out for each other. Don't just aim to be technically unimpeachable, try to be your best self. In particular, avoid flirting with offensive or sensitive issues, particularly if they're off-topic; this all too often leads to unnecessary fights, hurt feelings, and damaged trust; worse, it can drive people away from the community entirely. And if someone takes issue with something you said or did, resist the urge to be defensive. Just stop doing what it was they complained about and apologize. Even if you feel you were misinterpreted or unfairly accused, chances are good there was something you could've communicated better \u2014 remember that it's your responsibility to make your fellow tremolos comfortable. Everyone wants to get along and we are all here first and foremost because we want to talk about cool technology. You will find that people will be eager to assume good intent and forgive as long as you earn their trust. The enforcement policies listed above apply to all official Tremor venues; including official tremor-chat channels (#tremor, #tremor-*, on the CNCF slack, and every channel hosted in the tremor community); GitHub repositories under tremor-rs; and all forums under tremor.rs. For other projects adopting the Tremor Code of Conduct, please contact the maintainers of those projects for enforcement. If you wish to use this code of conduct for your own project, consider explicitly mentioning your moderation policy or making a copy with your own moderation policy so as to avoid confusion. Adapted from the Node.js Policy on Trolling as well as the Contributor Covenant v1.3.0 . Origins \u00b6 This code of conduct is derived from the Rust Language Community code of conduct that can be found here","title":"Code of Conduct"},{"location":"CodeOfConduct/#the-tremor-code-of-conduct","text":"A version of this document can be found online .","title":"The Tremor Code of Conduct"},{"location":"CodeOfConduct/#conduct","text":"Contact : opensource@wayfair.com We are committed to providing a friendly, safe and welcoming environment for all, regardless of level of experience, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion, nationality, or other similar characteristic. On tremor-chat, please avoid using overtly sexual nicknames or other nicknames that might detract from a friendly, safe and welcoming environment for all. Please be kind and courteous. There's no need to be mean or rude. Respect that people have differences of opinion and that every design or implementation choice carries a trade-off and numerous costs. There is seldom a right answer. Please keep unstructured critique to a minimum. If you have solid ideas you want to experiment with, make a fork and see how it works. We will exclude you from interaction if you insult, demean or harass anyone. That is not welcome behavior. We interpret the term \"harassment\" as including the definition in the Citizen Code of Conduct ; if you have any lack of clarity about what might be included in that concept, please read their definition. In particular, we don't tolerate behavior that excludes people in socially marginalized groups. Private harassment is also unacceptable. No matter who you are, if you feel you have been or are being harassed or made uncomfortable by a community member, please contact one of the channel ops or any of the Tremor moderation team immediately. Whether you're a regular contributor or a newcomer, we care about making this community a safe place for you and we've got your back. Likewise any spamming, trolling, flaming, baiting or other attention-stealing behavior is not welcome.","title":"Conduct"},{"location":"CodeOfConduct/#moderation","text":"These are the policies for upholding our community's standards of conduct. If you feel that a thread needs moderation, please contact the Tremor moderation team . Remarks that violate the Tremor standards of conduct, including hateful, hurtful, oppressive, or exclusionary remarks, are not allowed. (Cursing is allowed, but never targeting another user, and never in a hateful manner.) Remarks that moderators find inappropriate, whether listed in the code of conduct or not, are also not allowed. Moderators will first respond to such remarks with a warning. If the warning is unheeded, the user will be \"kicked,\" i.e., kicked out of the communication channel to cool off. If the user comes back and continues to make trouble, they will be banned, i.e., indefinitely excluded. Moderators may choose at their discretion to un-ban the user if it was a first offense and they offer the offended party a genuine apology. If a moderator bans someone and you think it was unjustified, please take it up with that moderator, or with a different moderator, in private . Complaints about bans in-channel are not allowed. Moderators are held to a higher standard than other community members. If a moderator creates an inappropriate situation, they should expect less leeway than others. In the Tremor community we strive to go the extra step to look out for each other. Don't just aim to be technically unimpeachable, try to be your best self. In particular, avoid flirting with offensive or sensitive issues, particularly if they're off-topic; this all too often leads to unnecessary fights, hurt feelings, and damaged trust; worse, it can drive people away from the community entirely. And if someone takes issue with something you said or did, resist the urge to be defensive. Just stop doing what it was they complained about and apologize. Even if you feel you were misinterpreted or unfairly accused, chances are good there was something you could've communicated better \u2014 remember that it's your responsibility to make your fellow tremolos comfortable. Everyone wants to get along and we are all here first and foremost because we want to talk about cool technology. You will find that people will be eager to assume good intent and forgive as long as you earn their trust. The enforcement policies listed above apply to all official Tremor venues; including official tremor-chat channels (#tremor, #tremor-*, on the CNCF slack, and every channel hosted in the tremor community); GitHub repositories under tremor-rs; and all forums under tremor.rs. For other projects adopting the Tremor Code of Conduct, please contact the maintainers of those projects for enforcement. If you wish to use this code of conduct for your own project, consider explicitly mentioning your moderation policy or making a copy with your own moderation policy so as to avoid confusion. Adapted from the Node.js Policy on Trolling as well as the Contributor Covenant v1.3.0 .","title":"Moderation"},{"location":"CodeOfConduct/#origins","text":"This code of conduct is derived from the Rust Language Community code of conduct that can be found here","title":"Origins"},{"location":"ConstraintsLimitations/","text":"Constraints and Limitations \u00b6 This section lists known limitations in the current version of tremor, these limitations are not oversights but tradeoffs driven by the use cases we focused on. If any of those limitations are prohibitive for you please reach out and we can discuss if tradeoffs can be adjusted The batch operator only flushes based on timeout when a new message arrival. Metrics are collected every 10 seconds, but again only when a message arrives at the pipeline. This means if there are no messages (and no new updates) we don't generate metrics. The metric output is currently only handling either InfluxDB line protocol or JSON encoding. Only incremental backoff is supported by back-pressure. Tremor is very opinionated towards performance and not so much towards exploratory work where it is not yet clear what the requirements are. The internal data representation is limited to what can be represented in JSON. Events that make it through all back-pressure and rate-limiting mechanism within the pipeline but arrive at a still overloaded offramp are dropped without the option diverting. The number of functions is the minimal set to cover current use cases. There is no way to 'create' new events within the pipeline.","title":"Constraints and Limitations"},{"location":"ConstraintsLimitations/#constraints-and-limitations","text":"This section lists known limitations in the current version of tremor, these limitations are not oversights but tradeoffs driven by the use cases we focused on. If any of those limitations are prohibitive for you please reach out and we can discuss if tradeoffs can be adjusted The batch operator only flushes based on timeout when a new message arrival. Metrics are collected every 10 seconds, but again only when a message arrives at the pipeline. This means if there are no messages (and no new updates) we don't generate metrics. The metric output is currently only handling either InfluxDB line protocol or JSON encoding. Only incremental backoff is supported by back-pressure. Tremor is very opinionated towards performance and not so much towards exploratory work where it is not yet clear what the requirements are. The internal data representation is limited to what can be represented in JSON. Events that make it through all back-pressure and rate-limiting mechanism within the pipeline but arrive at a still overloaded offramp are dropped without the option diverting. The number of functions is the minimal set to cover current use cases. There is no way to 'create' new events within the pipeline.","title":"Constraints and Limitations"},{"location":"Contributing/","text":"Contributing to Tremor \u00b6 Thank you for your interest in contributing to the Tremor project! There are many ways to contribute, and we appreciate all of them. Here's links to the primary ways to contribute to the Tremor project as an external contributor: Contributing to Tremor Feature Requests Bug Reports The Build System Pull Requests External Dependencies Writing Documentation Issue Triage Out-of-tree Contributions Tremor Chat If you have questions, please make a query hop on over to Tremor Chat . As a reminder, all contributors are expected to follow our Code of Conduct . If this is your first time contributing, we would like to thank you for spending time on the project! Please reach out directly to any core project member if you would like any guidance or assistance. Feature Requests \u00b6 To request a change to the way that Tremor works, please head over to the RFCs repository and view the README for instructions. Bug Reports \u00b6 While bugs are unfortunate, they're a reality in software. We can't fix what we don't know about, so please report liberally. If you're not sure if something is a bug or not, feel free to file a bug anyway. If you believe reporting your bug publicly represents a security risk to Tremor users, please follow our instructions for reporting security vulnerabilities . If you have the chance, before reporting a bug, please search existing issues , as it's possible that someone else has already reported your error. This doesn't always work, and sometimes it's hard to know what to search for, so consider this extra credit. We won't mind if you accidentally file a duplicate report. Similarly, to help others who encountered the bug find your issue, consider filing an issue with a descriptive title, which contains information that might be unique to it. This can be the language or compiler feature used, the conditions that trigger the bug, or part of the error message if there is any. An example could be: \"impossible case reached\" on match expression in tremor scripting language . To open an issue is as follow this link and filling out the fields. Here's a template that you can use to file a bug, though it's not necessary to use it exactly: <short summary of the bug> I tried this code: <code sample that causes the bug> I expected to see this happen: <explanation> Instead, this happened: <explanation> ## Meta `tremor-script --version`: Backtrace: All three components are important: what you did, what you expected, what happened instead. Please include the output of tremor --version , which includes important information about what platform you're on, what version of Rust you're using, etc. Sometimes, a backtrace is helpful, and so including that is nice. To get a backtrace, set the RUST_BACKTRACE environment variable to a value other than 0 . The easiest way to do this is to invoke tremor like this: $ RUST_BACKTRACE = 1 tremor... The Build System \u00b6 For info on how to configure and build the project, please see the tremor build guide . This guide contains info for contributions to the project and the standard facilities. It also lists some really useful commands to the build system, which could save you a lot of time. Pull Requests \u00b6 Pull requests are the primary mechanism we use to change Tremor. GitHub itself has some great documentation on using the Pull Request feature. We use the \"fork and pull\" model described here , where contributors push changes to their personal fork and create pull requests to bring those changes into the source repository. Please make pull requests against the main branch. Tremor follows a no merge policy, meaning, when you encounter merge conflicts you are expected to always rebase instead of merge. E.g. always use rebase when bringing the latest changes from the main branch to your feature branch. Also, please make sure that fixup commits are squashed into other related commits with meaningful commit messages. GitHub allows closing issues using keywords . This feature should be used to keep the issue tracker tidy. However, it is generally preferred to put the \"closes #123\" text in the PR description rather than the issue commit; particularly during rebasing, citing the issue number in the commit can \"spam\" the issue in question. All pull requests are reviewed by another person. If you want to request that a specific person reviews your pull request, you can add an r? to the pull request description. For example, Darach Ennis usually reviews documentation changes. So if you were to make a documentation change, add r ? @darach to the end of the pull request description. This is entirely optional. After someone has reviewed your pull request, they will leave an annotation on the pull request with an r+ . It will look something like this: r+ Once your merge request is approved it will enter the merge queue Speaking of tests, Rust has a comprehensive test suite. More information about it can be found [here][https://github.com/tremor-rs/tremor-runtime/blob/main/docs/development/testing.md]. External Dependencies \u00b6 Currently building the Tremor project will also build the following external projects: clippy rustfmt Breakage is not allowed in released branches and must be addressed before a PR is merged. Writing Documentation \u00b6 Documentation improvements are very welcome. The source of docs.tremor.rs is located in the tremor docs repo . Documentation pull requests function in the same way as other pull requests. To find documentation-related issues, sort by the doc label . Additionally, contributions to the [tremor-guide] are always welcome. Contributions can be made directly here repo. The issue tracker in that repo is also a great way to find things that need doing. Issue Triage \u00b6 Sometimes, an issue will stay open, even though the bug has been fixed. And sometimes, the original bug may go stale because something has changed in the meantime. It can be helpful to go through older bug reports and make sure that they are still valid. Load up an older issue, double check that it's still true, and leave a comment letting us know if it is or is not. The least recently updated sort is good for finding issues like this. Out-of-tree Contributions \u00b6 There are a number of other ways to contribute to Tremor that don't deal with this repository. Answer questions in the Get Help! channels from the Tremor Chat . Participate in the RFC process . Tremor Chat \u00b6 Join the tremor community discord","title":"Contributing"},{"location":"Contributing/#contributing-to-tremor","text":"Thank you for your interest in contributing to the Tremor project! There are many ways to contribute, and we appreciate all of them. Here's links to the primary ways to contribute to the Tremor project as an external contributor: Contributing to Tremor Feature Requests Bug Reports The Build System Pull Requests External Dependencies Writing Documentation Issue Triage Out-of-tree Contributions Tremor Chat If you have questions, please make a query hop on over to Tremor Chat . As a reminder, all contributors are expected to follow our Code of Conduct . If this is your first time contributing, we would like to thank you for spending time on the project! Please reach out directly to any core project member if you would like any guidance or assistance.","title":"Contributing to Tremor"},{"location":"Contributing/#feature-requests","text":"To request a change to the way that Tremor works, please head over to the RFCs repository and view the README for instructions.","title":"Feature Requests"},{"location":"Contributing/#bug-reports","text":"While bugs are unfortunate, they're a reality in software. We can't fix what we don't know about, so please report liberally. If you're not sure if something is a bug or not, feel free to file a bug anyway. If you believe reporting your bug publicly represents a security risk to Tremor users, please follow our instructions for reporting security vulnerabilities . If you have the chance, before reporting a bug, please search existing issues , as it's possible that someone else has already reported your error. This doesn't always work, and sometimes it's hard to know what to search for, so consider this extra credit. We won't mind if you accidentally file a duplicate report. Similarly, to help others who encountered the bug find your issue, consider filing an issue with a descriptive title, which contains information that might be unique to it. This can be the language or compiler feature used, the conditions that trigger the bug, or part of the error message if there is any. An example could be: \"impossible case reached\" on match expression in tremor scripting language . To open an issue is as follow this link and filling out the fields. Here's a template that you can use to file a bug, though it's not necessary to use it exactly: <short summary of the bug> I tried this code: <code sample that causes the bug> I expected to see this happen: <explanation> Instead, this happened: <explanation> ## Meta `tremor-script --version`: Backtrace: All three components are important: what you did, what you expected, what happened instead. Please include the output of tremor --version , which includes important information about what platform you're on, what version of Rust you're using, etc. Sometimes, a backtrace is helpful, and so including that is nice. To get a backtrace, set the RUST_BACKTRACE environment variable to a value other than 0 . The easiest way to do this is to invoke tremor like this: $ RUST_BACKTRACE = 1 tremor...","title":"Bug Reports"},{"location":"Contributing/#the-build-system","text":"For info on how to configure and build the project, please see the tremor build guide . This guide contains info for contributions to the project and the standard facilities. It also lists some really useful commands to the build system, which could save you a lot of time.","title":"The Build System"},{"location":"Contributing/#pull-requests","text":"Pull requests are the primary mechanism we use to change Tremor. GitHub itself has some great documentation on using the Pull Request feature. We use the \"fork and pull\" model described here , where contributors push changes to their personal fork and create pull requests to bring those changes into the source repository. Please make pull requests against the main branch. Tremor follows a no merge policy, meaning, when you encounter merge conflicts you are expected to always rebase instead of merge. E.g. always use rebase when bringing the latest changes from the main branch to your feature branch. Also, please make sure that fixup commits are squashed into other related commits with meaningful commit messages. GitHub allows closing issues using keywords . This feature should be used to keep the issue tracker tidy. However, it is generally preferred to put the \"closes #123\" text in the PR description rather than the issue commit; particularly during rebasing, citing the issue number in the commit can \"spam\" the issue in question. All pull requests are reviewed by another person. If you want to request that a specific person reviews your pull request, you can add an r? to the pull request description. For example, Darach Ennis usually reviews documentation changes. So if you were to make a documentation change, add r ? @darach to the end of the pull request description. This is entirely optional. After someone has reviewed your pull request, they will leave an annotation on the pull request with an r+ . It will look something like this: r+ Once your merge request is approved it will enter the merge queue Speaking of tests, Rust has a comprehensive test suite. More information about it can be found [here][https://github.com/tremor-rs/tremor-runtime/blob/main/docs/development/testing.md].","title":"Pull Requests"},{"location":"Contributing/#external-dependencies","text":"Currently building the Tremor project will also build the following external projects: clippy rustfmt Breakage is not allowed in released branches and must be addressed before a PR is merged.","title":"External Dependencies"},{"location":"Contributing/#writing-documentation","text":"Documentation improvements are very welcome. The source of docs.tremor.rs is located in the tremor docs repo . Documentation pull requests function in the same way as other pull requests. To find documentation-related issues, sort by the doc label . Additionally, contributions to the [tremor-guide] are always welcome. Contributions can be made directly here repo. The issue tracker in that repo is also a great way to find things that need doing.","title":"Writing Documentation"},{"location":"Contributing/#issue-triage","text":"Sometimes, an issue will stay open, even though the bug has been fixed. And sometimes, the original bug may go stale because something has changed in the meantime. It can be helpful to go through older bug reports and make sure that they are still valid. Load up an older issue, double check that it's still true, and leave a comment letting us know if it is or is not. The least recently updated sort is good for finding issues like this.","title":"Issue Triage"},{"location":"Contributing/#out-of-tree-contributions","text":"There are a number of other ways to contribute to Tremor that don't deal with this repository. Answer questions in the Get Help! channels from the Tremor Chat . Participate in the RFC process .","title":"Out-of-tree Contributions"},{"location":"Contributing/#tremor-chat","text":"Join the tremor community discord","title":"Tremor Chat"},{"location":"FeatureComparison/","text":"FEATURE COMPARISON \u00b6 This section compares the latest stable version of Tremor with other alternative products. Feature Tremor Hindsight Logstash Pipeline Architecture Y Y Y Additional Runtime Not required Not required Requires Java runtime Configurable via a script file Y Y Y Open Source N Y Y Grok support Y N Y JSON Codec Y Y Y Windows support N Y Y Logstash \u00b6 Logstash is an open source data collection engine with real time pipelining capabilities written by Elasticsearch. We compare Tremor with the Logstash 7.0 which is the latest stable version at the time of writing. Hindsight \u00b6 Hindsight is a C based data processing infrastructure based on the Lua sandbox.","title":"FEATURE COMPARISON"},{"location":"FeatureComparison/#feature-comparison","text":"This section compares the latest stable version of Tremor with other alternative products. Feature Tremor Hindsight Logstash Pipeline Architecture Y Y Y Additional Runtime Not required Not required Requires Java runtime Configurable via a script file Y Y Y Open Source N Y Y Grok support Y N Y JSON Codec Y Y Y Windows support N Y Y","title":"FEATURE COMPARISON"},{"location":"FeatureComparison/#logstash","text":"Logstash is an open source data collection engine with real time pipelining capabilities written by Elasticsearch. We compare Tremor with the Logstash 7.0 which is the latest stable version at the time of writing.","title":"Logstash"},{"location":"FeatureComparison/#hindsight","text":"Hindsight is a C based data processing infrastructure based on the Lua sandbox.","title":"Hindsight"},{"location":"Glossary/","text":"Glossary of Terms \u00b6 A set of terms in common or standardised usage by the tremor project and/or team Term Definition Artefact A unit of configuration management in tremor. As and from tremor v0.4 DAG Directed Acyclic Graph - A graph with no cycles and unidirectional edges Pipeline An artefact that describes a graph ( DAG ) of tremor operators Operator A vertex ( node ) in a tremor pipeline graph. Operators perform work in a tremor pipeline graph Source, Onramp An artefact that describes a connector of primarily inbound data available for pipelines to ingest Sink, Offramp An artefact that describes a connector of primarily outbound data produced by pipelines available for egress Peer, Linked Transport An artefact that describes a connector with both inbound and outbound data that can be routed through a pipeline conversationally Link A link is an edge or connection between operators in a pipeline or between pipelines and onramps/offramps Binding A specification of ( set of links ), describing one-or-many interconnections to/from pipelines Mapping A configuration of, ( set of bindings ), and set of key/value replacements that describes how to deploy pipelines and how to interconnect binding specifications and map to running instances of tremor artefacts Repository An in memory cache that tremor uses to store artefacts. Like a git repository for artefacts Registry An in memory cache that tremor uses to store running onramps, offramps and pipelines. Like the DNS registry for running code that can send, receive or process events Publish The act of publishing an artefact or deploying a running instance Find The act of finding an artefact or running instance by id Bind The act of deploying artefacts and making them runnable Publish-Find-Bind An Enterprise Integration Pattern common in Registry/Repository services for Application Server Platforms Deploy The act of publishing a tremor mapping, the side-effect of which MAY be the deployment of onramps, offramps and/or pipelines Undeploy The act of unpublishing a tremor mapping, the side-effect of which MAY be the undeployment of onramps, offramps and/or pipelines Meta Variables Global event metadata variables (prefixed with $ in tremor-script), accessible across pipeline nodes as well as offramps WAL, Write-ahead Log An in-memory or persistent data log used by the guaranteed delivery mechanism CB, Circuit-breaker A mechanism that can react to failure in sources and sinks in a robust and recoverable way GD, Guaranteed delivery A mechanism that guarantees that events that reach a pipeline are processed to completion. Depending on the source/sink this may extend end-to-end End to end GD A configuration of GD whereby the primary sources and sinks for a use case are guaranteed not to lose messages during normal processing conditions LT, Linked transports A mechanism that allows linking of source and sink nature into one ramp artefact","title":"Glossary"},{"location":"Glossary/#glossary-of-terms","text":"A set of terms in common or standardised usage by the tremor project and/or team Term Definition Artefact A unit of configuration management in tremor. As and from tremor v0.4 DAG Directed Acyclic Graph - A graph with no cycles and unidirectional edges Pipeline An artefact that describes a graph ( DAG ) of tremor operators Operator A vertex ( node ) in a tremor pipeline graph. Operators perform work in a tremor pipeline graph Source, Onramp An artefact that describes a connector of primarily inbound data available for pipelines to ingest Sink, Offramp An artefact that describes a connector of primarily outbound data produced by pipelines available for egress Peer, Linked Transport An artefact that describes a connector with both inbound and outbound data that can be routed through a pipeline conversationally Link A link is an edge or connection between operators in a pipeline or between pipelines and onramps/offramps Binding A specification of ( set of links ), describing one-or-many interconnections to/from pipelines Mapping A configuration of, ( set of bindings ), and set of key/value replacements that describes how to deploy pipelines and how to interconnect binding specifications and map to running instances of tremor artefacts Repository An in memory cache that tremor uses to store artefacts. Like a git repository for artefacts Registry An in memory cache that tremor uses to store running onramps, offramps and pipelines. Like the DNS registry for running code that can send, receive or process events Publish The act of publishing an artefact or deploying a running instance Find The act of finding an artefact or running instance by id Bind The act of deploying artefacts and making them runnable Publish-Find-Bind An Enterprise Integration Pattern common in Registry/Repository services for Application Server Platforms Deploy The act of publishing a tremor mapping, the side-effect of which MAY be the deployment of onramps, offramps and/or pipelines Undeploy The act of unpublishing a tremor mapping, the side-effect of which MAY be the undeployment of onramps, offramps and/or pipelines Meta Variables Global event metadata variables (prefixed with $ in tremor-script), accessible across pipeline nodes as well as offramps WAL, Write-ahead Log An in-memory or persistent data log used by the guaranteed delivery mechanism CB, Circuit-breaker A mechanism that can react to failure in sources and sinks in a robust and recoverable way GD, Guaranteed delivery A mechanism that guarantees that events that reach a pipeline are processed to completion. Depending on the source/sink this may extend end-to-end End to end GD A configuration of GD whereby the primary sources and sinks for a use case are guaranteed not to lose messages during normal processing conditions LT, Linked transports A mechanism that allows linking of source and sink nature into one ramp artefact","title":"Glossary of Terms"},{"location":"QA/","text":"1 What should we use as a community collaboration environment? \u00b6 [ ] IRC [ ] Slack [ ] Discord [ ] Zulip [ ] Other _ _ _ _ _ __ 2 \u00b6","title":"QA"},{"location":"QA/#1-what-should-we-use-as-a-community-collaboration-environment","text":"[ ] IRC [ ] Slack [ ] Discord [ ] Zulip [ ] Other _ _ _ _ _ __","title":"1 What should we use as a community collaboration environment?"},{"location":"QA/#2","text":"","title":"2"},{"location":"api/","text":"Document Status \u00b6 Work in Progress Well known API endpoints \u00b6 This document summarises the Tremor REST API Url Description http://localhost:9898/ The default ( development ) endpoint on a local ( development ) host Paths \u00b6 The endpoint paths supported by the Tremor REST API GET /onramp \u00b6 Lists onramps Description: Returns a list of identifiers for each onramp stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_onramps Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set POST /onramp \u00b6 Publish a new onramp to the tremor artefact repository Description: Publishes a new onramp to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an onramp of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_onramp Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/onramp 201 application/yaml #/components/schemas/onramp 409 empty no content GET /onramp/{artefact-id} \u00b6 Finds onramp data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_onramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/onramp_state 200 application/yaml #/components/schemas/onramp_state 404 empty no content DELETE /onramp/{artefact-id} \u00b6 Remove an onramp from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_onramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/onramp 200 application/yaml #/components/schemas/onramp 409 empty no content 404 empty no content GET /offramp \u00b6 Lists oframps Description: Returns a list of identifiers for each offramp stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_offramps Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set POST /offramp \u00b6 Publish a new offramp to the tremor artefact repository Description: Publishes a new offramp to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an arterfact of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_offramp Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/offramp 201 application/yaml #/components/schemas/offramp 409 empty no content GET /offramp/{artefact-id} \u00b6 Get offramp data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_offramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/offramp_state 200 application/yaml #/components/schemas/offramp_state 404 empty no content DELETE /offramp/{artefact-id} \u00b6 Remove artefact from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_offramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/offramp 200 application/yaml #/components/schemas/offramp 409 empty no content 404 empty no content GET /pipeline \u00b6 Lists pipelines Description: Returns a list of identifiers for each pipeline stored in the repository Response data is a trickle source code string. OperationId: find_pipelines Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set POST /pipeline \u00b6 Publish a new pipeline to the tremor artefact repository Description: Publishes a new pipeline to the tremor artefact repository if the artefact id is unique. The request body need to be valid trickle. Returns artefact data, on success. If an pipeline of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_pipeline Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/pipeline 201 application/yaml #/components/schemas/pipeline 201 application/vnd.trickle #/components/schemas/pipeline 409 empty no content GET /pipeline/{artefact-id} \u00b6 Get pipeline data from tremor artefact repository Description: Given a valid pipeline artefact identifier of a pipeline artefact stored in the tremor artefact repository Returns pipeline source code string, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ), but is essentially a trickle source code string. OperationId: get_pipeline_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/pipeline 200 application/yaml #/components/schemas/pipeline 200 application/vnd.trickle #/components/schemas/pipeline 404 empty no content DELETE /pipeline/{artefact-id} \u00b6 Remove pipeline from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_pipeline_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/pipeline 200 application/yaml #/components/schemas/pipeline 200 application/vnd.trickle #/components/schemas/pipeline 409 empty no content 404 empty no content GET /binding \u00b6 Lists bindings Description: Returns a list of identifiers for each binding stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_bindings Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set POST /binding \u00b6 Publish a new binding to the tremor artefact repository Description: Publishes a new binding to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an arterfact of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_binding Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/binding 201 application/yaml #/components/schemas/binding 409 empty no content GET /binding/{artefact-id} \u00b6 Get binding data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_binding_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding_state 200 application/yaml #/components/schemas/binding_state 404 empty no content DELETE /binding/{artefact-id} \u00b6 Remove binding from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_binding_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content GET /binding/{artefact-id}/{instance-id} \u00b6 Get deployed artefact servant data from tremor artefact registry Description: Given a valid identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Returns binding instance data on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_binding_instance_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content POST /binding/{artefact-id}/{instance-id} \u00b6 Publish, deploy and activate a binding Description: Given a valid binding artefact identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Creates new instances of artefacts ( if required ), publishes instances to the tremor instance registry. If instances are onramps, offramps or pipelines new registry values will be created. In the case of onramps and offramps these are deployed after any dependant pipeline instances and then they are interconnected. Returns the binding instance data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: activate-binding Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/binding 201 application/yaml #/components/schemas/binding 404 empty no content DELETE /binding/{artefact-id}/{instance-id} \u00b6 Deactivate and unpublish deployed bindings Description: Given a valid binding artefact identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Deactivates, stops and unpublishes the target instances and any dependant instances that are no longer referenced by the runtime. Returns old instance data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: deactivate-binding Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content GET /version \u00b6 Get's the current version Description: This endpoint returns version information for the current version of tremor. Versioning policy follows Semantic Versioning OperationId: get_version Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/version Schemas \u00b6 JSON Schema for types defioned in the Tremor REST API Schema for type: version \u00b6 Version information { \"description\" : \"Version information\" , \"properties\" : { \"additionalProperties\" : false , \"debug\" : { \"description\" : \"True if this is a debug build\" , \"type\" : \"boolean\" }, \"version\" : { \"description\" : \"The semantic version code\" , \"type\" : \"string\" } }, \"required\" : [ \"version\" ] } Schema for type: registry_set \u00b6 A list of registry artefacts { \"description\" : \"A list of registry artefacts\" , \"items\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"type\" : \"array\" } Schema for type: instance_set \u00b6 A list of artefact instances { \"description\" : \"A list of artefact instances\" , \"items\" : { \"$ref\" : \"#/components/schemas/instance_id\" }, \"type\" : \"array\" } Schema for type: artefact_id \u00b6 No description. { \"pattern\" : \"^[a-z][a-zA-Z_:]*$\" , \"type\" : \"string\" } Schema for type: instance_id \u00b6 No description. { \"$ref\" : \"#/components/schemas/artefact_id\" } Schema for type: port_id \u00b6 No description. { \"$ref\" : \"#/components/schemas/artefact_id\" } Schema for type: artefact \u00b6 No description. { \"oneOf\" : [ { \"$ref\" : \"#/components/schemas/pipeline\" }, { \"$ref\" : \"#/components/schemas/onramp\" }, { \"$ref\" : \"#/components/schemas/offramp\" }, { \"$ref\" : \"#/components/schemas/binding\" } ] } Schema for type: instance \u00b6 No description. { \"oneOf\" : [ { \"$ref\" : \"#/components/schemas/mapping\" } ] } Schema for type: publish_ok \u00b6 Response when a registry publish was succesful { \"description\" : \"Response when a registry publish was succesful\" , \"properties\" : { \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" , \"description\" : \"The id of the pubished artefact\" } }, \"required\" : [ \"id\" ] } Schema for type: pipeline \u00b6 State of an pipeline, expressed as trickle source code. { \"description\" : \"State of an pipeline, expressed as trickle source code.\" , \"type\" : \"string\" } Schema for type: onramp_state \u00b6 State of an onramp, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an onramp, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/onramp\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" } Schema for type: onramp \u00b6 A tremor onramp specification { \"additionalProperties\" : false , \"description\" : \"A tremor onramp specification\" , \"properties\" : { \"codec\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"codec_map\" : { \"$ref\" : \"#/components/schemas/codec_map\" }, \"config\" : { \"description\" : \"A map of key/value pairs used to configure this onramp\" , \"type\" : \"object\" }, \"description\" : { \"description\" : \"Documentation for this type\" , \"type\" : \"string\" }, \"err_required\" : { \"description\" : \"Whether a pipeline needs to be connected to the err port before startup\" , \"type\" : \"boolean\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"linked\" : { \"description\" : \"Whether this offramp is linked or not\" , \"type\" : \"boolean\" }, \"metrics_interval_s\" : { \"description\" : \"interval in which metrics info is published\" , \"minimum\" : 0 , \"type\" : \"integer\" }, \"postprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/postprocessor\" }, \"type\" : \"array\" }, \"preprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/preprocessor\" }, \"type\" : \"array\" }, \"type\" : { \"$ref\" : \"#/components/schemas/onramp_type\" } }, \"required\" : [ \"type\" , \"id\" ], \"type\" : \"object\" } Schema for type: offramp_state \u00b6 State of an offramp, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an offramp, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/offramp\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" } Schema for type: offramp \u00b6 A tremor offramp specification { \"additionalProperties\" : false , \"description\" : \"A tremor offramp specification\" , \"properties\" : { \"codec\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"codec_map\" : { \"$ref\" : \"#/components/schemas/codec_map\" }, \"config\" : { \"description\" : \"A map of key/value pairs used to configure this onramp\" , \"type\" : \"object\" }, \"description\" : { \"description\" : \"Documentation for this type\" , \"type\" : \"string\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"linked\" : { \"description\" : \"Whether this offramp is linked or not\" , \"type\" : \"boolean\" }, \"metrics_interval_s\" : { \"description\" : \"interval in which metrics info is published\" , \"minimum\" : 0 , \"type\" : \"integer\" }, \"postprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/postprocessor\" }, \"type\" : \"array\" }, \"preprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/preprocessor\" }, \"type\" : \"array\" }, \"type\" : { \"$ref\" : \"#/components/schemas/offramp_type\" } }, \"required\" : [ \"type\" , \"id\" ], \"type\" : \"object\" } Schema for type: binding_state \u00b6 State of an binding, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an binding, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/binding\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" } Schema for type: binding \u00b6 A tremor binding specification { \"additionalProperties\" : false , \"description\" : \"A tremor binding specification\" , \"properties\" : { \"description\" : { \"type\" : \"string\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"links\" : { \"$ref\" : \"#/components/schemas/binding_map\" } }, \"required\" : [ \"id\" , \"links\" ], \"type\" : \"object\" } Schema for type: binding_map \u00b6 A map of binding specification links { \"additionalProperties\" : false , \"description\" : \"A map of binding specification links\" , \"patternProperties\" : { \"^(tremor://localhost)?/(onramp|pipeline)/[a-zA-Z][A-Za-z0-9_]*/[a-zA-Z][A-Za-z0-9_]*$\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/binding_dst\" }, \"type\" : \"array\" } }, \"type\" : \"object\" } Schema for type: binding_dst \u00b6 No description. { \"pattern\" : \"^(tremor://localhost)?/(pipeline|offramp)/[a-zA-Z][A-Za-z0-9_]*/[a-zA-Z][A-Za-z0-9_]*$\" , \"type\" : \"string\" } Schema for type: mapping \u00b6 A tremor mapping specification { \"description\" : \"A tremor mapping specification\" , \"type\" : \"object\" } Schema for type: offramp_type \u00b6 supported offramp types { \"description\" : \"supported offramp types\" , \"enum\" : [ \"blackhole\" , \"debug\" , \"elastic\" , \"exit\" , \"file\" , \"kafka\" , \"newrelic\" , \"postgres\" , \"rest\" , \"stderr\" , \"stdout\" , \"tcp\" , \"udp\" , \"ws\" ], \"type\" : \"string\" } Schema for type: onramp_type \u00b6 supported onramp types { \"description\" : \"supported onramp types\" , \"enum\" : [ \"blaster\" , \"crononome\" , \"file\" , \"kafka\" , \"metronome\" , \"postgres\" , \"rest\" , \"tcp\" , \"udp\" , \"ws\" ], \"type\" : \"string\" } Schema for type: codec \u00b6 The data format supported for encoding/decoding to/from tremor types { \"description\" : \"The data format supported for encoding/decoding to/from tremor types\" , \"enum\" : [ \"binflux\" , \"influx\" , \"json\" , \"msgpack\" , \"null\" , \"statsd\" , \"string\" , \"yaml\" ], \"type\" : \"string\" } Schema for type: codec_map \u00b6 A map from mime-type to codec { \"additionalProperties\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"description\" : \"A map from mime-type to codec\" , \"type\" : \"object\" } Schema for type: preprocessor \u00b6 Supported preprocessors { \"description\" : \"Supported preprocessors\" , \"enum\" : [ \"base64\" , \"decompress\" , \"gelf-chunking\" , \"gzip\" , \"length-prefixed\" , \"lines\" , \"lines-null\" , \"lines-pipe\" , \"lines-no-buffer\" , \"lines-cr-no-buffer\" , \"lz4\" , \"remove-empty\" , \"snappy\" , \"xz\" , \"zlib\" ], \"type\" : \"string\" } Schema for type: postprocessor \u00b6 Supported postprocessors { \"description\" : \"Supported postprocessors\" , \"enum\" : [ \"base64\" , \"gelf-chunking\" , \"gzip\" , \"length-prefixed\" , \"lines\" , \"lz4\" , \"snappy\" , \"xz\" , \"zlib\" ], \"type\" : \"string\" }","title":"API"},{"location":"api/#document-status","text":"Work in Progress","title":"Document Status"},{"location":"api/#well-known-api-endpoints","text":"This document summarises the Tremor REST API Url Description http://localhost:9898/ The default ( development ) endpoint on a local ( development ) host","title":"Well known API endpoints"},{"location":"api/#paths","text":"The endpoint paths supported by the Tremor REST API","title":"Paths"},{"location":"api/#get-onramp","text":"Lists onramps Description: Returns a list of identifiers for each onramp stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_onramps Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set","title":"GET /onramp"},{"location":"api/#post-onramp","text":"Publish a new onramp to the tremor artefact repository Description: Publishes a new onramp to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an onramp of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_onramp Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/onramp 201 application/yaml #/components/schemas/onramp 409 empty no content","title":"POST /onramp"},{"location":"api/#get-onrampartefact-id","text":"Finds onramp data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_onramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/onramp_state 200 application/yaml #/components/schemas/onramp_state 404 empty no content","title":"GET /onramp/{artefact-id}"},{"location":"api/#delete-onrampartefact-id","text":"Remove an onramp from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_onramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/onramp 200 application/yaml #/components/schemas/onramp 409 empty no content 404 empty no content","title":"DELETE /onramp/{artefact-id}"},{"location":"api/#get-offramp","text":"Lists oframps Description: Returns a list of identifiers for each offramp stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_offramps Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set","title":"GET /offramp"},{"location":"api/#post-offramp","text":"Publish a new offramp to the tremor artefact repository Description: Publishes a new offramp to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an arterfact of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_offramp Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/offramp 201 application/yaml #/components/schemas/offramp 409 empty no content","title":"POST /offramp"},{"location":"api/#get-offrampartefact-id","text":"Get offramp data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_offramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/offramp_state 200 application/yaml #/components/schemas/offramp_state 404 empty no content","title":"GET /offramp/{artefact-id}"},{"location":"api/#delete-offrampartefact-id","text":"Remove artefact from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_offramp_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/offramp 200 application/yaml #/components/schemas/offramp 409 empty no content 404 empty no content","title":"DELETE /offramp/{artefact-id}"},{"location":"api/#get-pipeline","text":"Lists pipelines Description: Returns a list of identifiers for each pipeline stored in the repository Response data is a trickle source code string. OperationId: find_pipelines Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set","title":"GET /pipeline"},{"location":"api/#post-pipeline","text":"Publish a new pipeline to the tremor artefact repository Description: Publishes a new pipeline to the tremor artefact repository if the artefact id is unique. The request body need to be valid trickle. Returns artefact data, on success. If an pipeline of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_pipeline Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/pipeline 201 application/yaml #/components/schemas/pipeline 201 application/vnd.trickle #/components/schemas/pipeline 409 empty no content","title":"POST /pipeline"},{"location":"api/#get-pipelineartefact-id","text":"Get pipeline data from tremor artefact repository Description: Given a valid pipeline artefact identifier of a pipeline artefact stored in the tremor artefact repository Returns pipeline source code string, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ), but is essentially a trickle source code string. OperationId: get_pipeline_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/pipeline 200 application/yaml #/components/schemas/pipeline 200 application/vnd.trickle #/components/schemas/pipeline 404 empty no content","title":"GET /pipeline/{artefact-id}"},{"location":"api/#delete-pipelineartefact-id","text":"Remove pipeline from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_pipeline_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/pipeline 200 application/yaml #/components/schemas/pipeline 200 application/vnd.trickle #/components/schemas/pipeline 409 empty no content 404 empty no content","title":"DELETE /pipeline/{artefact-id}"},{"location":"api/#get-binding","text":"Lists bindings Description: Returns a list of identifiers for each binding stored in the repository Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: find_bindings Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/registry_set 200 application/yaml #/components/schemas/registry_set","title":"GET /binding"},{"location":"api/#post-binding","text":"Publish a new binding to the tremor artefact repository Description: Publishes a new binding to the tremor artefact repository if the artefact id is unique. Returns artefact data, on success. If an arterfact of the same name already exists, a conflict error is returned. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: publish_binding Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/binding 201 application/yaml #/components/schemas/binding 409 empty no content","title":"POST /binding"},{"location":"api/#get-bindingartefact-id","text":"Get binding data from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_binding_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding_state 200 application/yaml #/components/schemas/binding_state 404 empty no content","title":"GET /binding/{artefact-id}"},{"location":"api/#delete-bindingartefact-id","text":"Remove binding from tremor artefact repository Description: Given a valid artefact identifier of an artefact stored in the tremor artefact repository Returns old artefact data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: delete_binding_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content","title":"DELETE /binding/{artefact-id}"},{"location":"api/#get-bindingartefact-idinstance-id","text":"Get deployed artefact servant data from tremor artefact registry Description: Given a valid identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Returns binding instance data on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: get_binding_instance_by_id Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content","title":"GET /binding/{artefact-id}/{instance-id}"},{"location":"api/#post-bindingartefact-idinstance-id","text":"Publish, deploy and activate a binding Description: Given a valid binding artefact identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Creates new instances of artefacts ( if required ), publishes instances to the tremor instance registry. If instances are onramps, offramps or pipelines new registry values will be created. In the case of onramps and offramps these are deployed after any dependant pipeline instances and then they are interconnected. Returns the binding instance data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: activate-binding Returns: Status Code Content Type Schema Type 201 application/json #/components/schemas/binding 201 application/yaml #/components/schemas/binding 404 empty no content","title":"POST /binding/{artefact-id}/{instance-id}"},{"location":"api/#delete-bindingartefact-idinstance-id","text":"Deactivate and unpublish deployed bindings Description: Given a valid binding artefact identifier of a binding artefact stored in the tremor artefact repository Given a valid binding instance identifier for a deployed and running instance of the binding deployed and accesible via the tremor instance registry Deactivates, stops and unpublishes the target instances and any dependant instances that are no longer referenced by the runtime. Returns old instance data, on success. Response data may be either JSON or YAML formatted ( defaults to JSON ). OperationId: deactivate-binding Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/binding 200 application/yaml #/components/schemas/binding 404 empty no content","title":"DELETE /binding/{artefact-id}/{instance-id}"},{"location":"api/#get-version","text":"Get's the current version Description: This endpoint returns version information for the current version of tremor. Versioning policy follows Semantic Versioning OperationId: get_version Returns: Status Code Content Type Schema Type 200 application/json #/components/schemas/version","title":"GET /version"},{"location":"api/#schemas","text":"JSON Schema for types defioned in the Tremor REST API","title":"Schemas"},{"location":"api/#schema-for-type-version","text":"Version information { \"description\" : \"Version information\" , \"properties\" : { \"additionalProperties\" : false , \"debug\" : { \"description\" : \"True if this is a debug build\" , \"type\" : \"boolean\" }, \"version\" : { \"description\" : \"The semantic version code\" , \"type\" : \"string\" } }, \"required\" : [ \"version\" ] }","title":"Schema for type: version"},{"location":"api/#schema-for-type-registry_set","text":"A list of registry artefacts { \"description\" : \"A list of registry artefacts\" , \"items\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"type\" : \"array\" }","title":"Schema for type: registry_set"},{"location":"api/#schema-for-type-instance_set","text":"A list of artefact instances { \"description\" : \"A list of artefact instances\" , \"items\" : { \"$ref\" : \"#/components/schemas/instance_id\" }, \"type\" : \"array\" }","title":"Schema for type: instance_set"},{"location":"api/#schema-for-type-artefact_id","text":"No description. { \"pattern\" : \"^[a-z][a-zA-Z_:]*$\" , \"type\" : \"string\" }","title":"Schema for type: artefact_id"},{"location":"api/#schema-for-type-instance_id","text":"No description. { \"$ref\" : \"#/components/schemas/artefact_id\" }","title":"Schema for type: instance_id"},{"location":"api/#schema-for-type-port_id","text":"No description. { \"$ref\" : \"#/components/schemas/artefact_id\" }","title":"Schema for type: port_id"},{"location":"api/#schema-for-type-artefact","text":"No description. { \"oneOf\" : [ { \"$ref\" : \"#/components/schemas/pipeline\" }, { \"$ref\" : \"#/components/schemas/onramp\" }, { \"$ref\" : \"#/components/schemas/offramp\" }, { \"$ref\" : \"#/components/schemas/binding\" } ] }","title":"Schema for type: artefact"},{"location":"api/#schema-for-type-instance","text":"No description. { \"oneOf\" : [ { \"$ref\" : \"#/components/schemas/mapping\" } ] }","title":"Schema for type: instance"},{"location":"api/#schema-for-type-publish_ok","text":"Response when a registry publish was succesful { \"description\" : \"Response when a registry publish was succesful\" , \"properties\" : { \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" , \"description\" : \"The id of the pubished artefact\" } }, \"required\" : [ \"id\" ] }","title":"Schema for type: publish_ok"},{"location":"api/#schema-for-type-pipeline","text":"State of an pipeline, expressed as trickle source code. { \"description\" : \"State of an pipeline, expressed as trickle source code.\" , \"type\" : \"string\" }","title":"Schema for type: pipeline"},{"location":"api/#schema-for-type-onramp_state","text":"State of an onramp, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an onramp, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/onramp\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" }","title":"Schema for type: onramp_state"},{"location":"api/#schema-for-type-onramp","text":"A tremor onramp specification { \"additionalProperties\" : false , \"description\" : \"A tremor onramp specification\" , \"properties\" : { \"codec\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"codec_map\" : { \"$ref\" : \"#/components/schemas/codec_map\" }, \"config\" : { \"description\" : \"A map of key/value pairs used to configure this onramp\" , \"type\" : \"object\" }, \"description\" : { \"description\" : \"Documentation for this type\" , \"type\" : \"string\" }, \"err_required\" : { \"description\" : \"Whether a pipeline needs to be connected to the err port before startup\" , \"type\" : \"boolean\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"linked\" : { \"description\" : \"Whether this offramp is linked or not\" , \"type\" : \"boolean\" }, \"metrics_interval_s\" : { \"description\" : \"interval in which metrics info is published\" , \"minimum\" : 0 , \"type\" : \"integer\" }, \"postprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/postprocessor\" }, \"type\" : \"array\" }, \"preprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/preprocessor\" }, \"type\" : \"array\" }, \"type\" : { \"$ref\" : \"#/components/schemas/onramp_type\" } }, \"required\" : [ \"type\" , \"id\" ], \"type\" : \"object\" }","title":"Schema for type: onramp"},{"location":"api/#schema-for-type-offramp_state","text":"State of an offramp, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an offramp, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/offramp\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" }","title":"Schema for type: offramp_state"},{"location":"api/#schema-for-type-offramp","text":"A tremor offramp specification { \"additionalProperties\" : false , \"description\" : \"A tremor offramp specification\" , \"properties\" : { \"codec\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"codec_map\" : { \"$ref\" : \"#/components/schemas/codec_map\" }, \"config\" : { \"description\" : \"A map of key/value pairs used to configure this onramp\" , \"type\" : \"object\" }, \"description\" : { \"description\" : \"Documentation for this type\" , \"type\" : \"string\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"linked\" : { \"description\" : \"Whether this offramp is linked or not\" , \"type\" : \"boolean\" }, \"metrics_interval_s\" : { \"description\" : \"interval in which metrics info is published\" , \"minimum\" : 0 , \"type\" : \"integer\" }, \"postprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/postprocessor\" }, \"type\" : \"array\" }, \"preprocessors\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/preprocessor\" }, \"type\" : \"array\" }, \"type\" : { \"$ref\" : \"#/components/schemas/offramp_type\" } }, \"required\" : [ \"type\" , \"id\" ], \"type\" : \"object\" }","title":"Schema for type: offramp"},{"location":"api/#schema-for-type-binding_state","text":"State of an binding, including specification and instances { \"additionalProperties\" : false , \"description\" : \"State of an binding, including specification and instances\" , \"properties\" : { \"artefact\" : { \"$ref\" : \"#/components/schemas/binding\" }, \"instances\" : { \"$ref\" : \"#/components/schemas/instance_set\" } }, \"type\" : \"object\" }","title":"Schema for type: binding_state"},{"location":"api/#schema-for-type-binding","text":"A tremor binding specification { \"additionalProperties\" : false , \"description\" : \"A tremor binding specification\" , \"properties\" : { \"description\" : { \"type\" : \"string\" }, \"id\" : { \"$ref\" : \"#/components/schemas/artefact_id\" }, \"links\" : { \"$ref\" : \"#/components/schemas/binding_map\" } }, \"required\" : [ \"id\" , \"links\" ], \"type\" : \"object\" }","title":"Schema for type: binding"},{"location":"api/#schema-for-type-binding_map","text":"A map of binding specification links { \"additionalProperties\" : false , \"description\" : \"A map of binding specification links\" , \"patternProperties\" : { \"^(tremor://localhost)?/(onramp|pipeline)/[a-zA-Z][A-Za-z0-9_]*/[a-zA-Z][A-Za-z0-9_]*$\" : { \"additionalItems\" : false , \"items\" : { \"$ref\" : \"#/components/schemas/binding_dst\" }, \"type\" : \"array\" } }, \"type\" : \"object\" }","title":"Schema for type: binding_map"},{"location":"api/#schema-for-type-binding_dst","text":"No description. { \"pattern\" : \"^(tremor://localhost)?/(pipeline|offramp)/[a-zA-Z][A-Za-z0-9_]*/[a-zA-Z][A-Za-z0-9_]*$\" , \"type\" : \"string\" }","title":"Schema for type: binding_dst"},{"location":"api/#schema-for-type-mapping","text":"A tremor mapping specification { \"description\" : \"A tremor mapping specification\" , \"type\" : \"object\" }","title":"Schema for type: mapping"},{"location":"api/#schema-for-type-offramp_type","text":"supported offramp types { \"description\" : \"supported offramp types\" , \"enum\" : [ \"blackhole\" , \"debug\" , \"elastic\" , \"exit\" , \"file\" , \"kafka\" , \"newrelic\" , \"postgres\" , \"rest\" , \"stderr\" , \"stdout\" , \"tcp\" , \"udp\" , \"ws\" ], \"type\" : \"string\" }","title":"Schema for type: offramp_type"},{"location":"api/#schema-for-type-onramp_type","text":"supported onramp types { \"description\" : \"supported onramp types\" , \"enum\" : [ \"blaster\" , \"crononome\" , \"file\" , \"kafka\" , \"metronome\" , \"postgres\" , \"rest\" , \"tcp\" , \"udp\" , \"ws\" ], \"type\" : \"string\" }","title":"Schema for type: onramp_type"},{"location":"api/#schema-for-type-codec","text":"The data format supported for encoding/decoding to/from tremor types { \"description\" : \"The data format supported for encoding/decoding to/from tremor types\" , \"enum\" : [ \"binflux\" , \"influx\" , \"json\" , \"msgpack\" , \"null\" , \"statsd\" , \"string\" , \"yaml\" ], \"type\" : \"string\" }","title":"Schema for type: codec"},{"location":"api/#schema-for-type-codec_map","text":"A map from mime-type to codec { \"additionalProperties\" : { \"$ref\" : \"#/components/schemas/codec\" }, \"description\" : \"A map from mime-type to codec\" , \"type\" : \"object\" }","title":"Schema for type: codec_map"},{"location":"api/#schema-for-type-preprocessor","text":"Supported preprocessors { \"description\" : \"Supported preprocessors\" , \"enum\" : [ \"base64\" , \"decompress\" , \"gelf-chunking\" , \"gzip\" , \"length-prefixed\" , \"lines\" , \"lines-null\" , \"lines-pipe\" , \"lines-no-buffer\" , \"lines-cr-no-buffer\" , \"lz4\" , \"remove-empty\" , \"snappy\" , \"xz\" , \"zlib\" ], \"type\" : \"string\" }","title":"Schema for type: preprocessor"},{"location":"api/#schema-for-type-postprocessor","text":"Supported postprocessors { \"description\" : \"Supported postprocessors\" , \"enum\" : [ \"base64\" , \"gelf-chunking\" , \"gzip\" , \"length-prefixed\" , \"lines\" , \"lz4\" , \"snappy\" , \"xz\" , \"zlib\" ], \"type\" : \"string\" }","title":"Schema for type: postprocessor"},{"location":"history/","text":"History \u00b6 Tremor started with a straight forward problem statement: During peak events logs and metrics going to Elastic Search and InfluxDB back up in Kafka queues removing visibility from the system. tremor-0.9 \u00b6 The 0.9 release unifies the various CLI tools into a singular tremor binary that is located in the tremor-cli folder. In addition we introduce the underpinning for both guaranteed delivery and circuit breakers. On this foundation it adds support for linked transports which allows to use tremor in proxy like scenarios. tremor-0.8 (develop) \u00b6 In this release, the embedded script and query languages have been extended with support for modular development. Tremor scripts and queries can now be organised into nested module namespaces. In tremor-script - functions, constants and modules can be reused and organized into modules. In tremor-query - window, operator and script definitions can be reused into modules. Tremor-script has been extended to support fixed arity functions, with optional variable arguments. This form of function can be used for a constrained form of tail-recursion. Match functions allow pattern matching and support partial functions. Intrinsic functions wrap builtin functions. Tremor-script's pattern matching syntax has been extended with tuple-based positional matching. A new TREMOR_PATH allows modules to be referenced from the file system. Tremor preprocesses source and computes a singular preprocessed query or script source. This release was primarily focused on enabling teams to modularise, reuse and support more complex scripts and queries as the complexity and size of user defined logic deployed into tremor continues to grow. tremor-0.7 (develop) \u00b6 First open source release of tremor. tremor-0.6 (develop) \u00b6 In this release the event by event scripting language is used as the basis for a structured query language that supports time-series windowing, aggregate functions and expressive composition grouping functions via select statements. The language supercedes the now deprecated pipeline YAML format whilst providing a backwards compatible runtime that can leverage existing pipeline operators, allows branching and combining streams to form a graph, and specifying and overriding default operator port assignments. The release includes basic statistics (min, max, count, mean, stdev, var) and quartile/percentile estimation via the high dynamic range (HDR) and DDS (distributed data sketch) based algorithms. The SQL language, trickle, supports aggregate of aggregate summary statistics without error amplification through a tilt-frame mechanism combined with merge-capable aggregate functions and opens up use cases for tremor to aggregate/summary processing for metrics, alerting, prediction/forecasting and anomaly/outlier detection. Small enhancements to the scripting language include string interpolation, bitwise and shift binary expressions. A cron-based onramp for scheduled/periodic one-shot or repeated events has been added. tremor-0.5.3 (stable) \u00b6 In this release the focus was set on the ingest layer for Logstash and Telegraf. By adding support for UDP as well as reassembling GELF chunks we can replace both Telegraf, Logstash as well as the home grown GELF proxy that currently needs to be deployed alongside of Logstash. Part of the work was to introduce pre-processors that allow to handle binary manipulation on incoming messages without the codec needing to be aware of them. Examples would be decompression, base64 encoding, splitting multi line messages or assembling GELF chunks. While the involved code in this release is small the operational win that comes with a now unified logging and metrics pipeline is significant. tremor-0.5.2 (stable) \u00b6 tremor-0.5.2 resolved the degenerate case we detected in the 0.5.0 release and put tremor way ahead of Logstash in all our benchmarks. The performance work resulted in finding a lack of functionality in the last release, it was not possible to delete a field from a record from a key stored in a variable - this was resolved as well changing the syntax of the patch statement to require escaping of keys. In addition this patch included small improvements in documentation to improve operator experience when first using tremor. tremor-0.5.1 (stable) \u00b6 With this release we are experimenting with a shorter release cycle. While both 0.4 and 0.5 introduced major changes they lay a stable foundation that makes it easier to ship small features and wins to our users much more frequently. In this release in particular we introduce some quality of life improvements on extractors by improving error reporting on bad patterns and moving it to compile time with the same good errors we provide for language level errors. Along with that we fixed a bug on the influx line protocol renderer that caused integers to be re-rendered as floats and multi field items to be rendered in-correctly. We introduced the chash module that currently houses an implementation of Google's jump consistent hashing . This enables partitioning a data flow uniformly over multiple outputs with minimal cost and no need for synchronisation. tremor-0.5 (stable) \u00b6 Version 0.5 we introduced a completely overhauled scripting language. We moved from a rule to action based system to something that can best be described as a ETL focused language now with powerful constructs such as match statements with record- and array patterns and an extensive function library . It introduces the concept of extractors , part of tremor script they allow matching against complex patterns and extracting information from them. This work covers common things like regular expressions or globs, as well as grok and dissect patterns. But also allows decoding embedded influx, json, or even base64 data without much extra work. As always this release introduces some optimisations, since JSON is the main encoding used for data we introduced SIMD optimized decoding of this data base on Geoff Langdales, and Daniel Lemires work into this area. tremor-0.4 (stable) \u00b6 This release combined the lessons from the 0.3 and 0.2 looking at what worked in one and the other. The 0.4 release kept the dynamic pipelines but implemented them in a way closer to how they were implemented in 0.2 retaining the performance this way. Also, contraflow introduced in 0.3 was extended with signals to allow non-event carrying messages to move through the pipeline for operational purposes such periodic ticks. The matching language of the earlier releases got a complete overhaul becoming a more powerful scripting language - tremor-script . Tremor script introduced features such as event metadata variables to drive operator behavior outside of the script itself, mutation of event values, support for functions, along with a return statement that allows early returns from a script to save run time. The basic idea of a yaml file as configuration was carried over from 0.3 but the content dramatically altered to be more usable. Along with the new syntax also the ability to run multiple pipelines, onramps and offramp in the same tremor instance were introduced. With the new config tremor, 0.4 also introduced an API that allows adding, remove and alter the components running in an instance without requiring a restart. This feature came with the addition of tremor-cli to expose this API to operators without requiring to remember the interface details. tremor-0.3 (develop) \u00b6 The limitations of static steps imposed run 0.2 were a real limitation. With the 0.3 release tremor got the capability to run arbitrarily complex pipelines of interconnected nodes, along with an improved set of features in the matching language. Along with that, it introduced the ability to bridge asynchronous and synchronous inputs and outputs allowing for new combinations of on- and Off-Ramps. The most notable addition to the 0.3 version of tremor, however, was contraflow , a system that allowed us for downstream nodes to traverse the graph in reverse order to communicate back metrics and statistics. This allowed generalising the concept of back pressure from 0.2 and applying it in different places of the pipeline. With the dynamic pipelines, the configuration also went away from arguments passed to the command line to a yaml file that carried the specification of the pipeline which made it easier for an operator to maintain the pipeline. Those additions and the exploratory nature of the 0.3 released reduced performance by approximately the factor of 2 in this release. tremor-0.2 (stable) \u00b6 With kopy as the basis, the next step was what best could be described as an MVP. A bare minimum implementation that was good enough to serve the immediate need and form the foundation for going forward. The 0.2 release of tremor consisted of a set of static steps that were configured over command line arguments. It handled reading data from Kafka, writing data to Elastic Search and InfluxDB. It included a simplistic classification engine that allowed limiting events based on an assigned class. Also, last but not least a method for handling downstream system overload and back-pressure by applying an incremental backoff strategy. It solved the problem initially presented - during the next peak event there was no lag invisibility into metrics or logs. And not only did it work it also reduced the computer footprint of the system it replaced by 80%. kopy \u00b6 From this tremor started to build. Its root was a tool called kopy (short for k(afka-c)opy ) that, given a Kafka queue to copy from and one to copy to, would replicate the data from one to the other. kopy itself was far from sophisticated, but it was good enough to verify the idea of building a tool to solve the problem mentioned above in rust. It served us through the first iteration as the tool we used to collect test data and move it into private queues for replaying.","title":"History"},{"location":"history/#history","text":"Tremor started with a straight forward problem statement: During peak events logs and metrics going to Elastic Search and InfluxDB back up in Kafka queues removing visibility from the system.","title":"History"},{"location":"history/#tremor-09","text":"The 0.9 release unifies the various CLI tools into a singular tremor binary that is located in the tremor-cli folder. In addition we introduce the underpinning for both guaranteed delivery and circuit breakers. On this foundation it adds support for linked transports which allows to use tremor in proxy like scenarios.","title":"tremor-0.9"},{"location":"history/#tremor-08-develop","text":"In this release, the embedded script and query languages have been extended with support for modular development. Tremor scripts and queries can now be organised into nested module namespaces. In tremor-script - functions, constants and modules can be reused and organized into modules. In tremor-query - window, operator and script definitions can be reused into modules. Tremor-script has been extended to support fixed arity functions, with optional variable arguments. This form of function can be used for a constrained form of tail-recursion. Match functions allow pattern matching and support partial functions. Intrinsic functions wrap builtin functions. Tremor-script's pattern matching syntax has been extended with tuple-based positional matching. A new TREMOR_PATH allows modules to be referenced from the file system. Tremor preprocesses source and computes a singular preprocessed query or script source. This release was primarily focused on enabling teams to modularise, reuse and support more complex scripts and queries as the complexity and size of user defined logic deployed into tremor continues to grow.","title":"tremor-0.8 (develop)"},{"location":"history/#tremor-07-develop","text":"First open source release of tremor.","title":"tremor-0.7 (develop)"},{"location":"history/#tremor-06-develop","text":"In this release the event by event scripting language is used as the basis for a structured query language that supports time-series windowing, aggregate functions and expressive composition grouping functions via select statements. The language supercedes the now deprecated pipeline YAML format whilst providing a backwards compatible runtime that can leverage existing pipeline operators, allows branching and combining streams to form a graph, and specifying and overriding default operator port assignments. The release includes basic statistics (min, max, count, mean, stdev, var) and quartile/percentile estimation via the high dynamic range (HDR) and DDS (distributed data sketch) based algorithms. The SQL language, trickle, supports aggregate of aggregate summary statistics without error amplification through a tilt-frame mechanism combined with merge-capable aggregate functions and opens up use cases for tremor to aggregate/summary processing for metrics, alerting, prediction/forecasting and anomaly/outlier detection. Small enhancements to the scripting language include string interpolation, bitwise and shift binary expressions. A cron-based onramp for scheduled/periodic one-shot or repeated events has been added.","title":"tremor-0.6 (develop)"},{"location":"history/#tremor-053-stable","text":"In this release the focus was set on the ingest layer for Logstash and Telegraf. By adding support for UDP as well as reassembling GELF chunks we can replace both Telegraf, Logstash as well as the home grown GELF proxy that currently needs to be deployed alongside of Logstash. Part of the work was to introduce pre-processors that allow to handle binary manipulation on incoming messages without the codec needing to be aware of them. Examples would be decompression, base64 encoding, splitting multi line messages or assembling GELF chunks. While the involved code in this release is small the operational win that comes with a now unified logging and metrics pipeline is significant.","title":"tremor-0.5.3 (stable)"},{"location":"history/#tremor-052-stable","text":"tremor-0.5.2 resolved the degenerate case we detected in the 0.5.0 release and put tremor way ahead of Logstash in all our benchmarks. The performance work resulted in finding a lack of functionality in the last release, it was not possible to delete a field from a record from a key stored in a variable - this was resolved as well changing the syntax of the patch statement to require escaping of keys. In addition this patch included small improvements in documentation to improve operator experience when first using tremor.","title":"tremor-0.5.2 (stable)"},{"location":"history/#tremor-051-stable","text":"With this release we are experimenting with a shorter release cycle. While both 0.4 and 0.5 introduced major changes they lay a stable foundation that makes it easier to ship small features and wins to our users much more frequently. In this release in particular we introduce some quality of life improvements on extractors by improving error reporting on bad patterns and moving it to compile time with the same good errors we provide for language level errors. Along with that we fixed a bug on the influx line protocol renderer that caused integers to be re-rendered as floats and multi field items to be rendered in-correctly. We introduced the chash module that currently houses an implementation of Google's jump consistent hashing . This enables partitioning a data flow uniformly over multiple outputs with minimal cost and no need for synchronisation.","title":"tremor-0.5.1 (stable)"},{"location":"history/#tremor-05-stable","text":"Version 0.5 we introduced a completely overhauled scripting language. We moved from a rule to action based system to something that can best be described as a ETL focused language now with powerful constructs such as match statements with record- and array patterns and an extensive function library . It introduces the concept of extractors , part of tremor script they allow matching against complex patterns and extracting information from them. This work covers common things like regular expressions or globs, as well as grok and dissect patterns. But also allows decoding embedded influx, json, or even base64 data without much extra work. As always this release introduces some optimisations, since JSON is the main encoding used for data we introduced SIMD optimized decoding of this data base on Geoff Langdales, and Daniel Lemires work into this area.","title":"tremor-0.5 (stable)"},{"location":"history/#tremor-04-stable","text":"This release combined the lessons from the 0.3 and 0.2 looking at what worked in one and the other. The 0.4 release kept the dynamic pipelines but implemented them in a way closer to how they were implemented in 0.2 retaining the performance this way. Also, contraflow introduced in 0.3 was extended with signals to allow non-event carrying messages to move through the pipeline for operational purposes such periodic ticks. The matching language of the earlier releases got a complete overhaul becoming a more powerful scripting language - tremor-script . Tremor script introduced features such as event metadata variables to drive operator behavior outside of the script itself, mutation of event values, support for functions, along with a return statement that allows early returns from a script to save run time. The basic idea of a yaml file as configuration was carried over from 0.3 but the content dramatically altered to be more usable. Along with the new syntax also the ability to run multiple pipelines, onramps and offramp in the same tremor instance were introduced. With the new config tremor, 0.4 also introduced an API that allows adding, remove and alter the components running in an instance without requiring a restart. This feature came with the addition of tremor-cli to expose this API to operators without requiring to remember the interface details.","title":"tremor-0.4 (stable)"},{"location":"history/#tremor-03-develop","text":"The limitations of static steps imposed run 0.2 were a real limitation. With the 0.3 release tremor got the capability to run arbitrarily complex pipelines of interconnected nodes, along with an improved set of features in the matching language. Along with that, it introduced the ability to bridge asynchronous and synchronous inputs and outputs allowing for new combinations of on- and Off-Ramps. The most notable addition to the 0.3 version of tremor, however, was contraflow , a system that allowed us for downstream nodes to traverse the graph in reverse order to communicate back metrics and statistics. This allowed generalising the concept of back pressure from 0.2 and applying it in different places of the pipeline. With the dynamic pipelines, the configuration also went away from arguments passed to the command line to a yaml file that carried the specification of the pipeline which made it easier for an operator to maintain the pipeline. Those additions and the exploratory nature of the 0.3 released reduced performance by approximately the factor of 2 in this release.","title":"tremor-0.3 (develop)"},{"location":"history/#tremor-02-stable","text":"With kopy as the basis, the next step was what best could be described as an MVP. A bare minimum implementation that was good enough to serve the immediate need and form the foundation for going forward. The 0.2 release of tremor consisted of a set of static steps that were configured over command line arguments. It handled reading data from Kafka, writing data to Elastic Search and InfluxDB. It included a simplistic classification engine that allowed limiting events based on an assigned class. Also, last but not least a method for handling downstream system overload and back-pressure by applying an incremental backoff strategy. It solved the problem initially presented - during the next peak event there was no lag invisibility into metrics or logs. And not only did it work it also reduced the computer footprint of the system it replaced by 80%.","title":"tremor-0.2 (stable)"},{"location":"history/#kopy","text":"From this tremor started to build. Its root was a tool called kopy (short for k(afka-c)opy ) that, given a Kafka queue to copy from and one to copy to, would replicate the data from one to the other. kopy itself was far from sophisticated, but it was good enough to verify the idea of building a tool to solve the problem mentioned above in rust. It served us through the first iteration as the tool we used to collect test data and move it into private queues for replaying.","title":"kopy"},{"location":"overview/","text":"Architecture Overview \u00b6 This is a short architectural overview of Tremor Scope \u00b6 We cover the runtime, data and distribution model of Tremor from 50,000 feet. Goodness of fit \u00b6 Tremor is designed for high volume messaging environments and is good for: Man in the middle bridging - Tremor is designed to bridge asynchronous upstream sources with synchronous downstream sinks. More generally, Tremor excels at intelligently bridging from sync/async to async/sync so that message flows can be classified, dimensioned, segmented, routed using user defined logic whilst providing facilities to handle back-pressure and other hard distributed systems problems on behalf of its operators. For example - Log data distributed from Kafka to ElasticSearch via Logstash can introduce significant lag or delays as logstash becomes a publication bottleneck. Untimely, late delivery of log data to the network operations centre under severity zero or at/over capacity conditions reduces our ability to respond to major outage and other byzantine events in a timely fashion. In-flight redeployment - Tremor can be reconfigured via it's API allowing workloads to be migrated to/from new systems and logic to be retuned, reconfigured, or reconditioned without redeployment. event processing - Tremor adopts many principles from DEBS ( Distributed Event Based Systems ), ESP ( Event Stream Processor ) and the CEP ( Complex Event Processing ) communities. However in its current state of evolution Tremor has an incomplete feature set in this regard. Over time Tremor MAY evolve as an ESP or CEP solution but this is an explicit non-goal of the project. Tremor URLs \u00b6 Since Tremor v0.4, all internal artefacts and running instances of onramps , offramps and pipelines are dynamically configurable. The introduction of a dynamically configurable deployment model has resulted in the introduction of Tremor URLs. The Tremor API is built around this URL and the configuration space it enshrines: Example URL Description tremor://localhost:9898/ A local Tremor instance Accessible on the local host REST API on port 9898 of the local host tremor:/// The current Tremor instance or 'self' tremor:///pipeline A list of pipelines tremor:///pipeline/bob The pipeline identified as bob tremor:///onramp/alice The onramp identified as Alice tremor:///binding/talk A binding that allows alice and bob to connect tremor:///binding/talk/tls An active conversation or instance of a talk between alice and bob The Tremor REST API and configuration file formats also follow the same URL format. In the case of configuration, a shorthand URL form is often used. In the configuration model, we discriminate artefacts by type, so it is often sufficient to infer the tremor:///{artefact-kind} component when specifying ( configuring ) artefacts. In bindings, however, we minimally need to use the full URL path component ( for example: /pipeline/01 ). At this time, the full URL form is not used in the configuration model. Runtime model \u00b6 The Tremor runtime is composed of multiple internal components that communicate via queues across multiple threads of control managed/coordinated by a set of control plane actors driven by the Tremor REST API. Processing model \u00b6 Tremor uses an async task model ontop of the smol runtime and async-rs . Currently, the model is: A task is spawned per onramp A task is spawned per offramp A task is spawned per pipeline Tasks communicate via queues The Processing model is very likely to evolve over time as concurrency, threading, async and other primitives available in the rust ecosystem mature and evolve. Event ordering \u00b6 Events generally flow from onramps, where they are ingested into the system, through pipelines, into offramps where they are pushed to external systems. Tremor imposes causal event ordering over ingested events and processes events deterministically. This does not mean that Tremor imposes a total ordering over all ingested events, however ( ugh, because that is not tractable in a distributed system ). Events flowing into Tremor from multiple onramps are considered independent. Events flowing from multiple clients into Tremor are also considered independent. However, events sent by a specific client through a specific onramp into a passthrough pipeline would flow through Tremor in their origin order and be passed to offramps in the same origin order. Requests from multiple independent sources over the same pipeline may arbitrarily interleave, but should not re-order. In pipelines, events are processed in depth first order. Where Tremor operators have no intrinsic ordering ( such as a branch split ), Tremor internally imposes an order. Operator's may arbitrarily reorder messages. For example, a windowed operator might batch multiple events into a single batch. An iteration operator could reverse the batch and forward individual unbatched events in an order that is the reverse of the original ingest order for that batch of events. However, the engine itself does not re-order events. Events are handled and processed in a strictly deterministic order by design. Pipeline Model \u00b6 The core processing model of Tremor is based on a directed-acyclic-graph based dataflow model. Tremor pipelines are a graph of vertices ( nodes, or operators ) with directed edges ( or connections, or links ) between operators in the graph. Events from the outside world in a Tremor pipeline can only flow in one direction from inputs ( specific operators that connect pipeline operators to onramps ) via operators to outputs ( specific operators that connect pipeline operators to offramps). Operators process events and may produce zero or many output events for each event processed by the operator. As operators are the primary building block of Tremor processing logic they are designed for extension. Tremor pipelines understand three different types of events: Data events - these are data events delivered via onramps into a pipeline or to offramps from a pipeline. Most events that flow through a Tremor pipeline are of this type. Signal events - these are synthetic events delivered by the Tremor runtime into a pipeline under certain conditions. Contraflow events - these are synthetic events delivered by the Tremor runtime into a pipeline under certain conditions that are caused by the processing of events already in a Tremor system. Back-pressure events exploit contraflow. Dataflow \u00b6 Data-flow events are the bread and butter of Tremor. These are line of business data events ingested via onramps from external upstream systems, processed through pipelines and published downstream via offramps to downstream external systems. SignalFlow \u00b6 Transparent to pipeline authors, but visible to onramp, offramp and operator developers are signal events. Signal events are synthetic events generated by the tremor-runtime and system that can be exploited by operators for advanced event handling purposes. Contraflow \u00b6 A core conceit with distributed event-based systems arises due to their typically asynchronous nature. Tremor employs a relatively novel algorithm to handle back-pressure or other events that propagate backwards through a pipeline. But pipelines are directed-acyclic-graphs ( DAGs ), so how do we back-propagate events without introducing cycles? The answer is: There can be no cycles in a DAG DAGS are traversed in depth-first-search ( DFS ) order There can be no cycles in a DAG traversed in reverse-DFS order. If we join a DAG d, with its mirrored ( reversed ) DAG d' We get another DAG where Every output in the DAG d, can continue propagating events in its reverse DAG d', without cycles though its d' mirrored input Branches in DAG d, become Combinators in DAG d' Combinators in DAG d, become Branches in DAG d' Any back-pressure or other events detected in the processing of existing events can result in a synthetic signalling event being injected into the reverse-DAG. We call the injected events 'contraflow' events because they move backwards against the primary data flow. The cost or overhead of not injecting a contraflow event is zero The cost or overhead of an injected contraflow event ( in Tremor ) is minimised through pruning - for example - operators that are not contraflow aware do not need to receive or process contraflow events - Tremor optimises for this case. We call the output-input pairs at the heart of contraflow the 'pivot point' Contraflow has been used in other event processing systems and was designed /invented by one of the members of the Tremor core team ( in a previous life ). There are many other ways to handle back-pressure ( for example: those used by Spark, Storm, Hazelcast Jet, \u2026 ) but they are biasing for other nuances and tradeoffs than Tremor. Contraflow is far simpler to reason about and develop verifiable systems and code against as a user and puts a lot of the pressure for a good solution onto the Tremor project itself. Only time will tell which philosophy results in less pager duty! Although the contraflow mechanism may seem complex, its far simpler than back-pressure handling by almost all other reasonable mechanisms and with far fewer negative side-effects and tradeoffs. Guaranteed delivery \u00b6 Tremor supports guaranteed delivery as long as both onramps and offramps support it. Alternatively, the qos::wal can be used to introduce a layer of Guaranteed delivery for onramps that do not support it naturally. The basic concept is that each event has a monotonically growing ID, once this ID is acknowledged as delivered all events with the provided ID or a lower Id are considered delivered. If an ID is marked as failed to deliver all events up until this ID will be replayed. Runtime facilities \u00b6 Tremor's runtime is composed of a number of facilities that work together to provide service. Conductor \u00b6 The Tremor API is a REST based API that allows Tremor operators to manage the lifecycle of onramps, offramps and pipelines deployed into a Tremor based system. The set of facilities in the runtime that are related to service lifecycle, activation and management are often referred to collectively as the Tremor conductor or Tremor control plane. These terms can be used interchangeably. Operators CAN conduct or orchestrate one or many Tremor servers through its REST based API. The REST API is implemented as an actor in the Tremor runtime. The API actor in turn interfaces with registry and repository facilities. Tremor distinguishes between artefacts and instances. Artefacts in Tremor have no runtime overhead. Artefacts in Tremor are declarative specifications of: Onramps - An onramp specification is a specific configuration of a supported onramp kind Offramps - An offramp specification is a specific configuration of a supported offramp kind Pipelines - A pipeline specification is a specific configuration of a pipeline graph Bindings - A binding specification describes how onramps, pipelines and offramps should be interconnected Artefacts can be thought of analagously to code. They are a set of instructions, rules or configurations. As such they are registered with Tremor via its API and stored in Tremor's artefact repository. Deployment in Tremor, is achieved through a mapping artefact. The mapping artefact specifies how artefacts should be deployed into one or many runtime instances, activated, and connected to live instances of onramps or offramps. In Tremor, publishing a mapping results in instances being deployed as a side-effect. By unpublishing or deleting a mapping instances are undeployed as a side-effect. Metrics \u00b6 Metrics in Tremor are implemented as a pipeline and deployed during startup. Metrics are builtin and can not be undeployed. Operators MAY attach offramps to the metrics service to distribute metrics to external systems, such as InfluxDB or Kafka. Data model \u00b6 Tremor supports unstructured data. Data can be raw binary, JSON, MsgPack, Influx or other structures. When data is ingested into a Tremor pipeline it can be any supported format. Tremor pipeline operators however, often assume some structure. For hierarchic or nested formats such as JSON and MsgPack, Tremor uses the serde serialisation and deserialisation capabilities. Therefore, the in-memory format for JSON-like data in Tremor is effectively a simd_json::Value . This has the advantage of allowing Tremor-script to work against YAML, JSON or MsgPack data with no changes or considerations in the Tremor-script based on the origin data format. For line oriented formats such as the Influx Line Protocol, or GELF these are typically transformed to Tremor's in-memory format ( currently based on serde ). For raw binary or other data formats, Tremor provides a growing set of codecs that convert external data to Tremor in-memory form or that convert Tremor in-memory form to an external data format. In general, operators and developers should minimize the number of encoding and decoding steps required in the transit of data through Tremor or between Tremor instances. The major overhead in most Tremor systems is encoding and decoding overhead. To compensate that, as JSON is the most dominant format, we ported simd-json this reduces the cost of en- and decoding significantly compared to other JSON implementations in Rust. Distribution model \u00b6 Tremor does not ( yet ) have an out-of-the-box network protocol. A native Tremor protocol is planned in the immediate / medium term. As such, the distribution model for Tremor is currently limited to the set of available onramp and offramp connectors. However the websocket onramp and offramp can be used for Tremor to Tremor communication. Client/Server \u00b6 Tremor, in its current form, is a client-server system. Tremor exposes a synchronous blocking RESTful API over HTTP for conducting operations related to its high throughput and relatively high performance pipeline-oriented data plane. Tremor, in the near future, will add a clustering capability making it a distributed system. Tremor will still support client-server deployments through a 'standalone' mode of clustered operation. Tremor in 'standalone' mode can be thought of as client-server or a 'cluster of one' depending on your own bias or preferences, dear reader.","title":"Architecture Overview"},{"location":"overview/#architecture-overview","text":"This is a short architectural overview of Tremor","title":"Architecture Overview"},{"location":"overview/#scope","text":"We cover the runtime, data and distribution model of Tremor from 50,000 feet.","title":"Scope"},{"location":"overview/#goodness-of-fit","text":"Tremor is designed for high volume messaging environments and is good for: Man in the middle bridging - Tremor is designed to bridge asynchronous upstream sources with synchronous downstream sinks. More generally, Tremor excels at intelligently bridging from sync/async to async/sync so that message flows can be classified, dimensioned, segmented, routed using user defined logic whilst providing facilities to handle back-pressure and other hard distributed systems problems on behalf of its operators. For example - Log data distributed from Kafka to ElasticSearch via Logstash can introduce significant lag or delays as logstash becomes a publication bottleneck. Untimely, late delivery of log data to the network operations centre under severity zero or at/over capacity conditions reduces our ability to respond to major outage and other byzantine events in a timely fashion. In-flight redeployment - Tremor can be reconfigured via it's API allowing workloads to be migrated to/from new systems and logic to be retuned, reconfigured, or reconditioned without redeployment. event processing - Tremor adopts many principles from DEBS ( Distributed Event Based Systems ), ESP ( Event Stream Processor ) and the CEP ( Complex Event Processing ) communities. However in its current state of evolution Tremor has an incomplete feature set in this regard. Over time Tremor MAY evolve as an ESP or CEP solution but this is an explicit non-goal of the project.","title":"Goodness of fit"},{"location":"overview/#tremor-urls","text":"Since Tremor v0.4, all internal artefacts and running instances of onramps , offramps and pipelines are dynamically configurable. The introduction of a dynamically configurable deployment model has resulted in the introduction of Tremor URLs. The Tremor API is built around this URL and the configuration space it enshrines: Example URL Description tremor://localhost:9898/ A local Tremor instance Accessible on the local host REST API on port 9898 of the local host tremor:/// The current Tremor instance or 'self' tremor:///pipeline A list of pipelines tremor:///pipeline/bob The pipeline identified as bob tremor:///onramp/alice The onramp identified as Alice tremor:///binding/talk A binding that allows alice and bob to connect tremor:///binding/talk/tls An active conversation or instance of a talk between alice and bob The Tremor REST API and configuration file formats also follow the same URL format. In the case of configuration, a shorthand URL form is often used. In the configuration model, we discriminate artefacts by type, so it is often sufficient to infer the tremor:///{artefact-kind} component when specifying ( configuring ) artefacts. In bindings, however, we minimally need to use the full URL path component ( for example: /pipeline/01 ). At this time, the full URL form is not used in the configuration model.","title":"Tremor URLs"},{"location":"overview/#runtime-model","text":"The Tremor runtime is composed of multiple internal components that communicate via queues across multiple threads of control managed/coordinated by a set of control plane actors driven by the Tremor REST API.","title":"Runtime model"},{"location":"overview/#processing-model","text":"Tremor uses an async task model ontop of the smol runtime and async-rs . Currently, the model is: A task is spawned per onramp A task is spawned per offramp A task is spawned per pipeline Tasks communicate via queues The Processing model is very likely to evolve over time as concurrency, threading, async and other primitives available in the rust ecosystem mature and evolve.","title":"Processing model"},{"location":"overview/#event-ordering","text":"Events generally flow from onramps, where they are ingested into the system, through pipelines, into offramps where they are pushed to external systems. Tremor imposes causal event ordering over ingested events and processes events deterministically. This does not mean that Tremor imposes a total ordering over all ingested events, however ( ugh, because that is not tractable in a distributed system ). Events flowing into Tremor from multiple onramps are considered independent. Events flowing from multiple clients into Tremor are also considered independent. However, events sent by a specific client through a specific onramp into a passthrough pipeline would flow through Tremor in their origin order and be passed to offramps in the same origin order. Requests from multiple independent sources over the same pipeline may arbitrarily interleave, but should not re-order. In pipelines, events are processed in depth first order. Where Tremor operators have no intrinsic ordering ( such as a branch split ), Tremor internally imposes an order. Operator's may arbitrarily reorder messages. For example, a windowed operator might batch multiple events into a single batch. An iteration operator could reverse the batch and forward individual unbatched events in an order that is the reverse of the original ingest order for that batch of events. However, the engine itself does not re-order events. Events are handled and processed in a strictly deterministic order by design.","title":"Event ordering"},{"location":"overview/#pipeline-model","text":"The core processing model of Tremor is based on a directed-acyclic-graph based dataflow model. Tremor pipelines are a graph of vertices ( nodes, or operators ) with directed edges ( or connections, or links ) between operators in the graph. Events from the outside world in a Tremor pipeline can only flow in one direction from inputs ( specific operators that connect pipeline operators to onramps ) via operators to outputs ( specific operators that connect pipeline operators to offramps). Operators process events and may produce zero or many output events for each event processed by the operator. As operators are the primary building block of Tremor processing logic they are designed for extension. Tremor pipelines understand three different types of events: Data events - these are data events delivered via onramps into a pipeline or to offramps from a pipeline. Most events that flow through a Tremor pipeline are of this type. Signal events - these are synthetic events delivered by the Tremor runtime into a pipeline under certain conditions. Contraflow events - these are synthetic events delivered by the Tremor runtime into a pipeline under certain conditions that are caused by the processing of events already in a Tremor system. Back-pressure events exploit contraflow.","title":"Pipeline Model"},{"location":"overview/#dataflow","text":"Data-flow events are the bread and butter of Tremor. These are line of business data events ingested via onramps from external upstream systems, processed through pipelines and published downstream via offramps to downstream external systems.","title":"Dataflow"},{"location":"overview/#signalflow","text":"Transparent to pipeline authors, but visible to onramp, offramp and operator developers are signal events. Signal events are synthetic events generated by the tremor-runtime and system that can be exploited by operators for advanced event handling purposes.","title":"SignalFlow"},{"location":"overview/#contraflow","text":"A core conceit with distributed event-based systems arises due to their typically asynchronous nature. Tremor employs a relatively novel algorithm to handle back-pressure or other events that propagate backwards through a pipeline. But pipelines are directed-acyclic-graphs ( DAGs ), so how do we back-propagate events without introducing cycles? The answer is: There can be no cycles in a DAG DAGS are traversed in depth-first-search ( DFS ) order There can be no cycles in a DAG traversed in reverse-DFS order. If we join a DAG d, with its mirrored ( reversed ) DAG d' We get another DAG where Every output in the DAG d, can continue propagating events in its reverse DAG d', without cycles though its d' mirrored input Branches in DAG d, become Combinators in DAG d' Combinators in DAG d, become Branches in DAG d' Any back-pressure or other events detected in the processing of existing events can result in a synthetic signalling event being injected into the reverse-DAG. We call the injected events 'contraflow' events because they move backwards against the primary data flow. The cost or overhead of not injecting a contraflow event is zero The cost or overhead of an injected contraflow event ( in Tremor ) is minimised through pruning - for example - operators that are not contraflow aware do not need to receive or process contraflow events - Tremor optimises for this case. We call the output-input pairs at the heart of contraflow the 'pivot point' Contraflow has been used in other event processing systems and was designed /invented by one of the members of the Tremor core team ( in a previous life ). There are many other ways to handle back-pressure ( for example: those used by Spark, Storm, Hazelcast Jet, \u2026 ) but they are biasing for other nuances and tradeoffs than Tremor. Contraflow is far simpler to reason about and develop verifiable systems and code against as a user and puts a lot of the pressure for a good solution onto the Tremor project itself. Only time will tell which philosophy results in less pager duty! Although the contraflow mechanism may seem complex, its far simpler than back-pressure handling by almost all other reasonable mechanisms and with far fewer negative side-effects and tradeoffs.","title":"Contraflow"},{"location":"overview/#guaranteed-delivery","text":"Tremor supports guaranteed delivery as long as both onramps and offramps support it. Alternatively, the qos::wal can be used to introduce a layer of Guaranteed delivery for onramps that do not support it naturally. The basic concept is that each event has a monotonically growing ID, once this ID is acknowledged as delivered all events with the provided ID or a lower Id are considered delivered. If an ID is marked as failed to deliver all events up until this ID will be replayed.","title":"Guaranteed delivery"},{"location":"overview/#runtime-facilities","text":"Tremor's runtime is composed of a number of facilities that work together to provide service.","title":"Runtime facilities"},{"location":"overview/#conductor","text":"The Tremor API is a REST based API that allows Tremor operators to manage the lifecycle of onramps, offramps and pipelines deployed into a Tremor based system. The set of facilities in the runtime that are related to service lifecycle, activation and management are often referred to collectively as the Tremor conductor or Tremor control plane. These terms can be used interchangeably. Operators CAN conduct or orchestrate one or many Tremor servers through its REST based API. The REST API is implemented as an actor in the Tremor runtime. The API actor in turn interfaces with registry and repository facilities. Tremor distinguishes between artefacts and instances. Artefacts in Tremor have no runtime overhead. Artefacts in Tremor are declarative specifications of: Onramps - An onramp specification is a specific configuration of a supported onramp kind Offramps - An offramp specification is a specific configuration of a supported offramp kind Pipelines - A pipeline specification is a specific configuration of a pipeline graph Bindings - A binding specification describes how onramps, pipelines and offramps should be interconnected Artefacts can be thought of analagously to code. They are a set of instructions, rules or configurations. As such they are registered with Tremor via its API and stored in Tremor's artefact repository. Deployment in Tremor, is achieved through a mapping artefact. The mapping artefact specifies how artefacts should be deployed into one or many runtime instances, activated, and connected to live instances of onramps or offramps. In Tremor, publishing a mapping results in instances being deployed as a side-effect. By unpublishing or deleting a mapping instances are undeployed as a side-effect.","title":"Conductor"},{"location":"overview/#metrics","text":"Metrics in Tremor are implemented as a pipeline and deployed during startup. Metrics are builtin and can not be undeployed. Operators MAY attach offramps to the metrics service to distribute metrics to external systems, such as InfluxDB or Kafka.","title":"Metrics"},{"location":"overview/#data-model","text":"Tremor supports unstructured data. Data can be raw binary, JSON, MsgPack, Influx or other structures. When data is ingested into a Tremor pipeline it can be any supported format. Tremor pipeline operators however, often assume some structure. For hierarchic or nested formats such as JSON and MsgPack, Tremor uses the serde serialisation and deserialisation capabilities. Therefore, the in-memory format for JSON-like data in Tremor is effectively a simd_json::Value . This has the advantage of allowing Tremor-script to work against YAML, JSON or MsgPack data with no changes or considerations in the Tremor-script based on the origin data format. For line oriented formats such as the Influx Line Protocol, or GELF these are typically transformed to Tremor's in-memory format ( currently based on serde ). For raw binary or other data formats, Tremor provides a growing set of codecs that convert external data to Tremor in-memory form or that convert Tremor in-memory form to an external data format. In general, operators and developers should minimize the number of encoding and decoding steps required in the transit of data through Tremor or between Tremor instances. The major overhead in most Tremor systems is encoding and decoding overhead. To compensate that, as JSON is the most dominant format, we ported simd-json this reduces the cost of en- and decoding significantly compared to other JSON implementations in Rust.","title":"Data model"},{"location":"overview/#distribution-model","text":"Tremor does not ( yet ) have an out-of-the-box network protocol. A native Tremor protocol is planned in the immediate / medium term. As such, the distribution model for Tremor is currently limited to the set of available onramp and offramp connectors. However the websocket onramp and offramp can be used for Tremor to Tremor communication.","title":"Distribution model"},{"location":"overview/#clientserver","text":"Tremor, in its current form, is a client-server system. Tremor exposes a synchronous blocking RESTful API over HTTP for conducting operations related to its high throughput and relatively high performance pipeline-oriented data plane. Tremor, in the near future, will add a clustering capability making it a distributed system. Tremor will still support client-server deployments through a 'standalone' mode of clustered operation. Tremor in 'standalone' mode can be thought of as client-server or a 'cluster of one' depending on your own bias or preferences, dear reader.","title":"Client/Server"},{"location":"artefacts/codecs/","text":"Codecs \u00b6 Codecs are used to describe how to decode data from the wire and encode it back to wire format. Supported Codecs \u00b6 json \u00b6 En- and decodes JSON , for encoding a minified format is used (excluding newlines and spaces). string \u00b6 Treats the event as non structured string. It is required that the input is valid UTF-8 or the decoding will fail. msgpack \u00b6 Msgpack works based on the msgpack binary format that is structurally compatible with JSON. Being a binary format, message pack is significantly more performant and requires less space compared to JSON. It is an excellent candidate to use in tremor to tremor deployments but as well with any offramp that does support this format. influx \u00b6 En- and decodes the influx line protocol . The structural representation of the data is as follows: weather,location=us-midwest temperature=82 1465839830100400200 translates to: { \"measurement\" : \"weather\" , \"tags\" : { \"location\" : \"us-midwest\" }, \"fields\" : { \"temperature\" : 82.0 }, \"timestamp\" : 1465839830100400200 } binflux \u00b6 The binflux codec is a binary representation of influx data that is significantly faster encodes and decodes as well as takes less space on the wire. The format itself does not include framing but can be used with the size-prefix pre/post processors. For all numbers network byte order is used (big endian). The data is represented as follows: 2 byte (u16) length of the measurement in bytes n byte (utf8) the measurement (utf8 encoded string) 8 byte (u64) the timestamp 2 byte (u16) number of tags (key value pairs) repetitions of: 2 byte (u16) length of the tag name in bytes n byte (utf8) tag name (utf8 encoded string) 2 byte (u16) length of tag value in bytes n byte (utf8) tag value (utf8 encoded string) 2 byte (u16) number of fiends (key value pairs) repetition of: 2 byte (u16) length of the tag name in bytes n byte (utf8) tag name (utf8 encoded string) 1 byte (tag) type of the field value can be one of: TYPE_I64 = 0 followed by 8 byte (i64) TYPE_F64 = 1 followed by 8 byte (f64) TYPE_TRUE = 2 no following data TYPE_FALSE = 3 no following data TYPE_STRING = 4 followed by 2 byte (u16) length of the string in bytes and n byte string value (utf8 encoded string) statsd \u00b6 The same as the influx, the statsd codec translates a single statsd measurement into a structured format. The structure is as follows: sam:7|c|@0.1 Translates to: { \"type\" : \"c\" , \"metric\" : \"sam\" , \"value\" : 7 , \"sample_rate\" : 0.1 } The following types are supported: c for counter ms for timing g for gauge h for histogram s for sets For gauge there is also the field action which might be add if the value was prefixed with a + , or sub if the value was prefixed with a - yaml \u00b6 En- and decodes YAML . syslog \u00b6 En- and decodes syslog messages (both, the standard IETF format and the old BSD format). A syslog message following BSD format as follows: <13>Jan 5 15:33:03 74794bfb6795 root[8539]: i am foobar get translates to: { \"severity\" : \"notice\" , \"facility\" : \"user\" , \"hostname\" : \"74794bfb6795\" , \"appname\" : \"root\" , \"msg\" : \"i am foobar\" , \"procid\" : 8539 , \"msgid\" : null , \"protocol\" : \"RFC3164\" , \"protocol_version\" : null , \"structured_data\" : null , \"timestamp\" : 1609860783000000000 } Syslog message following IETF standard as follows: <165>1 2021-03-18T20:30:00.123Z mymachine.example.com evntslog - ID47 [exampleSDID@32473 iut=\\\"3\\\" eventSource=\\\"Application\\\" eventID=\\\"1011\\\"] BOMAn application event log entry...\" get translates to: { \"severity\" : \"notice\" , \"facility\" : \"local4\" , \"hostname\" : \"mymachine.example.com\" , \"appname\" : \"evntsog\" , \"msg\" : \"BOMAn application event log entry...\" , \"procid\" : null , \"msgid\" : \"ID47\" , \"protocol\" : \"RFC5424\" , \"protocol_version\" : 1 , \"structured_data\" : { \"exampleSDID@32473\" : [ { \"iut\" : \"3\" }, { \"eventSource\" : \"Application\" }, { \"eventID\" : \"1011\" } ] }, \"timestamp\" : 1616099400123000000 } Note invalid syslog message is treated under 3164 protocol and entire string goes to the msg of result object.","title":"Codecs"},{"location":"artefacts/codecs/#codecs","text":"Codecs are used to describe how to decode data from the wire and encode it back to wire format.","title":"Codecs"},{"location":"artefacts/codecs/#supported-codecs","text":"","title":"Supported Codecs"},{"location":"artefacts/codecs/#json","text":"En- and decodes JSON , for encoding a minified format is used (excluding newlines and spaces).","title":"json"},{"location":"artefacts/codecs/#string","text":"Treats the event as non structured string. It is required that the input is valid UTF-8 or the decoding will fail.","title":"string"},{"location":"artefacts/codecs/#msgpack","text":"Msgpack works based on the msgpack binary format that is structurally compatible with JSON. Being a binary format, message pack is significantly more performant and requires less space compared to JSON. It is an excellent candidate to use in tremor to tremor deployments but as well with any offramp that does support this format.","title":"msgpack"},{"location":"artefacts/codecs/#influx","text":"En- and decodes the influx line protocol . The structural representation of the data is as follows: weather,location=us-midwest temperature=82 1465839830100400200 translates to: { \"measurement\" : \"weather\" , \"tags\" : { \"location\" : \"us-midwest\" }, \"fields\" : { \"temperature\" : 82.0 }, \"timestamp\" : 1465839830100400200 }","title":"influx"},{"location":"artefacts/codecs/#binflux","text":"The binflux codec is a binary representation of influx data that is significantly faster encodes and decodes as well as takes less space on the wire. The format itself does not include framing but can be used with the size-prefix pre/post processors. For all numbers network byte order is used (big endian). The data is represented as follows: 2 byte (u16) length of the measurement in bytes n byte (utf8) the measurement (utf8 encoded string) 8 byte (u64) the timestamp 2 byte (u16) number of tags (key value pairs) repetitions of: 2 byte (u16) length of the tag name in bytes n byte (utf8) tag name (utf8 encoded string) 2 byte (u16) length of tag value in bytes n byte (utf8) tag value (utf8 encoded string) 2 byte (u16) number of fiends (key value pairs) repetition of: 2 byte (u16) length of the tag name in bytes n byte (utf8) tag name (utf8 encoded string) 1 byte (tag) type of the field value can be one of: TYPE_I64 = 0 followed by 8 byte (i64) TYPE_F64 = 1 followed by 8 byte (f64) TYPE_TRUE = 2 no following data TYPE_FALSE = 3 no following data TYPE_STRING = 4 followed by 2 byte (u16) length of the string in bytes and n byte string value (utf8 encoded string)","title":"binflux"},{"location":"artefacts/codecs/#statsd","text":"The same as the influx, the statsd codec translates a single statsd measurement into a structured format. The structure is as follows: sam:7|c|@0.1 Translates to: { \"type\" : \"c\" , \"metric\" : \"sam\" , \"value\" : 7 , \"sample_rate\" : 0.1 } The following types are supported: c for counter ms for timing g for gauge h for histogram s for sets For gauge there is also the field action which might be add if the value was prefixed with a + , or sub if the value was prefixed with a -","title":"statsd"},{"location":"artefacts/codecs/#yaml","text":"En- and decodes YAML .","title":"yaml"},{"location":"artefacts/codecs/#syslog","text":"En- and decodes syslog messages (both, the standard IETF format and the old BSD format). A syslog message following BSD format as follows: <13>Jan 5 15:33:03 74794bfb6795 root[8539]: i am foobar get translates to: { \"severity\" : \"notice\" , \"facility\" : \"user\" , \"hostname\" : \"74794bfb6795\" , \"appname\" : \"root\" , \"msg\" : \"i am foobar\" , \"procid\" : 8539 , \"msgid\" : null , \"protocol\" : \"RFC3164\" , \"protocol_version\" : null , \"structured_data\" : null , \"timestamp\" : 1609860783000000000 } Syslog message following IETF standard as follows: <165>1 2021-03-18T20:30:00.123Z mymachine.example.com evntslog - ID47 [exampleSDID@32473 iut=\\\"3\\\" eventSource=\\\"Application\\\" eventID=\\\"1011\\\"] BOMAn application event log entry...\" get translates to: { \"severity\" : \"notice\" , \"facility\" : \"local4\" , \"hostname\" : \"mymachine.example.com\" , \"appname\" : \"evntsog\" , \"msg\" : \"BOMAn application event log entry...\" , \"procid\" : null , \"msgid\" : \"ID47\" , \"protocol\" : \"RFC5424\" , \"protocol_version\" : 1 , \"structured_data\" : { \"exampleSDID@32473\" : [ { \"iut\" : \"3\" }, { \"eventSource\" : \"Application\" }, { \"eventID\" : \"1011\" } ] }, \"timestamp\" : 1616099400123000000 } Note invalid syslog message is treated under 3164 protocol and entire string goes to the msg of result object.","title":"syslog"},{"location":"artefacts/offramps/","text":"Offramps \u00b6 Specify how tremor connects to the outside world in order to publish to external systems. For example, the Elastic offramp pushes data to ElasticSearch via its bulk upload REST/HTTP API endpoint. All offramps are specified in the following form: offramp : - id : <unique offramp id> type : <offramp name> codec : <codec of the data> postprocessors : # can be omitted - <postprocessor 1> - <postprocessor 2> - ... preprocessors : # only for linked transport, can be omitted - <preprocessor 1> - <preprocessor 2> - ... linked : <true or false> # enable linked transport, default: false codec_map : \"<mime-type>\" : \"<codec handling the given mime-type>\" config : <key> : <value> Delivery Properties \u00b6 Each offramp is able to report events as being definitely sent off or as failed. It can also report itself as not functional anymore to the connected pipelines. How each offramp implements those abilities is described in the table below. The column Delivery acknowledgements describes under what circumstanced the offramp considers an event delivered and acknowledges it to the connected pipelines and operators, onramps etc. therein. Acknowledgements, Failures or missing Acknowledgements take effect e.g. when using the operators or onramps that support those mechanisms (e.g. the WAL operator or the Kafka onramp). The column Disconnect events describes under which circumstances this offramp is not considered functional anymore. Offramp Disconnect events Delivery acknowledgements blackhole never always cb never always debug never always dns never always elastic connection loss on 200 replies exit never always file never always kafka see librdkafka see librdkafka kv never always nats connection loss always newrelic never never otel connection loss on successful delivery Postgres never always rest connection loss on non 4xx/5xx replies stderr never always stdout never always tcp connection loss on send udp local socket loss on send ws connection loss on send System Offramps \u00b6 Each tremor runtime comes with some pre-configured offramps that can be used. system::stdout \u00b6 The offramp /offramp/system::stdout/system can be used to print to STDOUT. Data will be formatted as JSON. system::stderr \u00b6 The offramp /offramp/system::stderr/system can be used to print to STDERR. Data will be formatted as JSON. Supported Offramps \u00b6 blackhole \u00b6 The blackhole offramp is used for benchmarking it takes measurements of the end to end times of each event traversing the pipeline and at the end prints an HDR ( High Dynamic Range ) histogram . Supported configuration options are: warmup_secs - Number of seconds after startup in which latency won't be measured to allow for a warmup delay. stop_after_secs - Stop tremor after a given number of seconds and print the histogram. significant_figures - Significant figures for the HDR histogram. (the first digits of each measurement that are kept as precise values) Example: offramp : - id : bh type : blackhole config : warmup_secs : 10 stop_after_secs : 40 cb \u00b6 cb is short for circuit breaker. This offramp can be used to trigger certain circuit breaker events manually by sending the intended circuit breaker event in the event metadata or the event data. Supported payloads are: ack fail trigger or close open or restore No matter how many ack and fail strings the cb key contains, only ever one ack or fail CB event will be emitted, to stay within the CB protocol. The same is true for trigger / close and open / restore strings, only one of those two will be emitted, never more. Example config: offramp : - id : cb_tester type : cb Example payloads: { \"cb\" : \"ack\" , \"some_other_field\" : true } { \"cb\" : [ \"fail\" , \"close\" ] } Such an event or metadata will result in two CB insight events be sent back, one fail event, and one close event. debug \u00b6 The debug offramp is used to get an overview of how many events are put in which classification. This operator does not support configuration. Used metadata variables: $class - Class of the event to count by. (optional) Example: offramp : - id : dbg type : debug dns \u00b6 The dns linked offramp allows performing DNS queries against the system resolver. Note No codecs, configuration, or processors are supported. Example: - id : dns type : dns The event needs the following structure: { \"lookup\" : \"tremor.rs\" } { \"lookup\" : { \"name\" : \"tremor.rs\" , \"type\" : \"CNAME\" } } where type can be one of (please consult your DNS manual for the meaning of each): A AAAA ANAME CNAME TXT PTR CAA HINFO HTTPS MX NAPTR NULL NS OPENPGPKEY SOA SRV SSHFP SVCB TLSA Note If type is not specified A records will be looked up Responses are an Array of objects denoting the type of record found as a key, followed by the entry as a string and a ttl for the record (please consult your DNS manual for the return value of different record types): [ { \"A\" : \"1.2.3.4\" , \"ttl\" : 60 }, { \"CNAME\" : \"www.tremor.rs\" , \"ttl\" : 120 } ] elastic \u00b6 The elastic offramp writes to one or more ElasticSearch nodes. This is currently tested with ES v6 and v7. Supported configuration options are: nodes - A list of elastic search nodes to contact. These are the nodes to which tremor will send traffic to. concurrency - Maximum number of parallel requests (default: 4). Events will be sent to the connected ElasticSearch cluster via the ES Bulk API using the index action. It is recommended to batch events sent to this sink using the generic::batch operator to reduce the overhead introduced by the ES Bulk API . The configuration options codec and postprocessors are not used, as elastic will always serialize event payloads as JSON. If the number of parallel requests surpass concurrency , an error event will be emitted to the err port, which can be used for appropriate error handling. The following metadata variables can be specified on a per event basis: $elastic._index - The index to write to (required). $elastic._type - The document type for elastic (optional), deprecated in ES 7. $elastic._id - The document id for elastic (optional). $elastic.pipeline - The ElasticSearch pipeline to use (optional). $elastic.action - The bulk action to perform, one of delete , create , update or index . If no action is provided it defaults to index . delete and update require $elastic._id to be set or elastic search will have error. Linked Transport \u00b6 If used as a linked transport, the sink will emit events in case of errors sending data to ES or if the incoming event is malformed via the err port. Also, upon a returned ES Bulk request, it will emit an event for each bulk item, denoting success or failure. Events denoting success are sent via the out port and have the following format: { \"success\" : true , \"source\" : { \"event_id\" : \"1:2:3\" , \"origin\" : \"file:///tmp/input.json.xz\" }, \"payload\" : {} } The event metadata will contain the following: { \"elastic\" : { \"_id\" : \"ES document id\" , \"_type\" : \"ES document type\" , \"_index\" : \"ES index\" , \"version\" : \"ES document version\" } } Events denoting bulk item failure are sent via the err port and have the following format: { \"success\" : false , \"source\" : { \"event_id\" : \"2:3:4\" , \"origin\" : null }, \"error\" : {}, \"payload\" : {} } error will contain the error description object returned from ES. The event metadata for failed events looks as follows: { \"elastic\" : { \"_id\" : \"ES document id\" , \"_type\" : \"ES document type\" , \"_index\" : \"ES index\" , } } For both event types payload is the event payload that was sent to ES. Example Configuration: offramp : - id : es type : elastic config : nodes : - http://elastic:9200 Example Configuration for linked transport (including binding): offramp : - id : es-linked type : elastic linked : true config : nodes : - http://elastic1:9200 - http://elastic2:9200 concurrency : 8 binding : id : \"es-linked-binding\" links : \"/onramp/example/{instance}/out\" : [ \"/pipeline/to_elastic/{instance}/in\" ] \"/pipeline/to_elastic/{instance}/in\" : [ \"/offramp/es-linked/{instance}/in\" ] # handle success and error messages with different pipelines \"/offramp/es-linked/{instance}/out\" : [ \"/pipeline/handle-es-success/{instance}/in\" ] \"/offramp/es-linked/{instance}/err\" : [ \"/pipeline/handle-es-error/{instance}/in\" ] # more links... ... exit \u00b6 The exit offramp terminates the runtime with a system exit status. The offramp accepts events via its standard input port and responds to events with a record structure containing a numeric exit field. To indicate successful termination, an exit status of zero may be used: { \"exit\" : 0 } To indicate non-successful termination, a non-zero exit status may be used: { \"exit\" : 1 } Exit codes should follow standard UNIX/Linux guidelines when being integrated with bash or other shell-based environments, as follows: Code Meaning 0 Success 1 General errors 2 Misuse of builtins 126 Command invoked cannot run due to credentials/auth constraints 127 Command not understood, not well-formed or illegal To delay the exit (to allow flushing of other offramps ) the delay key can be used to delay the exit by a number of milliseconds: { \"exit\" : 1 , \"delay\" : 1000 } Example: offramp : - id : terminate type : exit file \u00b6 The file offramp writes events to a file, one event per line. The file is overwritten if it exists. The default codec is json . Supported configuration options are: file - The file to write to. Example: offramp : - id : in type : file config : file : /my/path/to/a/file.json Kafka \u00b6 The Kafka offramp connects sends events to Kafka topics. It uses librdkafka to handle connections and can use the full set of librdkaka 1.5.0 configuration options . The default codec is json . Supported configuration options are: topic - The topic to send to. brokers - Broker servers to connect to. (Kafka nodes) hostname - Hostname to identify the client with. (default: the systems hostname) key - Key to use for messages (default: none) rdkafka_options - An optional map of option to value, where both sides need to be strings. Used metadata variables: $kafka - Record consisting of the following meta information: $headers : A record denoting the headers for the message. $key : Same as config key (optional. overrides related config param when present) Example: offramp : - id : kafka-out type : kafka config : brokers : - kafka:9092 topic : demo kv \u00b6 The kv offramp is intended to allow for a decouple way of persisting and retrieving state in a non blocking way. Example: - id : kv type : kv linked : true # this needs to be true codec : json config : dir : \"temp/kv\" # directory to store data in Events sent to the KV offramp are commands. The following are supported: get \u00b6 Fetches the data for a given key. Request : { \"get\" : { \"key\" : \"<string|binary>\" }} Response : { \"ok\" : \"<decoded>\" } // key was found, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not found put \u00b6 Writes a value to a key, returns the old value if there was any. Request : { \"put\" : { \"key\" : \"<string|binary>\" , \"value\" : \"<to encode>\" // the format of value depends on the codec (does NOT have to be a string) }} Response : { \"ok\" : \"<decoded>\" } // key was used before, this is the old value, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not used before delete \u00b6 Deletes a key, returns the old value if there was any. Request : { \"delete\" : { \"key\" : \"<string|binary>\" }} Response : { \"ok\" : \"<decoded>\" } // key was used before, this is the old value, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not used before scan \u00b6 Reads a range of keys Request : { \"scan\" : { \"start\" : \"<string|binary>\" , // optional, if not set will start with the first key \"end\" : \"<string|binary>\" , // optional, if not set will read to the end key }} Response : { \"ok\" : [ { \"key\" : \"<binary>\" , // keys are ALWAYS encoded as binary since we don't know if it's a string or binary \"value\" : \"<decoded>\" // the value, the format of decoded depends on the codec (does NOT have to be a string) } // repeated, may be empty ]} cas \u00b6 Compare And Swap operation. Those operations require old values to match what it is compared to Request : { \"cas\" : { \"key\" : \"<string|binary>\" , // The key to operate on \"old\" : \"<to encode|not-set>\" , // The old value, if not set means \"this value wasn't present\" \"new\" : \"<to encode|not-set>\" , // The new value, if not set it means it gets deleted }} Response : { \"ok\" : null } // The operation succeeded { \"error\" : { // the operation failed \"current\" : \"<decoded>\" , // the value that is currently stored, the format of decoded depends on the codec (does NOT have to be a string) \"proposed\" : \"<decoded>\" // the value that was proposed/expected to be there, the format of decoded depends on the codec (does NOT have to be a string) }} nats \u00b6 The nats offramp connects to Nats server(s) and publishes a message to specified subject for every event. The default codec is json . Supported configuration operations are: hosts - List of hosts to connect to. subject - Subject for the message to be published. reply - Optional string specifying the reply subject. headers - Option key-value pairs, specifying message headers, where the key is a string and the value is a list of strings. options - Optional struct, which can be used to customize the connection to the server (see nats.rs configuration options for more info): token : String; authenticate using a token. username : String; authenticate using a username and password. password : String; authenticate using a username and password. credentials_path : String; path to a .creds file for authentication. cert_path : String; path to the client certificate file. key_path : String; path to private key file. name : String; name this configuration. echo : Boolean; if true, published messages will not be delivered. max_reconnects : Integer; max number of reconnection attempts. reconnect_buffer_size : Integer; max amount of bytes to buffer when accepting outgoing traffic in disconnected mode. tls : Boolean; if true, sets tls for all server connections. root_cert : String; path to a root certificate. Used metadata variables are: $nats : Record consisting of the following metadata: $reply : Overrides config.reply if present. $headers : Overrides config.headers if present. Example: offramp : - id : nats-out type : nats config : hosts : - \"127.0.0.1:4444\" subject : demo reply : ghost headers : snot : - badger - ferris options : name : nats-demo reconnect_buffer_size : 1 newrelic \u00b6 Send events to New Relic platform, using its log apis (variable by region). This offramp encodes events as json, as this is required by the newrelic log api. Postprocessors are not used. Supported configuration options are: license_key - New Relic's license (or insert only) key compress_logs - Whether logs should be compressed before sending to New Relic (avoids extra egress costs but at the cost of more cpu usage by tremor) (default: false) region - Region to use to send logs. Available choices: USA, Europe (default: USA) Example: offramp : - id : newrelic type : newrelic config : license_key : keystring compress_logs : true region : europe otel \u00b6 CNCF OpenTelemetry offramp. Publishes to the specified host or IP and destination TCP port via gRPC messages conforming to the CNCF OpenTelemetry protocol specification v1. Forwards tremor value variants of logs , trace and metrics messages from tremor query pipelines downstream to remote OpenTelemetry endpoints. Note The offramp is experimental. Supported configuration options are: host - String - The host or IP to listen on port - integer - The TCP port to listen on 'logs' - boolean - Is logging enabled for this instance. Defaults to true . Received logs events are dropped when false . 'metrics' - boolean - Is metrics enabled for this instance. Defaults to true . Defaults to true . Received metrics events are dropped when false . 'trace' - boolean - Is trace enabled for this instance. Defaults to true . Defaults to true . Received trace events are dopped when false . Pipelines that leverage the OpenTelemetry integration can use utility modules in the cncf::otel module to simplify working with the tremor value mapping of the event data. The connector translates the tremor value level data to protocol buffers automatically for distribution to downstream OpenTelemetry systems. The connector can be used with the qos::wal operator for transient in-memory or persistent disk-based guaranteed delivery. If either tremor or the downstream system fails or becomes uncontactable users can configure ( bytes and/or number of messages retained ) retention for lossless recovery. For events marked as transactional that are explicitly acknowledged, fail insights are propagated for events that are not succesfully transmitted downstream. Non-transactional events ( those not marked as transactional ) are delivered on a best effort basis. Regardless of the transaction configuration, when paired with qos operators upstream pipelines, the sink will coordinate failover and recovery to the configured retention, replaying the retained messages upon recovery of network accessibility of the downstream endpoints. For best effort delivery - the qos::wal can be omitted and events distributed when downstream endpoints are inaccessible will be lost. Example: offramp : - id : otlp type : otel codec : json config : port : 4317 host : 10.0.2.1 PostgreSQL \u00b6 PostgreSQL offramp. Supported configuration options are: host - PostgreSQL database hostname port - PostgresSQL database port user - Username for authentication password - Password for authentication dbname - Database name table - Database table name Data that comes in will be used to run an INSERT statement. It is required that data comes in objects representing columns. The object key must represent field name in the database and must contain following fields: fieldType - a PostgreSQL field type (e.g. VARCHAR , INT4 , TIMESTAMPTZ , etc.) name - field name as represented by database table schema value - the value of the field codec and postprocessors config values are ignored as they cannot apply to this offramp, since the event is transformed into a SQL query. Example: id: db type: postgres config: host: localhost port: 5432 user: postgres password: example dbname: sales table: transactions rest \u00b6 The rest offramp is used to send events to the specified endpoint. Supported configuration options are: endpoint - Endpoint URL. Can be provided as string or as struct. The struct form is composed of the following standard URL fields: scheme - String, required, typically http username - String, optional password - String, optional host - String, required, hostname or ip address port - Number, optional, defaults to 80 path - String, optional, defaults to / query - String, optional fragment - String, optional method - HTTP method to use (default: POST ) headers - A map of headers to set for the requests, where both sides are strings concurrency - Number of parallel in-flight requests (default: 4 ) Used metadata variables: Setting these metadata variables here allows users to dynamically change the behaviour of the rest offramp: $endpoint - same format as config endpoint (optional. overrides related config param when present) $request - A record capturing the HTTP request attributes. Available fields within: method - same as config method (optional. overrides related config param when present) headers - A map from header name (string) to header value (string or array of strings)(optional. overrides related config param when present) These variables are aligned with the similar variables generated by the rest onramp . Note that $request.url is not utilized here -- instead the same can be configured by setting $endpoint (since enabling the former here can lead to request loops when events sourced from rest onramp are fed to the rest offramp, without overriding it from the pipeline. also, $endpoint can be separately configured as a URL string, which is convenient for simple offramp use). The rest offramp encodes the event as request body using the Content-Type header if present, using the customizable builtin codec_map to determine a matching coded. It falls back to use the configured codec if no Content-Type header is available. When used as Linked Transport the same handling is applied to the incoming HTTP response, giving precedence to the Content-Type header and only falling back to the configured codec . If the number of parallel requests surpass concurrency , an error event will be emitted to the err port, which can be used for appropriate error handling. Set metadata variables: These metadata variables are used for HTTP response events emitted through the OUT port: $response - A record capturing the HTTP response attributes. Available fields within: status - Numeric HTTP status code headers - A record that maps header name (lowercase string) to value (array of strings) $request - A record with the related HTTP request attributes. Available fields within: method - HTTP method used headers - A record containing all the request headers endpoint - A record containing all the fields that config endpoint does When used as a linked offramp, batched events are rejected by the offramp. Example 1: offramp : - id : rest-offramp type : rest codec : json postprocessors : - gzip linked : true config : endpoint : host : httpbin.org port : 80 path : /anything query : \"q=search\" headers : \"Accept\" : \"application/json\" \"Transfer-Encoding\" : \"gzip\" Example 2: offramp : - id : rest-offramp-2 type : rest codec : json codec_map : \"text/html\" : \"string\" \"application/vnd.stuff\" : \"string\" config : endpoint : host : httpbin.org path : /patch method : PATCH rest offramp example for InfluxDB \u00b6 The structure is given for context. offramp : - id : influxdb type : rest codec : influx postprocessors : - lines config : endpoint : host : influx path : /write query : db=metrics headers : \"Client\" : \"Tremor\" stderr \u00b6 A custom stderr offramp can be configured by using this offramp type. But beware that this will share the single stderr stream with system::stderr . The default codec is json . The stderr offramp will write a \\n right after each event, and optionally prefix every event with a configurable prefix . If the event data (after codec and postprocessing) is not a valid UTF8 string (e.g. if it is binary data) if will by default output the bytes with debug formatting. If raw is set to true, the event data will be put on stderr as is. Supported configuration options: prefix - A prefix written before each event (optional string). raw - Write event data bytes as is to stderr. Example: offramp : - id : raw_stuff type : stderr codec : json postprocessors : - snappy config : raw : true stdout \u00b6 A custom stdout offramp can be configured by using this offramp type. But beware that this will share the single stdout stream with system::stdout . The default codec is json . The stdout offramp will write a \\n right after each event, and optionally prefix every event with a configurable prefix . If the event data (after codec and postprocessing) is not a valid UTF8 string (e.g. if it is binary data) if will by default output the bytes with debug formatting. If raw is set to true, the event data will be put on stdout as is. Supported configuration options: prefix - A prefix written before each event (optional string). raw - Write event data bytes as is to stdout. Example: offramp : - id : like_a_python_repl type : stdout config : prefix : \">>> \" tcp \u00b6 This connects on a specified port for distributing outbound TCP data. The offramp can leverage postprocessors to frame data after codecs are applied and events are forwarded to external TCP protocol distribution endpoints. The default codec is json . Supported configuration options are: host - The host to advertise as port - The TCP port to listen on is_non_blocking - Is the socket configured as non-blocking ( default: false ) ttl - Set the socket's time-to-live ( default: 64 ) is_no_delay - Set the socket's Nagle ( delay ) algorithm to off ( default: true ) Example: offramp : - id : tcp type : tcp codec : json postprocessors : - gzip - base64 - lines config : host : \"localhost\" port : 9000 udp \u00b6 The UDP offramp sends data to a given host and port as UDP datagram. The default codec is json . When the UDP onramp gets a batch of messages it will send each element of the batch as a distinct UDP datagram. Supported configuration options are: bind.host - the local host to send data from bind.port - the local port to send data from host - the destination host to send data to port - the destination port to send data to. Warn Setting bound to false makes the UDP offramp potentially extremely slow as it forces a lookup of the destination on each event! Used metadata variables: $udp.host : This overwrites the configured destination host for this event. Expects a string. $udp.port : This overwrites the configured destination port for this event. Expects an integer. Warn Be careful to set $udp.host to an IP, not a DNS name or the OS will resolve it on every event, which will be extremely slow! Example: offramp : - id : udp-out type : udp postprocessors : - base64 config : bind : host : \"10.11.12.13\" port : 1234 host : \"20.21.22.23\" port : 2345 ws \u00b6 Sends events over a WebSocket connection. Each event is a WebSocket message. The default codec is json . Supported configuration options are: url - WebSocket endpoint to send data to. binary - If data should be send as binary instead of text (default: false ). Used metadata variables: $url - same as config url (optional. overrides related config param when present) $binary - same as config binary (optional. overrides related config param when present) Set metadata variables (for reply with linked transports ): $binary - true if the WebSocket message reply came as binary ( false otherwise) When used as a linked offramp, batched events are rejected by the offramp. Example: onramp : - id : ws type : ws config : url : \"ws://localhost:1234\" gcs \u00b6 Google Cloud Storage offramp. This offramp can issue basic operations to list buckets and objects and to create, insert and delete objects from the Google Cloud Platform cloud storage service. Note The offramp is experimental. This offramp assumes that the environment variable GOOGLE_APPLICATION_CREDENTIALS been exported to the execution environment and it has been configured to point to a valid non-expired service account token json file. Example: offramp : - id : gcs type : gcs codec : json linked : true postprocessors : - gzip preprocessors : - gzip If the use case for the offramp requires metadata from the Google Cloud Storage service on the supported operations for this offramp, then set linked to true and configure the output port out . If the use case does not require metadata, then set linked to false. list_buckets \u00b6 Lists all the buckets in the Google Cloud Storage project. Request : { \"command\" : \"list_buckets\" , \"project_id\" : \"<project-id>\" } Response : [ { \"cmd\" : \"list_buckets\" , \"data\" : { \"items\" : [ { \"location\" : \"EU\" , \"id\" : \"<bucket-id>\" , \"locationType\" : \"multi-region\" , \"storageClass\" : \"STANDARD\" , \"metageneration\" : \"4\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-id>\" , \"kind\" : \"storage#bucket\" , \"name\" : \"<bucket-name>\" , \"timeCreated\" : \"<time of creation>\" , \"iamConfiguration\" : { \"uniformBucketLevelAccess\" : { \"enabled\" : true , \"lockedTime\" : \"2021-06-17T09:12:22.122Z\" }, \"bucketPolicyOnly\" : { \"enabled\" : true , \"lockedTime\" : \"2021-06-17T09:12:22.122Z\" } }, \"etag\" : \"CAQ=\" , \"defaultEventBasedHold\" : false , \"retentionPolicy\" : { \"effectiveTime\" : \"2021-03-19T09:12:22.122Z\" , \"retentionPeriod\" : \"60\" , \"isLocked\" : false }, \"updated\" : \"2021-04-13T17:05:29.404Z\" , \"satisfiesPZS\" : false , \"projectNumber\" : \"<project-number>\" } ], \"kind\" : \"storage#buckets\" } } ] Where - <project-id> - The project id of the Google Cloud Storage project where the buckets are. - <bucket-id> - The unique identifier of the bucket. - <project-number> - The project number for the Google Cloud Storage project. list_objects \u00b6 Lists all the objects in the specified bucket. Request : { \"command\" : \"list_objects\" , \"bucket\" : \"<bucket-name>\" } Response : [ { \"cmd\" : \"list_objects\" , \"data\" : { \"items\" : [ { \"mediaLink\" : \"\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"1943550\" , \"crc32c\" : \"0QXXmA==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1616145246990906\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-03-19T09:14:07.012Z\" , \"selfLink\" : \"\" , \"kind\" : \"storage#object\" , \"name\" : \"<object-name>\" , \"md5Hash\" : \"ucrP5b1N+6JHxn1TKavn/A==\" , \"timeCreated\" : \"2021-03-19T09:14:07.012Z\" , \"etag\" : \"CLrk6JqCvO8CEAE=\" , \"updated\" : \"2021-03-19T09:14:07.012Z\" , \"retentionExpirationTime\" : \"2021-03-19T09:15:07.012Z\" , \"contentType\" : \"<content-type>\" } ], \"kind\" : \"storage#objects\" } } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. - <content-type> - The type of the object uploaded. create_bucket \u00b6 Creates a bucket in the project specified in the command. Request : { \"command\" : \"create_bucket\" , \"project_id\" : \"<project-id>\" , \"bucket\" : \"<bucket>\" } Response : [ { \"cmd\" : \"create_bucket\" , \"data\" : { \"projectNumber\" : \"<project-number>\" , \"location\" : \"US\" , \"id\" : \"<bucket-id>\" , \"etag\" : \"CAE=\" , \"locationType\" : \"multi-region\" , \"storageClass\" : \"STANDARD\" , \"metageneration\" : \"1\" , \"updated\" : \"2021-04-22T07:37:23.702Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>\" , \"kind\" : \"storage#bucket\" , \"name\" : \"tremor\" , \"iamConfiguration\" : { \"uniformBucketLevelAccess\" : { \"enabled\" : false }, \"bucketPolicyOnly\" : { \"enabled\" : false } }, \"timeCreated\" : \"2021-04-22T07:37:23.702Z\" } } ] Where - <project-id> - The project id of the Google Cloud Storage project where the bucket is. - <bucket-id> - The unique identifier of the bucket. - <project-number> - The project number for the Google Cloud Storage project. remove_bucket \u00b6 Removes the bucket from the specified project in Google Cloud Storage project. Request : { \"command\" : \"remove_bucket\" , \"bucket\" : \"<bucket-name>\" } Response : Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. upload_object \u00b6 Uploads the object to a Google Cloud Storage bucket. Request : { \"command\" : \"upload_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" , \"body\" : `<object>` } Response : [ { \"cmd\" : \"upload_object\" , \"data\" : { \"mediaLink\" : \"https://storage.googleapis.com/download/storage/v1/b/<bucket-name>/o/<object-name>?generation=1619077199260663&alt=media\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"47\" , \"crc32c\" : \"a7mrxw==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1619077199260663\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-04-22T07:39:59.278Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>/o/<object-name>\" , \"kind\" : \"storage#object\" , \"name\" : \"april\" , \"md5Hash\" : \"qhr9326j+3PeIK9koXMHCg==\" , \"timeCreated\" : \"2021-04-22T07:39:59.278Z\" , \"etag\" : \"CPfXzcqskfACEAE=\" , \"updated\" : \"2021-04-22T07:39:59.278Z\" , \"retentionExpirationTime\" : \"2021-04-22T07:40:59.278Z\" , \"contentType\" : \"application/json\" } } ] Where - <object> - The object data to be uploaded to a Google Cloud Storage bucket. - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. fetch_object \u00b6 Returns the metadata for the object fetched. Request : { \"command\" : \"fetch\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : [ { \"cmd\" : \"fetch\" , \"data\" : { \"mediaLink\" : \"https://storage.googleapis.com/download/storage/v1/b/<bucket-name>/o/<object-name>?generation=1619077199260663&alt=media\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"47\" , \"crc32c\" : \"a7mrxw==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1619077199260663\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-04-22T07:39:59.278Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>/o/<object-name>\" , \"kind\" : \"storage#object\" , \"name\" : \"april\" , \"md5Hash\" : \"qhr9326j+3PeIK9koXMHCg==\" , \"timeCreated\" : \"2021-04-22T07:39:59.278Z\" , \"etag\" : \"CPfXzcqskfACEAE=\" , \"updated\" : \"2021-04-22T07:39:59.278Z\" , \"retentionExpirationTime\" : \"2021-04-22T07:40:59.278Z\" , \"contentType\" : \"application/json\" } } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. download_object \u00b6 Downloads the object. Request : { \"command\" : \"download_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : [ { \"cmd\" : \"download_object\" , \"data\" : `<object>` } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. - <object> - The object downloaded. remove_object \u00b6 Removes the object from the specified bucket. Request : { \"command\" : \"remove_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-name> - The name of the object in the Google Cloud Stoarge bucket.","title":"Offramps"},{"location":"artefacts/offramps/#offramps","text":"Specify how tremor connects to the outside world in order to publish to external systems. For example, the Elastic offramp pushes data to ElasticSearch via its bulk upload REST/HTTP API endpoint. All offramps are specified in the following form: offramp : - id : <unique offramp id> type : <offramp name> codec : <codec of the data> postprocessors : # can be omitted - <postprocessor 1> - <postprocessor 2> - ... preprocessors : # only for linked transport, can be omitted - <preprocessor 1> - <preprocessor 2> - ... linked : <true or false> # enable linked transport, default: false codec_map : \"<mime-type>\" : \"<codec handling the given mime-type>\" config : <key> : <value>","title":"Offramps"},{"location":"artefacts/offramps/#delivery-properties","text":"Each offramp is able to report events as being definitely sent off or as failed. It can also report itself as not functional anymore to the connected pipelines. How each offramp implements those abilities is described in the table below. The column Delivery acknowledgements describes under what circumstanced the offramp considers an event delivered and acknowledges it to the connected pipelines and operators, onramps etc. therein. Acknowledgements, Failures or missing Acknowledgements take effect e.g. when using the operators or onramps that support those mechanisms (e.g. the WAL operator or the Kafka onramp). The column Disconnect events describes under which circumstances this offramp is not considered functional anymore. Offramp Disconnect events Delivery acknowledgements blackhole never always cb never always debug never always dns never always elastic connection loss on 200 replies exit never always file never always kafka see librdkafka see librdkafka kv never always nats connection loss always newrelic never never otel connection loss on successful delivery Postgres never always rest connection loss on non 4xx/5xx replies stderr never always stdout never always tcp connection loss on send udp local socket loss on send ws connection loss on send","title":"Delivery Properties"},{"location":"artefacts/offramps/#system-offramps","text":"Each tremor runtime comes with some pre-configured offramps that can be used.","title":"System Offramps"},{"location":"artefacts/offramps/#systemstdout","text":"The offramp /offramp/system::stdout/system can be used to print to STDOUT. Data will be formatted as JSON.","title":"system::stdout"},{"location":"artefacts/offramps/#systemstderr","text":"The offramp /offramp/system::stderr/system can be used to print to STDERR. Data will be formatted as JSON.","title":"system::stderr"},{"location":"artefacts/offramps/#supported-offramps","text":"","title":"Supported Offramps"},{"location":"artefacts/offramps/#blackhole","text":"The blackhole offramp is used for benchmarking it takes measurements of the end to end times of each event traversing the pipeline and at the end prints an HDR ( High Dynamic Range ) histogram . Supported configuration options are: warmup_secs - Number of seconds after startup in which latency won't be measured to allow for a warmup delay. stop_after_secs - Stop tremor after a given number of seconds and print the histogram. significant_figures - Significant figures for the HDR histogram. (the first digits of each measurement that are kept as precise values) Example: offramp : - id : bh type : blackhole config : warmup_secs : 10 stop_after_secs : 40","title":"blackhole"},{"location":"artefacts/offramps/#cb","text":"cb is short for circuit breaker. This offramp can be used to trigger certain circuit breaker events manually by sending the intended circuit breaker event in the event metadata or the event data. Supported payloads are: ack fail trigger or close open or restore No matter how many ack and fail strings the cb key contains, only ever one ack or fail CB event will be emitted, to stay within the CB protocol. The same is true for trigger / close and open / restore strings, only one of those two will be emitted, never more. Example config: offramp : - id : cb_tester type : cb Example payloads: { \"cb\" : \"ack\" , \"some_other_field\" : true } { \"cb\" : [ \"fail\" , \"close\" ] } Such an event or metadata will result in two CB insight events be sent back, one fail event, and one close event.","title":"cb"},{"location":"artefacts/offramps/#debug","text":"The debug offramp is used to get an overview of how many events are put in which classification. This operator does not support configuration. Used metadata variables: $class - Class of the event to count by. (optional) Example: offramp : - id : dbg type : debug","title":"debug"},{"location":"artefacts/offramps/#dns","text":"The dns linked offramp allows performing DNS queries against the system resolver. Note No codecs, configuration, or processors are supported. Example: - id : dns type : dns The event needs the following structure: { \"lookup\" : \"tremor.rs\" } { \"lookup\" : { \"name\" : \"tremor.rs\" , \"type\" : \"CNAME\" } } where type can be one of (please consult your DNS manual for the meaning of each): A AAAA ANAME CNAME TXT PTR CAA HINFO HTTPS MX NAPTR NULL NS OPENPGPKEY SOA SRV SSHFP SVCB TLSA Note If type is not specified A records will be looked up Responses are an Array of objects denoting the type of record found as a key, followed by the entry as a string and a ttl for the record (please consult your DNS manual for the return value of different record types): [ { \"A\" : \"1.2.3.4\" , \"ttl\" : 60 }, { \"CNAME\" : \"www.tremor.rs\" , \"ttl\" : 120 } ]","title":"dns"},{"location":"artefacts/offramps/#elastic","text":"The elastic offramp writes to one or more ElasticSearch nodes. This is currently tested with ES v6 and v7. Supported configuration options are: nodes - A list of elastic search nodes to contact. These are the nodes to which tremor will send traffic to. concurrency - Maximum number of parallel requests (default: 4). Events will be sent to the connected ElasticSearch cluster via the ES Bulk API using the index action. It is recommended to batch events sent to this sink using the generic::batch operator to reduce the overhead introduced by the ES Bulk API . The configuration options codec and postprocessors are not used, as elastic will always serialize event payloads as JSON. If the number of parallel requests surpass concurrency , an error event will be emitted to the err port, which can be used for appropriate error handling. The following metadata variables can be specified on a per event basis: $elastic._index - The index to write to (required). $elastic._type - The document type for elastic (optional), deprecated in ES 7. $elastic._id - The document id for elastic (optional). $elastic.pipeline - The ElasticSearch pipeline to use (optional). $elastic.action - The bulk action to perform, one of delete , create , update or index . If no action is provided it defaults to index . delete and update require $elastic._id to be set or elastic search will have error.","title":"elastic"},{"location":"artefacts/offramps/#linked-transport","text":"If used as a linked transport, the sink will emit events in case of errors sending data to ES or if the incoming event is malformed via the err port. Also, upon a returned ES Bulk request, it will emit an event for each bulk item, denoting success or failure. Events denoting success are sent via the out port and have the following format: { \"success\" : true , \"source\" : { \"event_id\" : \"1:2:3\" , \"origin\" : \"file:///tmp/input.json.xz\" }, \"payload\" : {} } The event metadata will contain the following: { \"elastic\" : { \"_id\" : \"ES document id\" , \"_type\" : \"ES document type\" , \"_index\" : \"ES index\" , \"version\" : \"ES document version\" } } Events denoting bulk item failure are sent via the err port and have the following format: { \"success\" : false , \"source\" : { \"event_id\" : \"2:3:4\" , \"origin\" : null }, \"error\" : {}, \"payload\" : {} } error will contain the error description object returned from ES. The event metadata for failed events looks as follows: { \"elastic\" : { \"_id\" : \"ES document id\" , \"_type\" : \"ES document type\" , \"_index\" : \"ES index\" , } } For both event types payload is the event payload that was sent to ES. Example Configuration: offramp : - id : es type : elastic config : nodes : - http://elastic:9200 Example Configuration for linked transport (including binding): offramp : - id : es-linked type : elastic linked : true config : nodes : - http://elastic1:9200 - http://elastic2:9200 concurrency : 8 binding : id : \"es-linked-binding\" links : \"/onramp/example/{instance}/out\" : [ \"/pipeline/to_elastic/{instance}/in\" ] \"/pipeline/to_elastic/{instance}/in\" : [ \"/offramp/es-linked/{instance}/in\" ] # handle success and error messages with different pipelines \"/offramp/es-linked/{instance}/out\" : [ \"/pipeline/handle-es-success/{instance}/in\" ] \"/offramp/es-linked/{instance}/err\" : [ \"/pipeline/handle-es-error/{instance}/in\" ] # more links... ...","title":"Linked Transport"},{"location":"artefacts/offramps/#exit","text":"The exit offramp terminates the runtime with a system exit status. The offramp accepts events via its standard input port and responds to events with a record structure containing a numeric exit field. To indicate successful termination, an exit status of zero may be used: { \"exit\" : 0 } To indicate non-successful termination, a non-zero exit status may be used: { \"exit\" : 1 } Exit codes should follow standard UNIX/Linux guidelines when being integrated with bash or other shell-based environments, as follows: Code Meaning 0 Success 1 General errors 2 Misuse of builtins 126 Command invoked cannot run due to credentials/auth constraints 127 Command not understood, not well-formed or illegal To delay the exit (to allow flushing of other offramps ) the delay key can be used to delay the exit by a number of milliseconds: { \"exit\" : 1 , \"delay\" : 1000 } Example: offramp : - id : terminate type : exit","title":"exit"},{"location":"artefacts/offramps/#file","text":"The file offramp writes events to a file, one event per line. The file is overwritten if it exists. The default codec is json . Supported configuration options are: file - The file to write to. Example: offramp : - id : in type : file config : file : /my/path/to/a/file.json","title":"file"},{"location":"artefacts/offramps/#kafka","text":"The Kafka offramp connects sends events to Kafka topics. It uses librdkafka to handle connections and can use the full set of librdkaka 1.5.0 configuration options . The default codec is json . Supported configuration options are: topic - The topic to send to. brokers - Broker servers to connect to. (Kafka nodes) hostname - Hostname to identify the client with. (default: the systems hostname) key - Key to use for messages (default: none) rdkafka_options - An optional map of option to value, where both sides need to be strings. Used metadata variables: $kafka - Record consisting of the following meta information: $headers : A record denoting the headers for the message. $key : Same as config key (optional. overrides related config param when present) Example: offramp : - id : kafka-out type : kafka config : brokers : - kafka:9092 topic : demo","title":"Kafka"},{"location":"artefacts/offramps/#kv","text":"The kv offramp is intended to allow for a decouple way of persisting and retrieving state in a non blocking way. Example: - id : kv type : kv linked : true # this needs to be true codec : json config : dir : \"temp/kv\" # directory to store data in Events sent to the KV offramp are commands. The following are supported:","title":"kv"},{"location":"artefacts/offramps/#get","text":"Fetches the data for a given key. Request : { \"get\" : { \"key\" : \"<string|binary>\" }} Response : { \"ok\" : \"<decoded>\" } // key was found, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not found","title":"get"},{"location":"artefacts/offramps/#put","text":"Writes a value to a key, returns the old value if there was any. Request : { \"put\" : { \"key\" : \"<string|binary>\" , \"value\" : \"<to encode>\" // the format of value depends on the codec (does NOT have to be a string) }} Response : { \"ok\" : \"<decoded>\" } // key was used before, this is the old value, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not used before","title":"put"},{"location":"artefacts/offramps/#delete","text":"Deletes a key, returns the old value if there was any. Request : { \"delete\" : { \"key\" : \"<string|binary>\" }} Response : { \"ok\" : \"<decoded>\" } // key was used before, this is the old value, the format of decoded depends on the codec (does NOT have to be a string) null , // key was not used before","title":"delete"},{"location":"artefacts/offramps/#scan","text":"Reads a range of keys Request : { \"scan\" : { \"start\" : \"<string|binary>\" , // optional, if not set will start with the first key \"end\" : \"<string|binary>\" , // optional, if not set will read to the end key }} Response : { \"ok\" : [ { \"key\" : \"<binary>\" , // keys are ALWAYS encoded as binary since we don't know if it's a string or binary \"value\" : \"<decoded>\" // the value, the format of decoded depends on the codec (does NOT have to be a string) } // repeated, may be empty ]}","title":"scan"},{"location":"artefacts/offramps/#cas","text":"Compare And Swap operation. Those operations require old values to match what it is compared to Request : { \"cas\" : { \"key\" : \"<string|binary>\" , // The key to operate on \"old\" : \"<to encode|not-set>\" , // The old value, if not set means \"this value wasn't present\" \"new\" : \"<to encode|not-set>\" , // The new value, if not set it means it gets deleted }} Response : { \"ok\" : null } // The operation succeeded { \"error\" : { // the operation failed \"current\" : \"<decoded>\" , // the value that is currently stored, the format of decoded depends on the codec (does NOT have to be a string) \"proposed\" : \"<decoded>\" // the value that was proposed/expected to be there, the format of decoded depends on the codec (does NOT have to be a string) }}","title":"cas"},{"location":"artefacts/offramps/#nats","text":"The nats offramp connects to Nats server(s) and publishes a message to specified subject for every event. The default codec is json . Supported configuration operations are: hosts - List of hosts to connect to. subject - Subject for the message to be published. reply - Optional string specifying the reply subject. headers - Option key-value pairs, specifying message headers, where the key is a string and the value is a list of strings. options - Optional struct, which can be used to customize the connection to the server (see nats.rs configuration options for more info): token : String; authenticate using a token. username : String; authenticate using a username and password. password : String; authenticate using a username and password. credentials_path : String; path to a .creds file for authentication. cert_path : String; path to the client certificate file. key_path : String; path to private key file. name : String; name this configuration. echo : Boolean; if true, published messages will not be delivered. max_reconnects : Integer; max number of reconnection attempts. reconnect_buffer_size : Integer; max amount of bytes to buffer when accepting outgoing traffic in disconnected mode. tls : Boolean; if true, sets tls for all server connections. root_cert : String; path to a root certificate. Used metadata variables are: $nats : Record consisting of the following metadata: $reply : Overrides config.reply if present. $headers : Overrides config.headers if present. Example: offramp : - id : nats-out type : nats config : hosts : - \"127.0.0.1:4444\" subject : demo reply : ghost headers : snot : - badger - ferris options : name : nats-demo reconnect_buffer_size : 1","title":"nats"},{"location":"artefacts/offramps/#newrelic","text":"Send events to New Relic platform, using its log apis (variable by region). This offramp encodes events as json, as this is required by the newrelic log api. Postprocessors are not used. Supported configuration options are: license_key - New Relic's license (or insert only) key compress_logs - Whether logs should be compressed before sending to New Relic (avoids extra egress costs but at the cost of more cpu usage by tremor) (default: false) region - Region to use to send logs. Available choices: USA, Europe (default: USA) Example: offramp : - id : newrelic type : newrelic config : license_key : keystring compress_logs : true region : europe","title":"newrelic"},{"location":"artefacts/offramps/#otel","text":"CNCF OpenTelemetry offramp. Publishes to the specified host or IP and destination TCP port via gRPC messages conforming to the CNCF OpenTelemetry protocol specification v1. Forwards tremor value variants of logs , trace and metrics messages from tremor query pipelines downstream to remote OpenTelemetry endpoints. Note The offramp is experimental. Supported configuration options are: host - String - The host or IP to listen on port - integer - The TCP port to listen on 'logs' - boolean - Is logging enabled for this instance. Defaults to true . Received logs events are dropped when false . 'metrics' - boolean - Is metrics enabled for this instance. Defaults to true . Defaults to true . Received metrics events are dropped when false . 'trace' - boolean - Is trace enabled for this instance. Defaults to true . Defaults to true . Received trace events are dopped when false . Pipelines that leverage the OpenTelemetry integration can use utility modules in the cncf::otel module to simplify working with the tremor value mapping of the event data. The connector translates the tremor value level data to protocol buffers automatically for distribution to downstream OpenTelemetry systems. The connector can be used with the qos::wal operator for transient in-memory or persistent disk-based guaranteed delivery. If either tremor or the downstream system fails or becomes uncontactable users can configure ( bytes and/or number of messages retained ) retention for lossless recovery. For events marked as transactional that are explicitly acknowledged, fail insights are propagated for events that are not succesfully transmitted downstream. Non-transactional events ( those not marked as transactional ) are delivered on a best effort basis. Regardless of the transaction configuration, when paired with qos operators upstream pipelines, the sink will coordinate failover and recovery to the configured retention, replaying the retained messages upon recovery of network accessibility of the downstream endpoints. For best effort delivery - the qos::wal can be omitted and events distributed when downstream endpoints are inaccessible will be lost. Example: offramp : - id : otlp type : otel codec : json config : port : 4317 host : 10.0.2.1","title":"otel"},{"location":"artefacts/offramps/#postgresql","text":"PostgreSQL offramp. Supported configuration options are: host - PostgreSQL database hostname port - PostgresSQL database port user - Username for authentication password - Password for authentication dbname - Database name table - Database table name Data that comes in will be used to run an INSERT statement. It is required that data comes in objects representing columns. The object key must represent field name in the database and must contain following fields: fieldType - a PostgreSQL field type (e.g. VARCHAR , INT4 , TIMESTAMPTZ , etc.) name - field name as represented by database table schema value - the value of the field codec and postprocessors config values are ignored as they cannot apply to this offramp, since the event is transformed into a SQL query. Example: id: db type: postgres config: host: localhost port: 5432 user: postgres password: example dbname: sales table: transactions","title":"PostgreSQL"},{"location":"artefacts/offramps/#rest","text":"The rest offramp is used to send events to the specified endpoint. Supported configuration options are: endpoint - Endpoint URL. Can be provided as string or as struct. The struct form is composed of the following standard URL fields: scheme - String, required, typically http username - String, optional password - String, optional host - String, required, hostname or ip address port - Number, optional, defaults to 80 path - String, optional, defaults to / query - String, optional fragment - String, optional method - HTTP method to use (default: POST ) headers - A map of headers to set for the requests, where both sides are strings concurrency - Number of parallel in-flight requests (default: 4 ) Used metadata variables: Setting these metadata variables here allows users to dynamically change the behaviour of the rest offramp: $endpoint - same format as config endpoint (optional. overrides related config param when present) $request - A record capturing the HTTP request attributes. Available fields within: method - same as config method (optional. overrides related config param when present) headers - A map from header name (string) to header value (string or array of strings)(optional. overrides related config param when present) These variables are aligned with the similar variables generated by the rest onramp . Note that $request.url is not utilized here -- instead the same can be configured by setting $endpoint (since enabling the former here can lead to request loops when events sourced from rest onramp are fed to the rest offramp, without overriding it from the pipeline. also, $endpoint can be separately configured as a URL string, which is convenient for simple offramp use). The rest offramp encodes the event as request body using the Content-Type header if present, using the customizable builtin codec_map to determine a matching coded. It falls back to use the configured codec if no Content-Type header is available. When used as Linked Transport the same handling is applied to the incoming HTTP response, giving precedence to the Content-Type header and only falling back to the configured codec . If the number of parallel requests surpass concurrency , an error event will be emitted to the err port, which can be used for appropriate error handling. Set metadata variables: These metadata variables are used for HTTP response events emitted through the OUT port: $response - A record capturing the HTTP response attributes. Available fields within: status - Numeric HTTP status code headers - A record that maps header name (lowercase string) to value (array of strings) $request - A record with the related HTTP request attributes. Available fields within: method - HTTP method used headers - A record containing all the request headers endpoint - A record containing all the fields that config endpoint does When used as a linked offramp, batched events are rejected by the offramp. Example 1: offramp : - id : rest-offramp type : rest codec : json postprocessors : - gzip linked : true config : endpoint : host : httpbin.org port : 80 path : /anything query : \"q=search\" headers : \"Accept\" : \"application/json\" \"Transfer-Encoding\" : \"gzip\" Example 2: offramp : - id : rest-offramp-2 type : rest codec : json codec_map : \"text/html\" : \"string\" \"application/vnd.stuff\" : \"string\" config : endpoint : host : httpbin.org path : /patch method : PATCH","title":"rest"},{"location":"artefacts/offramps/#rest-offramp-example-for-influxdb","text":"The structure is given for context. offramp : - id : influxdb type : rest codec : influx postprocessors : - lines config : endpoint : host : influx path : /write query : db=metrics headers : \"Client\" : \"Tremor\"","title":"rest offramp example for InfluxDB"},{"location":"artefacts/offramps/#stderr","text":"A custom stderr offramp can be configured by using this offramp type. But beware that this will share the single stderr stream with system::stderr . The default codec is json . The stderr offramp will write a \\n right after each event, and optionally prefix every event with a configurable prefix . If the event data (after codec and postprocessing) is not a valid UTF8 string (e.g. if it is binary data) if will by default output the bytes with debug formatting. If raw is set to true, the event data will be put on stderr as is. Supported configuration options: prefix - A prefix written before each event (optional string). raw - Write event data bytes as is to stderr. Example: offramp : - id : raw_stuff type : stderr codec : json postprocessors : - snappy config : raw : true","title":"stderr"},{"location":"artefacts/offramps/#stdout","text":"A custom stdout offramp can be configured by using this offramp type. But beware that this will share the single stdout stream with system::stdout . The default codec is json . The stdout offramp will write a \\n right after each event, and optionally prefix every event with a configurable prefix . If the event data (after codec and postprocessing) is not a valid UTF8 string (e.g. if it is binary data) if will by default output the bytes with debug formatting. If raw is set to true, the event data will be put on stdout as is. Supported configuration options: prefix - A prefix written before each event (optional string). raw - Write event data bytes as is to stdout. Example: offramp : - id : like_a_python_repl type : stdout config : prefix : \">>> \"","title":"stdout"},{"location":"artefacts/offramps/#tcp","text":"This connects on a specified port for distributing outbound TCP data. The offramp can leverage postprocessors to frame data after codecs are applied and events are forwarded to external TCP protocol distribution endpoints. The default codec is json . Supported configuration options are: host - The host to advertise as port - The TCP port to listen on is_non_blocking - Is the socket configured as non-blocking ( default: false ) ttl - Set the socket's time-to-live ( default: 64 ) is_no_delay - Set the socket's Nagle ( delay ) algorithm to off ( default: true ) Example: offramp : - id : tcp type : tcp codec : json postprocessors : - gzip - base64 - lines config : host : \"localhost\" port : 9000","title":"tcp"},{"location":"artefacts/offramps/#udp","text":"The UDP offramp sends data to a given host and port as UDP datagram. The default codec is json . When the UDP onramp gets a batch of messages it will send each element of the batch as a distinct UDP datagram. Supported configuration options are: bind.host - the local host to send data from bind.port - the local port to send data from host - the destination host to send data to port - the destination port to send data to. Warn Setting bound to false makes the UDP offramp potentially extremely slow as it forces a lookup of the destination on each event! Used metadata variables: $udp.host : This overwrites the configured destination host for this event. Expects a string. $udp.port : This overwrites the configured destination port for this event. Expects an integer. Warn Be careful to set $udp.host to an IP, not a DNS name or the OS will resolve it on every event, which will be extremely slow! Example: offramp : - id : udp-out type : udp postprocessors : - base64 config : bind : host : \"10.11.12.13\" port : 1234 host : \"20.21.22.23\" port : 2345","title":"udp"},{"location":"artefacts/offramps/#ws","text":"Sends events over a WebSocket connection. Each event is a WebSocket message. The default codec is json . Supported configuration options are: url - WebSocket endpoint to send data to. binary - If data should be send as binary instead of text (default: false ). Used metadata variables: $url - same as config url (optional. overrides related config param when present) $binary - same as config binary (optional. overrides related config param when present) Set metadata variables (for reply with linked transports ): $binary - true if the WebSocket message reply came as binary ( false otherwise) When used as a linked offramp, batched events are rejected by the offramp. Example: onramp : - id : ws type : ws config : url : \"ws://localhost:1234\"","title":"ws"},{"location":"artefacts/offramps/#gcs","text":"Google Cloud Storage offramp. This offramp can issue basic operations to list buckets and objects and to create, insert and delete objects from the Google Cloud Platform cloud storage service. Note The offramp is experimental. This offramp assumes that the environment variable GOOGLE_APPLICATION_CREDENTIALS been exported to the execution environment and it has been configured to point to a valid non-expired service account token json file. Example: offramp : - id : gcs type : gcs codec : json linked : true postprocessors : - gzip preprocessors : - gzip If the use case for the offramp requires metadata from the Google Cloud Storage service on the supported operations for this offramp, then set linked to true and configure the output port out . If the use case does not require metadata, then set linked to false.","title":"gcs"},{"location":"artefacts/offramps/#list_buckets","text":"Lists all the buckets in the Google Cloud Storage project. Request : { \"command\" : \"list_buckets\" , \"project_id\" : \"<project-id>\" } Response : [ { \"cmd\" : \"list_buckets\" , \"data\" : { \"items\" : [ { \"location\" : \"EU\" , \"id\" : \"<bucket-id>\" , \"locationType\" : \"multi-region\" , \"storageClass\" : \"STANDARD\" , \"metageneration\" : \"4\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-id>\" , \"kind\" : \"storage#bucket\" , \"name\" : \"<bucket-name>\" , \"timeCreated\" : \"<time of creation>\" , \"iamConfiguration\" : { \"uniformBucketLevelAccess\" : { \"enabled\" : true , \"lockedTime\" : \"2021-06-17T09:12:22.122Z\" }, \"bucketPolicyOnly\" : { \"enabled\" : true , \"lockedTime\" : \"2021-06-17T09:12:22.122Z\" } }, \"etag\" : \"CAQ=\" , \"defaultEventBasedHold\" : false , \"retentionPolicy\" : { \"effectiveTime\" : \"2021-03-19T09:12:22.122Z\" , \"retentionPeriod\" : \"60\" , \"isLocked\" : false }, \"updated\" : \"2021-04-13T17:05:29.404Z\" , \"satisfiesPZS\" : false , \"projectNumber\" : \"<project-number>\" } ], \"kind\" : \"storage#buckets\" } } ] Where - <project-id> - The project id of the Google Cloud Storage project where the buckets are. - <bucket-id> - The unique identifier of the bucket. - <project-number> - The project number for the Google Cloud Storage project.","title":"list_buckets"},{"location":"artefacts/offramps/#list_objects","text":"Lists all the objects in the specified bucket. Request : { \"command\" : \"list_objects\" , \"bucket\" : \"<bucket-name>\" } Response : [ { \"cmd\" : \"list_objects\" , \"data\" : { \"items\" : [ { \"mediaLink\" : \"\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"1943550\" , \"crc32c\" : \"0QXXmA==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1616145246990906\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-03-19T09:14:07.012Z\" , \"selfLink\" : \"\" , \"kind\" : \"storage#object\" , \"name\" : \"<object-name>\" , \"md5Hash\" : \"ucrP5b1N+6JHxn1TKavn/A==\" , \"timeCreated\" : \"2021-03-19T09:14:07.012Z\" , \"etag\" : \"CLrk6JqCvO8CEAE=\" , \"updated\" : \"2021-03-19T09:14:07.012Z\" , \"retentionExpirationTime\" : \"2021-03-19T09:15:07.012Z\" , \"contentType\" : \"<content-type>\" } ], \"kind\" : \"storage#objects\" } } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. - <content-type> - The type of the object uploaded.","title":"list_objects"},{"location":"artefacts/offramps/#create_bucket","text":"Creates a bucket in the project specified in the command. Request : { \"command\" : \"create_bucket\" , \"project_id\" : \"<project-id>\" , \"bucket\" : \"<bucket>\" } Response : [ { \"cmd\" : \"create_bucket\" , \"data\" : { \"projectNumber\" : \"<project-number>\" , \"location\" : \"US\" , \"id\" : \"<bucket-id>\" , \"etag\" : \"CAE=\" , \"locationType\" : \"multi-region\" , \"storageClass\" : \"STANDARD\" , \"metageneration\" : \"1\" , \"updated\" : \"2021-04-22T07:37:23.702Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>\" , \"kind\" : \"storage#bucket\" , \"name\" : \"tremor\" , \"iamConfiguration\" : { \"uniformBucketLevelAccess\" : { \"enabled\" : false }, \"bucketPolicyOnly\" : { \"enabled\" : false } }, \"timeCreated\" : \"2021-04-22T07:37:23.702Z\" } } ] Where - <project-id> - The project id of the Google Cloud Storage project where the bucket is. - <bucket-id> - The unique identifier of the bucket. - <project-number> - The project number for the Google Cloud Storage project.","title":"create_bucket"},{"location":"artefacts/offramps/#remove_bucket","text":"Removes the bucket from the specified project in Google Cloud Storage project. Request : { \"command\" : \"remove_bucket\" , \"bucket\" : \"<bucket-name>\" } Response : Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is.","title":"remove_bucket"},{"location":"artefacts/offramps/#upload_object","text":"Uploads the object to a Google Cloud Storage bucket. Request : { \"command\" : \"upload_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" , \"body\" : `<object>` } Response : [ { \"cmd\" : \"upload_object\" , \"data\" : { \"mediaLink\" : \"https://storage.googleapis.com/download/storage/v1/b/<bucket-name>/o/<object-name>?generation=1619077199260663&alt=media\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"47\" , \"crc32c\" : \"a7mrxw==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1619077199260663\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-04-22T07:39:59.278Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>/o/<object-name>\" , \"kind\" : \"storage#object\" , \"name\" : \"april\" , \"md5Hash\" : \"qhr9326j+3PeIK9koXMHCg==\" , \"timeCreated\" : \"2021-04-22T07:39:59.278Z\" , \"etag\" : \"CPfXzcqskfACEAE=\" , \"updated\" : \"2021-04-22T07:39:59.278Z\" , \"retentionExpirationTime\" : \"2021-04-22T07:40:59.278Z\" , \"contentType\" : \"application/json\" } } ] Where - <object> - The object data to be uploaded to a Google Cloud Storage bucket. - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket.","title":"upload_object"},{"location":"artefacts/offramps/#fetch_object","text":"Returns the metadata for the object fetched. Request : { \"command\" : \"fetch\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : [ { \"cmd\" : \"fetch\" , \"data\" : { \"mediaLink\" : \"https://storage.googleapis.com/download/storage/v1/b/<bucket-name>/o/<object-name>?generation=1619077199260663&alt=media\" , \"bucket\" : \"<bucket-name>\" , \"id\" : \"<object-id>\" , \"size\" : \"47\" , \"crc32c\" : \"a7mrxw==\" , \"storageClass\" : \"STANDARD\" , \"generation\" : \"1619077199260663\" , \"metageneration\" : \"1\" , \"timeStorageClassUpdated\" : \"2021-04-22T07:39:59.278Z\" , \"selfLink\" : \"https://www.googleapis.com/storage/v1/b/<bucket-name>/o/<object-name>\" , \"kind\" : \"storage#object\" , \"name\" : \"april\" , \"md5Hash\" : \"qhr9326j+3PeIK9koXMHCg==\" , \"timeCreated\" : \"2021-04-22T07:39:59.278Z\" , \"etag\" : \"CPfXzcqskfACEAE=\" , \"updated\" : \"2021-04-22T07:39:59.278Z\" , \"retentionExpirationTime\" : \"2021-04-22T07:40:59.278Z\" , \"contentType\" : \"application/json\" } } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-id> - The unique object identifier. - <object-name> - The name of the object in the Google Cloud Stoarge bucket.","title":"fetch_object"},{"location":"artefacts/offramps/#download_object","text":"Downloads the object. Request : { \"command\" : \"download_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : [ { \"cmd\" : \"download_object\" , \"data\" : `<object>` } ] Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-name> - The name of the object in the Google Cloud Stoarge bucket. - <object> - The object downloaded.","title":"download_object"},{"location":"artefacts/offramps/#remove_object","text":"Removes the object from the specified bucket. Request : { \"command\" : \"remove_object\" , \"bucket\" : \"<bucket-name>\" , \"object\" : \"<object-name>\" } Response : Where - <bucket-name> - The name of the Google Cloud Storage bucket where the object is. - <object-name> - The name of the object in the Google Cloud Stoarge bucket.","title":"remove_object"},{"location":"artefacts/onramps/","text":"Onramps \u00b6 Specify how Tremor connects to the outside world in order to receive from external systems. All Onramps support circuit breakers as in that no new events are read from it in the case of a circuit breaker triggering. For example, the Kafka onramp receives data from a Kafka cluster by creating a local record consumer, connecting to a set of topics and ingesting Kafka record data. All onramps are of the form: onramp : - id : <unique onramp id> type : <onramp name> preprocessors : # can be omitted - <preprocessor 1> - <preprocessor 2> - ... postprocessors : # only for linked transport, can be omitted - <postprocessor 1> - <postprocessor 2> - ... linked : <true or false> # enable linked transport, default: false codec : <codec of the data> codec_map : \"<mime-type>\" : <coded handling events of this mime-type> config : <key> : <value> The codec field is optional and if not provided will use onramps default codec. The err_required field can be set to true if the onramp should not start unless both out and err ports are connected to at least one pipeline. The config contains a map (key-value pairs) specific to the onramp type. Delivery Properties \u00b6 Onramps are able to act upon both circuit breaker and guaranteed delivery events from the downstream pipelines. Those are triggered when event delivery is acknowledged or when event delivery fails. Also when some part (offramps, operators) signals itself being broken, the circuit breaker opens, or when the downstream system heals, the circuit breaker closes again, signaling it is safe to send further events. How each onramp reacts, is described in the table below: The column Delivery Acknowledgements describes when the onramp considers and reports the event delivered to the upstream it is connected to. Onramp Delivery Acknowledgements blaster not supported cb not supported crononome not supported discord not supported file not supported kafka always, only on ack event if enable.auto.commit is set to false metronome not supported nats not supported otel not supported PostgreSQL not supported rest not supported stdin not supported tcp not supported udp not supported ws not supported Supported Onramps \u00b6 blaster \u00b6 Note This onramp is for benchmarking use, it should not be deployed in a live production system. The blaster onramp is built for performance testing, but it can be used for spaced-out replays of events as well. Files to replay can be xz compressed. It will keep looping over the file. The default codec is json . The event origin URI set by the onramp is of the form: tremor-blaster://<tremor-host.local>/<config_source_file> Supported configuration options are: source - The file to read from. interval - The interval in which events are sent in nanoseconds. iters - Number of times the file will be repeated. Example: onramp : - id : blaster type : blaster codec : json config : source : ./demo/data/data.json.xz cb \u00b6 The cb onramp is for testing how downstream pipeline and offramps issue circuit breaker events. It expects a circuit breaker event for each event it sent out, and then, the latest after the configured timeout is exceeded, it exits the tremor process. If some events didn't receive circuit breaker events, it exits with status code 1 , if everything is fine it exits with 0 . Supported configuration options are: source - The file to read from, expecting 1 event payload per line. timeout - The time to wait for circuit breaker events in milliseconds. If this timeout is exceeded, the tremor process is terminated. (Default: 10000 ms) Example: onramp : - id : cb_test type : cb codec : json config : source : in.json timeout : 1000 crononome \u00b6 This sends a scheduled tick down the offramp. Schedules can be one-off or repeating and use a cron-like format. Multiple cron entries can be configured, each with a symbolic name and an optional JSON payload in addition to the cron expression. The event origin URI set by the onramp is of the form: tremor-crononome://<tremor-host.local> Supported configuration options are: entries - A sequence of entries Example onramp : - id : crononome type : crononome codec : json config : entries : ## every second - name : 1s expr : \"* * * * * *\" ## every 5 seconds - name : 5s expr : \"0/5 * * * * *\" ## every minute - name : 1m expr : \"0 * * * * *\" payload : snot : badger Cron entries that are historic or in the past ( relative to the current UTC time ) will be ignored. Cron entries beyond 2038 will not work due to underlying libraries ( rust , chrono , cron.rs ) suffering from the year 2038 problem . discord \u00b6 This onramp can be linked The discord onramp allows consuming events from the Discord API . It uses the event structure as provided by serenity wrapped in event-named records. Replies send to this onramp can perform multiple operations: Guild related \u00b6 { \"guild\" : { \"id\" : 1234 , // guild id , required // member sec t io n required \"member\" : { \"id\" : 2345 , // member id , required // Roles t o remove , op t io nal \"remove_roles\" : [ 3456 , // ... role ids ], // Roles t o add , op t io nal \"add_roles\" : [ 4567 , // ... role ids ], \"deafen\" : true , // (u n )dea fen t he member , op t io nal \"mute\" : true , // (u n )dea fen t he member , op t io nal }, }} Message related \u00b6 { \"message\" : { \"channel_id\" : 1234 , // cha nnel id , required // Upda te message sec t io n , op t io nal \"update\" : { \"message_id\" : 2345 , // message id t o upda te , required // Reac t io ns t o add \"add_reactions\" : [ \"\ud83d\ude00\" , // emoji reac t io n { // cus t om reac t io n \"id\" : 3456 , // emoji id , required \"name\" : \"seal-of-approval\" // emoji na me , op t io nal \"animated\" : true , // a n ima te d , op t io nal } // ... ], }, // Se n d sec t io n , op t io nal \"send\" : { \"content\" : \"hello!\" , // message co ntent , op t io nal , \"reference_message\" : 4567 , // Re feren ce t o o t her message , op t io nal \"reference_channel\" : 5678 , // re feren ce cha nnel , op t io nal , de fault is `cha nnel _id` (ig n ored w/o `re feren ce_message`) \"tts\" : false , // use te x t t o speech , op t io nal // Embed sec t io n , op t io nal \"embed\" : { // Au t hor sec t io n , op t io nal \"author\" : { \"icon_url\" : \"https://...\" , // url o f t he au t hor ico n , op t io nal \"name\" : \"Snottus Badgerus\" , // na me o f t he au t hor , op t io nal \"url\" : \"https://...\" , // url o f t he au t hor pro f ile , op t io nal }, \"colour\" : 0 , // color (as nu mber) o f t he embed , op t io nal (hi nt : use hex i n tre mor scrip t i t makes i t easier)\\ \"description\" : \"This is an embed\" , // A descrip t io n f or t he embed , op t io nal // Embedded f ields , op t io nal \"fields\" : [ { \"name\" : \"field 1\" , // na me o f t he f ield , required \"value\" : \"explenation\" , // 'body' o f t he f ield , required \"inline\" : true , // i f t he f ield should be i nl i ne d , op t io nal , de fault : false } // ... ], \"footer\" : \"look at my feet!\" , // simple f oo ter , op t io nal // Foo ter sec t io n , op t io nal , al ternat ive t o simple f oo ter \"footer\" : { \"text\" : \"look at my feet!\" , // f oo ter te x t , op t io nal \"icon_url\" : \"https://...\" , // f oo ter ico n , op t io nal } }, // Reac t io ns t o add \"reactions\" : [ \"\ud83d\ude00\" , // emoji reac t io n { // cus t om reac t io n \"id\" : 3456 , // emoji id , required \"name\" : \"seal-of-approval\" // emoji na me , op t io nal \"animated\" : true , // a n ima te d , op t io nal } // ... ], } }} file \u00b6 The file onramp reads the content of a file, line by line. And sends each line as an event. It has the ability to shut down the system upon completion. Files can be xz compressed. The default codec is json . The event origin URI set by the onramp is of the form: tremor-file://<tremor-host.local>/<config_source_file> Supported configuration options are: source - The file to read from. close_on_done - Terminates tremor once the file is processed. sleep_on_done - Waits for the given number of milliseconds, before terminating tremor. Intended to be used with close_on_done . Example: onramp : - id : in type : file config : source : /my/path/to/a/file.json close_on_done : true sleep_on_done : 1000 # wait for a second before terminating Kafka \u00b6 The Kafka onramp connects to one or more Kafka topics. It uses librdkafka to handle connections and can use the full set of librdkafka 1.5.0 configuration options . The default codec is json . The event origin URI set by the onramp is of the form: tremor-kafka://<config_first_broker_host>[:<config_first_broker_port>]/<topic>/<partition>/<offset> Supported configuration options are: group_id - The Kafka consumer group id to use. topics - A list of topics to subscribe to. brokers - Broker servers to connect to. (Kafka nodes) rdkafka_options - A optional map of an option to value, where both sides need to be strings. retry_failed_events - If set to false the source will not seek back the consumer offset upon a failed events and thus not retry those when enable.auto.commit is set to false in rdkafka_options . (default true ) poll_interval - Duration in milliseconds to wait until we poll again if no message is in the kafka queue. (default: 100 ) Set metadata variables are: $kafka - Record consisting of two optional keys: headers : A record denoting the headers for the message (if any). key : The key used for this message in bytes (if any). topic : The topic the message was on (if any). offset : The offset in the partition the message was on (if any). partition : The partition the message was on (if any). timestamp : The timestamp provided by kafka in milliseconds (if any). Example: onramp : - id : kafka-in type : kafka codec : json config : brokers : - kafka:9092 topics : - demo - snotbadger group_id : demo A more involved example, only committing on successful circuit breaker event and not retrying failed events, while also decreasing the poll interval to 10ms to get notified of new messages faster: onramp : - id : involved-kafka type : kafka codec : msgpack preprocessors : - lines config : brokers : - kafka01:9092 - kafka02:9092 topics : - my_topic group_id : my_group_id retry_failed_events : false poll_interval : 10 rdkafka_options : 'enable.auto.commit' : false Semantics with enable.auto.commit \u00b6 If enable.auto.commit: false is set in rdkafka_options , the consumer offset in kafka will only be committed when the event has been successfully reached the other end of the pipeline (typically some offramp ). If an event failed during processing within the pipeline or at a downstream offramp, the consumer offset will be reset to the offset of the failed event, so it will be retried. This has some consequences worth mentioning: Already processed kafka messages (that have succeeded before the failed message failed) might be seen again multiple times. If the message is persistently failing (e.g. due to an malformed payload or similar), tremor will retry those messages infinitely. If persistent failures are to be expected (e.g. due to incorrect event payloads) or if repeating messages in general are a problem for the application, avoiding retries with retry_failed_events: false is advised. If enable.auto.commit: true is set in rdkafka_options , which is the default behaviour if nothing is specified, the offset is immediately committed upon event reception in tremor, regardless of success or failure of processing the kafka message as event in tremor. metronome \u00b6 This sends a periodic tick downstream. It is an excellent tool to generate some test traffic to validate pipelines. The default codec is pass . (since we already output decoded JSON) The event origin URI set by the onramp is of the form: tremor-metronome://<tremor-host.local>/<config_interval> Supported configuration options are: interval - The interval in which events are sent in milliseconds. Example: onramp : - id : metronome type : metronome config : interval : 10000 nats \u00b6 The nats onramp connects to Nats server(s) and subscribes to a specified subject. The default codec is json . The event origin URI set by the onramp is of the form: tremor-nats://<config_first_host_host_addr>[:<config_first_host_port>]/<subject> Supported configuration operations are: hosts - List of hosts to connect to. subject - Subject to subscribe to for listening to messages. queue - Optional queue to subscribe to. options - Optional struct, which can be used to customize the connection to the server (see nats.rs configuration options for more info): token : String; authenticate using a token. username : String; authenticate using a username and password. password : String; authenticate using a username and password. credentials_path : String; path to a .creds file for authentication. cert_path : String; path to the client certificate file. key_path : String; path to private key file. name : String; name this configuration. echo : Boolean; if true, published messages will not be delivered. max_reconnects : Integer; max number of reconnection attempts. reconnect_buffer_size : Integer; max amount of bytes to buffer when accepting outgoing traffic in disconnected mode. tls : Boolean; if true, sets tls for all server connections. root_cert : String; path to a root certificate. Set metadata variables are: $nats : Record consisting of the following metadata: $reply : Reply associated with the message (if any). $headers : Record denoting the headers for the message (if any). Example: onramp : - id : nats-in type : nats config : hosts : - \"127.0.0.1:4444\" subject : demo queue : stack options : name : nats-demo reconnect_buffer_size : 1 PostgreSQL \u00b6 PostgreSQL onramp. Supported configuration options are: host - PostgreSQL database hostname port - PostgresSQL database port user - Username for authentication password - Password for authentication dbname - Database name query - Query run to retrieve data interval_ms - Query interval in milliseconds cache - Location ( path ) and size ( size ) for caching of latest successful query interval query must include two arguments to be filled with start and end interval timestamps. Data will come out of onramp in objects representing columns. If schema specifies there are two fields, username ( VARCHAR ) and created_at ( TIMESTAMPTZ ) then the actual JSON coming out of onramp looks like: \"username\": { \"fieldType\": \"VARCHAR\", \"name\": \"username\", \"value\": \"actual\\_username\" }, \"created\\_at\": { \"fieldType\": \"TIMESTAMPTZ\", \"name\": \"created\\_at\", \"value\": \"2020-04-04 00:00:00.000000 +00:00\" } Example: id: db type: postgres codec: json config: host: localhost port: 5432 user: postgres password: example dbname: sales query: \"SELECT id, name from events WHERE produced_at <= $1 AND produced_at > $2\" interval_ms: 1000 cache: path: \"/path/to/cache.json\" size: 4096 rest \u00b6 This onramp can be linked The rest onramp listens on a specified port for inbound RESTful ( http ) data, treating the decoded and preprocessed http body as event data (and attaching other request attributes as event metadata). The event origin URI set by the onramp is of the form: tremor-rest://<tremor-rest-client-host.remote> Supported configuration options are: host - The host to advertise as port - The TCP port to listen on The rest onramp respects the HTTP Content-Type header and will use it to decode the request body when it's present (otherwise it defaults to using the codec specified in the onramp config). Tremor supports a limited set of builtin codecs used for well known MIME types (e.g. application/json , application/yaml , text/plain ). In order to customize how certain Content-Type s are handled, provide a codec_map providing a mapping from MIME type to Tremor codec in the top level artifact config (where the codec is set). Set metadata variables: $request - A record capturing the HTTP request attributes. Available fields within: url - A record with the following standard URL fields (optional fields might not be present): scheme - String, typically http username - String, optional password - String, optional host - String port - number, optional, absence means 80 path - String query - String, optional fragment - String, optional method - HTTP method used by the incoming request headers - A record that maps header name (lowercase string) to values (array of strings) Used metadata variables: These variables can be used to dynamically change how responses are handled when using this onramp as linked transport : $response - A record capturing the HTTP response attributes. Available fields within: status - Numeric HTTP status code. (optional. status code defaults to 200 when not set) headers - A record that maps header name (string) to value (string or array of strings) (optional) When not used as a linked onramp, the status code returned with the response is 202 . Example: onramp : - id : rest type : rest preprocessors : - lines codec : json codec_map : \"text/html\" : \"string\" config : host : \"localhost\" port : 9000 Known limitations: It is currently not possible to configure rest onramps via swagger, RAML or OpenAPI configuration files. stdin \u00b6 An onramp that takes input from stdin . The default codec is string . The event origin URI set by the onramp is of the form: tremor-stdin://<tremor-host.local> tcp \u00b6 This listens on a specified port for inbound tcp data. The onramp can leverage preprocessors to segment data before codecs are applied and events are forwarded to pipelines. The default codec is json . The event origin URI set by the onramp is of the form: tremor-tcp://<client_ip>:<client_port>/<config_server_port> Supported configuration options are: host - The IP to listen on port - The Port to listen on Example: onramp : - id : tcp type : tcp preprocessors : - base64 - lines codec : json config : host : \"127.0.0.1\" port : 9000 tcp onramp example for GELF \u00b6 onramp : - id : gelf-tcp type : tcp preprocessors : - lines-null codec : json config : host : \"127.0.0.1\" port : 12201 udp \u00b6 The UDP onramp allows receiving data via UDP datagrams. The default codec is string . The event origin URI set by the onramp is of the form: tremor-udp://<sender_ip>:<sender_port>/<config_receive_port> Supported configuration options are: host - The IP to listen on port - The Port to listen on Example: onramp : - id : udp type : udp codec : json config : host : \"127.0.0.1\" port : 9000 udp onramp example for GELF \u00b6 onramp : - id : gelf-udp type : udp preprocessors : - decompress - gelf-chunking - decompress codec : json config : host : \"127.0.0.1\" port : 12201 ws \u00b6 This onramp can be linked WebSocket onramp. Receiving either binary or text packages from a WebSocket connection. the url is: ws://<host>:<port>/ The event origin URI set by the onramp is of the form: tremor-ws://<tremor-ws-client-host.remote> Supported configuration options are: host - The IP to listen on port - The Port to listen on Set metadata variables: $binary - true if the incoming WebSocket message came as binary ( false otherwise) Used metadata variables (for reply with linked transports ): $binary - If reply data should be send as binary instead of text (optional. data format defaults to text when not set). Example: onramp : - id : ws type : ws codec : json config : port : 12201 host : \"127.0.0.1\" otel \u00b6 CNCF OpenTelemetry onramp. Listens on TCP port 4317 for gRPC traffic conforming to the CNCF OpenTelemetry protocol specification. Forwards tremor value variants of logs , trace and metrics messages. The onramp is experimental. Supported configuration options are: host - String - The host or IP to listen on port - integer - The TCP port to listen on 'logs' - boolean - Is logging enabled for this instance. Defaults to true . Received logs events are dropped when false . 'metrics' - boolean - Is metrics enabled for this instance. Defaults to true . Defaults to true . Received metrics events are dropped when false . 'trace' - boolean - Is trace enabled for this instance. Defaults to true . Defaults to true . Received trace events are dropped when false . Pipelines that leverage the OpenTelemetry integration can use utility modules in the cncf::otel module to simplify working with the tremor value mapping of the event data. The connector translates the wire level data from protocol buffers to tremor values automatically. Example: onramp : - id : otlp type : otel codec : json config : port : 4317 host : 127.0.0.1","title":"Onramps"},{"location":"artefacts/onramps/#onramps","text":"Specify how Tremor connects to the outside world in order to receive from external systems. All Onramps support circuit breakers as in that no new events are read from it in the case of a circuit breaker triggering. For example, the Kafka onramp receives data from a Kafka cluster by creating a local record consumer, connecting to a set of topics and ingesting Kafka record data. All onramps are of the form: onramp : - id : <unique onramp id> type : <onramp name> preprocessors : # can be omitted - <preprocessor 1> - <preprocessor 2> - ... postprocessors : # only for linked transport, can be omitted - <postprocessor 1> - <postprocessor 2> - ... linked : <true or false> # enable linked transport, default: false codec : <codec of the data> codec_map : \"<mime-type>\" : <coded handling events of this mime-type> config : <key> : <value> The codec field is optional and if not provided will use onramps default codec. The err_required field can be set to true if the onramp should not start unless both out and err ports are connected to at least one pipeline. The config contains a map (key-value pairs) specific to the onramp type.","title":"Onramps"},{"location":"artefacts/onramps/#delivery-properties","text":"Onramps are able to act upon both circuit breaker and guaranteed delivery events from the downstream pipelines. Those are triggered when event delivery is acknowledged or when event delivery fails. Also when some part (offramps, operators) signals itself being broken, the circuit breaker opens, or when the downstream system heals, the circuit breaker closes again, signaling it is safe to send further events. How each onramp reacts, is described in the table below: The column Delivery Acknowledgements describes when the onramp considers and reports the event delivered to the upstream it is connected to. Onramp Delivery Acknowledgements blaster not supported cb not supported crononome not supported discord not supported file not supported kafka always, only on ack event if enable.auto.commit is set to false metronome not supported nats not supported otel not supported PostgreSQL not supported rest not supported stdin not supported tcp not supported udp not supported ws not supported","title":"Delivery Properties"},{"location":"artefacts/onramps/#supported-onramps","text":"","title":"Supported Onramps"},{"location":"artefacts/onramps/#blaster","text":"Note This onramp is for benchmarking use, it should not be deployed in a live production system. The blaster onramp is built for performance testing, but it can be used for spaced-out replays of events as well. Files to replay can be xz compressed. It will keep looping over the file. The default codec is json . The event origin URI set by the onramp is of the form: tremor-blaster://<tremor-host.local>/<config_source_file> Supported configuration options are: source - The file to read from. interval - The interval in which events are sent in nanoseconds. iters - Number of times the file will be repeated. Example: onramp : - id : blaster type : blaster codec : json config : source : ./demo/data/data.json.xz","title":"blaster"},{"location":"artefacts/onramps/#cb","text":"The cb onramp is for testing how downstream pipeline and offramps issue circuit breaker events. It expects a circuit breaker event for each event it sent out, and then, the latest after the configured timeout is exceeded, it exits the tremor process. If some events didn't receive circuit breaker events, it exits with status code 1 , if everything is fine it exits with 0 . Supported configuration options are: source - The file to read from, expecting 1 event payload per line. timeout - The time to wait for circuit breaker events in milliseconds. If this timeout is exceeded, the tremor process is terminated. (Default: 10000 ms) Example: onramp : - id : cb_test type : cb codec : json config : source : in.json timeout : 1000","title":"cb"},{"location":"artefacts/onramps/#crononome","text":"This sends a scheduled tick down the offramp. Schedules can be one-off or repeating and use a cron-like format. Multiple cron entries can be configured, each with a symbolic name and an optional JSON payload in addition to the cron expression. The event origin URI set by the onramp is of the form: tremor-crononome://<tremor-host.local> Supported configuration options are: entries - A sequence of entries Example onramp : - id : crononome type : crononome codec : json config : entries : ## every second - name : 1s expr : \"* * * * * *\" ## every 5 seconds - name : 5s expr : \"0/5 * * * * *\" ## every minute - name : 1m expr : \"0 * * * * *\" payload : snot : badger Cron entries that are historic or in the past ( relative to the current UTC time ) will be ignored. Cron entries beyond 2038 will not work due to underlying libraries ( rust , chrono , cron.rs ) suffering from the year 2038 problem .","title":"crononome"},{"location":"artefacts/onramps/#discord","text":"This onramp can be linked The discord onramp allows consuming events from the Discord API . It uses the event structure as provided by serenity wrapped in event-named records. Replies send to this onramp can perform multiple operations:","title":"discord"},{"location":"artefacts/onramps/#guild-related","text":"{ \"guild\" : { \"id\" : 1234 , // guild id , required // member sec t io n required \"member\" : { \"id\" : 2345 , // member id , required // Roles t o remove , op t io nal \"remove_roles\" : [ 3456 , // ... role ids ], // Roles t o add , op t io nal \"add_roles\" : [ 4567 , // ... role ids ], \"deafen\" : true , // (u n )dea fen t he member , op t io nal \"mute\" : true , // (u n )dea fen t he member , op t io nal }, }}","title":"Guild related"},{"location":"artefacts/onramps/#message-related","text":"{ \"message\" : { \"channel_id\" : 1234 , // cha nnel id , required // Upda te message sec t io n , op t io nal \"update\" : { \"message_id\" : 2345 , // message id t o upda te , required // Reac t io ns t o add \"add_reactions\" : [ \"\ud83d\ude00\" , // emoji reac t io n { // cus t om reac t io n \"id\" : 3456 , // emoji id , required \"name\" : \"seal-of-approval\" // emoji na me , op t io nal \"animated\" : true , // a n ima te d , op t io nal } // ... ], }, // Se n d sec t io n , op t io nal \"send\" : { \"content\" : \"hello!\" , // message co ntent , op t io nal , \"reference_message\" : 4567 , // Re feren ce t o o t her message , op t io nal \"reference_channel\" : 5678 , // re feren ce cha nnel , op t io nal , de fault is `cha nnel _id` (ig n ored w/o `re feren ce_message`) \"tts\" : false , // use te x t t o speech , op t io nal // Embed sec t io n , op t io nal \"embed\" : { // Au t hor sec t io n , op t io nal \"author\" : { \"icon_url\" : \"https://...\" , // url o f t he au t hor ico n , op t io nal \"name\" : \"Snottus Badgerus\" , // na me o f t he au t hor , op t io nal \"url\" : \"https://...\" , // url o f t he au t hor pro f ile , op t io nal }, \"colour\" : 0 , // color (as nu mber) o f t he embed , op t io nal (hi nt : use hex i n tre mor scrip t i t makes i t easier)\\ \"description\" : \"This is an embed\" , // A descrip t io n f or t he embed , op t io nal // Embedded f ields , op t io nal \"fields\" : [ { \"name\" : \"field 1\" , // na me o f t he f ield , required \"value\" : \"explenation\" , // 'body' o f t he f ield , required \"inline\" : true , // i f t he f ield should be i nl i ne d , op t io nal , de fault : false } // ... ], \"footer\" : \"look at my feet!\" , // simple f oo ter , op t io nal // Foo ter sec t io n , op t io nal , al ternat ive t o simple f oo ter \"footer\" : { \"text\" : \"look at my feet!\" , // f oo ter te x t , op t io nal \"icon_url\" : \"https://...\" , // f oo ter ico n , op t io nal } }, // Reac t io ns t o add \"reactions\" : [ \"\ud83d\ude00\" , // emoji reac t io n { // cus t om reac t io n \"id\" : 3456 , // emoji id , required \"name\" : \"seal-of-approval\" // emoji na me , op t io nal \"animated\" : true , // a n ima te d , op t io nal } // ... ], } }}","title":"Message related"},{"location":"artefacts/onramps/#file","text":"The file onramp reads the content of a file, line by line. And sends each line as an event. It has the ability to shut down the system upon completion. Files can be xz compressed. The default codec is json . The event origin URI set by the onramp is of the form: tremor-file://<tremor-host.local>/<config_source_file> Supported configuration options are: source - The file to read from. close_on_done - Terminates tremor once the file is processed. sleep_on_done - Waits for the given number of milliseconds, before terminating tremor. Intended to be used with close_on_done . Example: onramp : - id : in type : file config : source : /my/path/to/a/file.json close_on_done : true sleep_on_done : 1000 # wait for a second before terminating","title":"file"},{"location":"artefacts/onramps/#kafka","text":"The Kafka onramp connects to one or more Kafka topics. It uses librdkafka to handle connections and can use the full set of librdkafka 1.5.0 configuration options . The default codec is json . The event origin URI set by the onramp is of the form: tremor-kafka://<config_first_broker_host>[:<config_first_broker_port>]/<topic>/<partition>/<offset> Supported configuration options are: group_id - The Kafka consumer group id to use. topics - A list of topics to subscribe to. brokers - Broker servers to connect to. (Kafka nodes) rdkafka_options - A optional map of an option to value, where both sides need to be strings. retry_failed_events - If set to false the source will not seek back the consumer offset upon a failed events and thus not retry those when enable.auto.commit is set to false in rdkafka_options . (default true ) poll_interval - Duration in milliseconds to wait until we poll again if no message is in the kafka queue. (default: 100 ) Set metadata variables are: $kafka - Record consisting of two optional keys: headers : A record denoting the headers for the message (if any). key : The key used for this message in bytes (if any). topic : The topic the message was on (if any). offset : The offset in the partition the message was on (if any). partition : The partition the message was on (if any). timestamp : The timestamp provided by kafka in milliseconds (if any). Example: onramp : - id : kafka-in type : kafka codec : json config : brokers : - kafka:9092 topics : - demo - snotbadger group_id : demo A more involved example, only committing on successful circuit breaker event and not retrying failed events, while also decreasing the poll interval to 10ms to get notified of new messages faster: onramp : - id : involved-kafka type : kafka codec : msgpack preprocessors : - lines config : brokers : - kafka01:9092 - kafka02:9092 topics : - my_topic group_id : my_group_id retry_failed_events : false poll_interval : 10 rdkafka_options : 'enable.auto.commit' : false","title":"Kafka"},{"location":"artefacts/onramps/#semantics-with-enableautocommit","text":"If enable.auto.commit: false is set in rdkafka_options , the consumer offset in kafka will only be committed when the event has been successfully reached the other end of the pipeline (typically some offramp ). If an event failed during processing within the pipeline or at a downstream offramp, the consumer offset will be reset to the offset of the failed event, so it will be retried. This has some consequences worth mentioning: Already processed kafka messages (that have succeeded before the failed message failed) might be seen again multiple times. If the message is persistently failing (e.g. due to an malformed payload or similar), tremor will retry those messages infinitely. If persistent failures are to be expected (e.g. due to incorrect event payloads) or if repeating messages in general are a problem for the application, avoiding retries with retry_failed_events: false is advised. If enable.auto.commit: true is set in rdkafka_options , which is the default behaviour if nothing is specified, the offset is immediately committed upon event reception in tremor, regardless of success or failure of processing the kafka message as event in tremor.","title":"Semantics with enable.auto.commit"},{"location":"artefacts/onramps/#metronome","text":"This sends a periodic tick downstream. It is an excellent tool to generate some test traffic to validate pipelines. The default codec is pass . (since we already output decoded JSON) The event origin URI set by the onramp is of the form: tremor-metronome://<tremor-host.local>/<config_interval> Supported configuration options are: interval - The interval in which events are sent in milliseconds. Example: onramp : - id : metronome type : metronome config : interval : 10000","title":"metronome"},{"location":"artefacts/onramps/#nats","text":"The nats onramp connects to Nats server(s) and subscribes to a specified subject. The default codec is json . The event origin URI set by the onramp is of the form: tremor-nats://<config_first_host_host_addr>[:<config_first_host_port>]/<subject> Supported configuration operations are: hosts - List of hosts to connect to. subject - Subject to subscribe to for listening to messages. queue - Optional queue to subscribe to. options - Optional struct, which can be used to customize the connection to the server (see nats.rs configuration options for more info): token : String; authenticate using a token. username : String; authenticate using a username and password. password : String; authenticate using a username and password. credentials_path : String; path to a .creds file for authentication. cert_path : String; path to the client certificate file. key_path : String; path to private key file. name : String; name this configuration. echo : Boolean; if true, published messages will not be delivered. max_reconnects : Integer; max number of reconnection attempts. reconnect_buffer_size : Integer; max amount of bytes to buffer when accepting outgoing traffic in disconnected mode. tls : Boolean; if true, sets tls for all server connections. root_cert : String; path to a root certificate. Set metadata variables are: $nats : Record consisting of the following metadata: $reply : Reply associated with the message (if any). $headers : Record denoting the headers for the message (if any). Example: onramp : - id : nats-in type : nats config : hosts : - \"127.0.0.1:4444\" subject : demo queue : stack options : name : nats-demo reconnect_buffer_size : 1","title":"nats"},{"location":"artefacts/onramps/#postgresql","text":"PostgreSQL onramp. Supported configuration options are: host - PostgreSQL database hostname port - PostgresSQL database port user - Username for authentication password - Password for authentication dbname - Database name query - Query run to retrieve data interval_ms - Query interval in milliseconds cache - Location ( path ) and size ( size ) for caching of latest successful query interval query must include two arguments to be filled with start and end interval timestamps. Data will come out of onramp in objects representing columns. If schema specifies there are two fields, username ( VARCHAR ) and created_at ( TIMESTAMPTZ ) then the actual JSON coming out of onramp looks like: \"username\": { \"fieldType\": \"VARCHAR\", \"name\": \"username\", \"value\": \"actual\\_username\" }, \"created\\_at\": { \"fieldType\": \"TIMESTAMPTZ\", \"name\": \"created\\_at\", \"value\": \"2020-04-04 00:00:00.000000 +00:00\" } Example: id: db type: postgres codec: json config: host: localhost port: 5432 user: postgres password: example dbname: sales query: \"SELECT id, name from events WHERE produced_at <= $1 AND produced_at > $2\" interval_ms: 1000 cache: path: \"/path/to/cache.json\" size: 4096","title":"PostgreSQL"},{"location":"artefacts/onramps/#rest","text":"This onramp can be linked The rest onramp listens on a specified port for inbound RESTful ( http ) data, treating the decoded and preprocessed http body as event data (and attaching other request attributes as event metadata). The event origin URI set by the onramp is of the form: tremor-rest://<tremor-rest-client-host.remote> Supported configuration options are: host - The host to advertise as port - The TCP port to listen on The rest onramp respects the HTTP Content-Type header and will use it to decode the request body when it's present (otherwise it defaults to using the codec specified in the onramp config). Tremor supports a limited set of builtin codecs used for well known MIME types (e.g. application/json , application/yaml , text/plain ). In order to customize how certain Content-Type s are handled, provide a codec_map providing a mapping from MIME type to Tremor codec in the top level artifact config (where the codec is set). Set metadata variables: $request - A record capturing the HTTP request attributes. Available fields within: url - A record with the following standard URL fields (optional fields might not be present): scheme - String, typically http username - String, optional password - String, optional host - String port - number, optional, absence means 80 path - String query - String, optional fragment - String, optional method - HTTP method used by the incoming request headers - A record that maps header name (lowercase string) to values (array of strings) Used metadata variables: These variables can be used to dynamically change how responses are handled when using this onramp as linked transport : $response - A record capturing the HTTP response attributes. Available fields within: status - Numeric HTTP status code. (optional. status code defaults to 200 when not set) headers - A record that maps header name (string) to value (string or array of strings) (optional) When not used as a linked onramp, the status code returned with the response is 202 . Example: onramp : - id : rest type : rest preprocessors : - lines codec : json codec_map : \"text/html\" : \"string\" config : host : \"localhost\" port : 9000 Known limitations: It is currently not possible to configure rest onramps via swagger, RAML or OpenAPI configuration files.","title":"rest"},{"location":"artefacts/onramps/#stdin","text":"An onramp that takes input from stdin . The default codec is string . The event origin URI set by the onramp is of the form: tremor-stdin://<tremor-host.local>","title":"stdin"},{"location":"artefacts/onramps/#tcp","text":"This listens on a specified port for inbound tcp data. The onramp can leverage preprocessors to segment data before codecs are applied and events are forwarded to pipelines. The default codec is json . The event origin URI set by the onramp is of the form: tremor-tcp://<client_ip>:<client_port>/<config_server_port> Supported configuration options are: host - The IP to listen on port - The Port to listen on Example: onramp : - id : tcp type : tcp preprocessors : - base64 - lines codec : json config : host : \"127.0.0.1\" port : 9000","title":"tcp"},{"location":"artefacts/onramps/#tcp-onramp-example-for-gelf","text":"onramp : - id : gelf-tcp type : tcp preprocessors : - lines-null codec : json config : host : \"127.0.0.1\" port : 12201","title":"tcp onramp example for GELF"},{"location":"artefacts/onramps/#udp","text":"The UDP onramp allows receiving data via UDP datagrams. The default codec is string . The event origin URI set by the onramp is of the form: tremor-udp://<sender_ip>:<sender_port>/<config_receive_port> Supported configuration options are: host - The IP to listen on port - The Port to listen on Example: onramp : - id : udp type : udp codec : json config : host : \"127.0.0.1\" port : 9000","title":"udp"},{"location":"artefacts/onramps/#udp-onramp-example-for-gelf","text":"onramp : - id : gelf-udp type : udp preprocessors : - decompress - gelf-chunking - decompress codec : json config : host : \"127.0.0.1\" port : 12201","title":"udp onramp example for GELF"},{"location":"artefacts/onramps/#ws","text":"This onramp can be linked WebSocket onramp. Receiving either binary or text packages from a WebSocket connection. the url is: ws://<host>:<port>/ The event origin URI set by the onramp is of the form: tremor-ws://<tremor-ws-client-host.remote> Supported configuration options are: host - The IP to listen on port - The Port to listen on Set metadata variables: $binary - true if the incoming WebSocket message came as binary ( false otherwise) Used metadata variables (for reply with linked transports ): $binary - If reply data should be send as binary instead of text (optional. data format defaults to text when not set). Example: onramp : - id : ws type : ws codec : json config : port : 12201 host : \"127.0.0.1\"","title":"ws"},{"location":"artefacts/onramps/#otel","text":"CNCF OpenTelemetry onramp. Listens on TCP port 4317 for gRPC traffic conforming to the CNCF OpenTelemetry protocol specification. Forwards tremor value variants of logs , trace and metrics messages. The onramp is experimental. Supported configuration options are: host - String - The host or IP to listen on port - integer - The TCP port to listen on 'logs' - boolean - Is logging enabled for this instance. Defaults to true . Received logs events are dropped when false . 'metrics' - boolean - Is metrics enabled for this instance. Defaults to true . Defaults to true . Received metrics events are dropped when false . 'trace' - boolean - Is trace enabled for this instance. Defaults to true . Defaults to true . Received trace events are dropped when false . Pipelines that leverage the OpenTelemetry integration can use utility modules in the cncf::otel module to simplify working with the tremor value mapping of the event data. The connector translates the wire level data from protocol buffers to tremor values automatically. Example: onramp : - id : otlp type : otel codec : json config : port : 4317 host : 127.0.0.1","title":"otel"},{"location":"artefacts/postprocessors/","text":"Postprocessors \u00b6 Postprocessors operate on the raw data stream and transform it. They are run after data reaches the codec and do not know or care about tremors internal representation. Online codecs, postprocessors can be chained to perform multiple operations in succession. Supported Postprocessors \u00b6 lines \u00b6 Delimits the output ( events ) into lines ( by '\\n' ). base64 \u00b6 Encodes raw data into base64 encoded bytes. length-prefixed \u00b6 Prefixes the data with a network byte order (big endian) length of the data in bytes. gelf-chunking \u00b6 Splits the data using GELF chunking protocol . compression \u00b6 Compresses event data. Unlike decompression processors, the compression algorithm must be selected. The following compression post-processors are supported. Each format can be configured as a postprocessor. Supported formats: gzip - GZip compression zlib - ZLib compression xz - Xz2 level 9 compression snappy - Snappy compression lz4 - Lz level 4 compression","title":"Postprocessors"},{"location":"artefacts/postprocessors/#postprocessors","text":"Postprocessors operate on the raw data stream and transform it. They are run after data reaches the codec and do not know or care about tremors internal representation. Online codecs, postprocessors can be chained to perform multiple operations in succession.","title":"Postprocessors"},{"location":"artefacts/postprocessors/#supported-postprocessors","text":"","title":"Supported Postprocessors"},{"location":"artefacts/postprocessors/#lines","text":"Delimits the output ( events ) into lines ( by '\\n' ).","title":"lines"},{"location":"artefacts/postprocessors/#base64","text":"Encodes raw data into base64 encoded bytes.","title":"base64"},{"location":"artefacts/postprocessors/#length-prefixed","text":"Prefixes the data with a network byte order (big endian) length of the data in bytes.","title":"length-prefixed"},{"location":"artefacts/postprocessors/#gelf-chunking","text":"Splits the data using GELF chunking protocol .","title":"gelf-chunking"},{"location":"artefacts/postprocessors/#compression","text":"Compresses event data. Unlike decompression processors, the compression algorithm must be selected. The following compression post-processors are supported. Each format can be configured as a postprocessor. Supported formats: gzip - GZip compression zlib - ZLib compression xz - Xz2 level 9 compression snappy - Snappy compression lz4 - Lz level 4 compression","title":"compression"},{"location":"artefacts/preprocessors/","text":"Preprocessors \u00b6 Preprocessors operate on the raw data stream and transform it. They are run before data reaches the codec and do not know or care about tremor's internal representation. Online codecs, preprocessors can be chained to perform multiple operations in succession. Supported Preprocessors \u00b6 lines \u00b6 Splits the input into lines, using character 10 \\n as the line separator. Buffers any line fragment that may be present (after the last line separator), till more data arrives. This makes it ideal for use with streaming onramps like tcp , to break down incoming data into distinct events. Any empty lines present are forwarded as is -- if you want to remove them, please chain the remove-empty preprocessor with this preprocessor. An example: preprocessors : - lines - remove-empty Note: the proliferation of various lines preprocessors here will go away once preprocessors support configuration . lines-null \u00b6 Variant of the lines preprocessor, that uses null byte \\0 as the line separator. lines-pipe \u00b6 Variant of the lines preprocessor, that uses pipe character | as the line separator. lines-no-buffer \u00b6 Variant of the lines preprocessor, that does not buffer any data that may be present after the last line separator -- the fragment is forwarded as is (i.e. treated as a full event). lines-cr-no-buffer \u00b6 Variant of the lines-no-buffer preprocessor, that uses character 13 \\r ( carriage return ) as the line separator. base64 \u00b6 Decodes base64 encoded data to the raw bytes. decompress \u00b6 Decompresses a data stream. It is assumed that each message reaching the decompressor is a complete compressed entity. The compression algorithm is detected automatically from the supported formats. If it can't be detected, the assumption is that the data was decompressed and will be sent on. Errors then can be transparently handled in the codec. Supported formats: gzip zlib xz snappy lz4 gzip \u00b6 Decompress GZ compressed payload. zlib \u00b6 Decompress Zlib ( deflate ) compressed payload. xz \u00b6 Decompress Xz2 ( 7z ) compressed payload. snappy \u00b6 Decompress framed snappy compressed payload ( does not support raw snappy ). lz4 \u00b6 Decompress Lz4 compressed payload. gelf-chunking \u00b6 Reassembles messages that were split apart using the GELF chunking protocol . If the GELF messages were sent compressed, you can decompress them by chaining the decompress preprocessor. An example is documented here -- you may need to apply decompress either before and/or after the reassembly here, depending on how your GELF client(s) behave. remove-empty \u00b6 Removes empty messages (aka zero len). length-prefixed \u00b6 Separates a continuous stream of data based on length prefixing. The length for each package in a stream is based on the first 64 bit decoded as an unsigned big endian value.","title":"Preprocessors"},{"location":"artefacts/preprocessors/#preprocessors","text":"Preprocessors operate on the raw data stream and transform it. They are run before data reaches the codec and do not know or care about tremor's internal representation. Online codecs, preprocessors can be chained to perform multiple operations in succession.","title":"Preprocessors"},{"location":"artefacts/preprocessors/#supported-preprocessors","text":"","title":"Supported Preprocessors"},{"location":"artefacts/preprocessors/#lines","text":"Splits the input into lines, using character 10 \\n as the line separator. Buffers any line fragment that may be present (after the last line separator), till more data arrives. This makes it ideal for use with streaming onramps like tcp , to break down incoming data into distinct events. Any empty lines present are forwarded as is -- if you want to remove them, please chain the remove-empty preprocessor with this preprocessor. An example: preprocessors : - lines - remove-empty Note: the proliferation of various lines preprocessors here will go away once preprocessors support configuration .","title":"lines"},{"location":"artefacts/preprocessors/#lines-null","text":"Variant of the lines preprocessor, that uses null byte \\0 as the line separator.","title":"lines-null"},{"location":"artefacts/preprocessors/#lines-pipe","text":"Variant of the lines preprocessor, that uses pipe character | as the line separator.","title":"lines-pipe"},{"location":"artefacts/preprocessors/#lines-no-buffer","text":"Variant of the lines preprocessor, that does not buffer any data that may be present after the last line separator -- the fragment is forwarded as is (i.e. treated as a full event).","title":"lines-no-buffer"},{"location":"artefacts/preprocessors/#lines-cr-no-buffer","text":"Variant of the lines-no-buffer preprocessor, that uses character 13 \\r ( carriage return ) as the line separator.","title":"lines-cr-no-buffer"},{"location":"artefacts/preprocessors/#base64","text":"Decodes base64 encoded data to the raw bytes.","title":"base64"},{"location":"artefacts/preprocessors/#decompress","text":"Decompresses a data stream. It is assumed that each message reaching the decompressor is a complete compressed entity. The compression algorithm is detected automatically from the supported formats. If it can't be detected, the assumption is that the data was decompressed and will be sent on. Errors then can be transparently handled in the codec. Supported formats: gzip zlib xz snappy lz4","title":"decompress"},{"location":"artefacts/preprocessors/#gzip","text":"Decompress GZ compressed payload.","title":"gzip"},{"location":"artefacts/preprocessors/#zlib","text":"Decompress Zlib ( deflate ) compressed payload.","title":"zlib"},{"location":"artefacts/preprocessors/#xz","text":"Decompress Xz2 ( 7z ) compressed payload.","title":"xz"},{"location":"artefacts/preprocessors/#snappy","text":"Decompress framed snappy compressed payload ( does not support raw snappy ).","title":"snappy"},{"location":"artefacts/preprocessors/#lz4","text":"Decompress Lz4 compressed payload.","title":"lz4"},{"location":"artefacts/preprocessors/#gelf-chunking","text":"Reassembles messages that were split apart using the GELF chunking protocol . If the GELF messages were sent compressed, you can decompress them by chaining the decompress preprocessor. An example is documented here -- you may need to apply decompress either before and/or after the reassembly here, depending on how your GELF client(s) behave.","title":"gelf-chunking"},{"location":"artefacts/preprocessors/#remove-empty","text":"Removes empty messages (aka zero len).","title":"remove-empty"},{"location":"artefacts/preprocessors/#length-prefixed","text":"Separates a continuous stream of data based on length prefixing. The length for each package in a stream is based on the first 64 bit decoded as an unsigned big endian value.","title":"length-prefixed"},{"location":"development/benchmarking/","text":"Benchmarking \u00b6 This is a short synopsis of benchmarking in tremor Scope \u00b6 How to run individual benchmarks comprising the benchmark suite in tremor. Run all benchmarks \u00b6 make bench Run individual benchmarks \u00b6 In order to run individual benchmarks, issue a command of the form: ./bench/run <name-of-benchmark> Where: variable value name-of-benchmark Should be replaced with the basename of the yaml file for that benchmark's pipeline For example: ./bench/run real-workflow-througput-json Will run the 'real-workflow-througput-json' benchmark and publish a HDR histogram to standard output upon completion. it takes about 1 minute to run. Anatomy of a benchmark \u00b6 Tremor benchmarks composed of: An impossibly fast source of data - using the blaster onramp An impossibly fast sink of data - using the blackhole offramp A pipeline that is representative of the workload under measurement Example blaster onramp configuration \u00b6 Blaster loads data from a compressed archive and reads json source data line by line into memory. The in memory cached copy is replayed repeatedly forever. --- onramp : - id : blaster type : blaster codec : json config : source : ./demo/data/data.json.xz Example blackhole offramp configuration \u00b6 Blackhole is a null sink for received data. It also records the latency from ingest time ( created and enqueued in blaster ) to egress ( when it hits the blackhole ) of an event. As such, blaster and blackhole are biased 'unreasonably fast' and they capture intrinsic performance - or, the best case performance that tremor can sustain for the representative workload. Blackhole uses high dynamic range histograms to record performance data ( latency measurements ). offramp : - id : blackhole type : blackhole codec : json config : warmup_secs : 10 stop_after_secs : 100 significant_figures : 2 The pipeline and binding configuration will vary by benchmark, for the real world throughput benchmark they are structured as follows: pipeline : - id : main interface : inputs : - in outputs : - out nodes : - id : runtime op : runtime::tremor config : script : | match event.application of case \"app1\" => let $class = \"applog_app1\", let $rate = 1250, let $dimension = event.application, emit event case \"app2\" => let $class = \"applog_app1\", let $rate = 2500, let $dimension = event.application, emit event case \"app3\" => let $class = \"applog_app1\", let $rate = 18750, let $dimension = event.application, emit event case \"app4\" => let $class = \"applog_app1\", let $rate = 750, let $dimension = event.application, emit event case \"app5\" => let $class = \"applog_app1\", let $rate = 18750, let $dimension = event.application, emit event default => null end; match event.index_type of case \"applog_app6\" => let $class = \"applog_app6\", let $rate = 4500, let $dimensions = event.logger_name, emit event case \"syslog_app1\" => let $class = \"syslog_app1\", let $rate = 2500, let $dimensions = event.syslog_hostname, emit event default => null end; match array::contains(event.tags, \"tag1\") of case true => let $class = \"syslog_app2\", let $rate = 125, let $dimensions = event.syslog_hostname, emit event default => null end; match event.index_type of case \"syslog_app3\" => let $class = \"syslog_app3\", let $rate = 1750, let $dimensions = event.syslog_hostname case \"syslog_app4\" => let $class = \"syslog_app4\", let $rate = 7500, let $dimensions = event.syslog_hostname case \"syslog_app5\" => let $class = \"syslog_app5\", let $rate = 125, let $dimensions = event.syslog_hostname case \"syslog_app6\" => let $class = \"syslog_app6\", let $rate = 3750, let $dimensions = event.syslog_hostname default => let $class = \"default\", let $rate = 250 end; event; - id : group op : grouper::bucket links : in : [ runtime ] runtime : [ group ] group : [ out ] group/overflow : [ out ] Other \u00b6 All the above configuration are provided in a single yaml file and evaluated through the run script. The make target bench calls the run script for each known benchmark file and redirects test / benchmark output into a file. Recommendations \u00b6 To account for run-on-run variance ( difference in measured or recorded performance from one run to another ) we typically run benchmarks repeatedly on development machines with non-essential services such as docker or other services not engaged in the benchmark such as IDEs shut down during benchmarking. Even then, development laptops are not lab quality environments so results should be taken as indicative and with a grain of salt.","title":"Overview"},{"location":"development/benchmarking/#benchmarking","text":"This is a short synopsis of benchmarking in tremor","title":"Benchmarking"},{"location":"development/benchmarking/#scope","text":"How to run individual benchmarks comprising the benchmark suite in tremor.","title":"Scope"},{"location":"development/benchmarking/#run-all-benchmarks","text":"make bench","title":"Run all benchmarks"},{"location":"development/benchmarking/#run-individual-benchmarks","text":"In order to run individual benchmarks, issue a command of the form: ./bench/run <name-of-benchmark> Where: variable value name-of-benchmark Should be replaced with the basename of the yaml file for that benchmark's pipeline For example: ./bench/run real-workflow-througput-json Will run the 'real-workflow-througput-json' benchmark and publish a HDR histogram to standard output upon completion. it takes about 1 minute to run.","title":"Run individual benchmarks"},{"location":"development/benchmarking/#anatomy-of-a-benchmark","text":"Tremor benchmarks composed of: An impossibly fast source of data - using the blaster onramp An impossibly fast sink of data - using the blackhole offramp A pipeline that is representative of the workload under measurement","title":"Anatomy of a benchmark"},{"location":"development/benchmarking/#example-blaster-onramp-configuration","text":"Blaster loads data from a compressed archive and reads json source data line by line into memory. The in memory cached copy is replayed repeatedly forever. --- onramp : - id : blaster type : blaster codec : json config : source : ./demo/data/data.json.xz","title":"Example blaster onramp configuration"},{"location":"development/benchmarking/#example-blackhole-offramp-configuration","text":"Blackhole is a null sink for received data. It also records the latency from ingest time ( created and enqueued in blaster ) to egress ( when it hits the blackhole ) of an event. As such, blaster and blackhole are biased 'unreasonably fast' and they capture intrinsic performance - or, the best case performance that tremor can sustain for the representative workload. Blackhole uses high dynamic range histograms to record performance data ( latency measurements ). offramp : - id : blackhole type : blackhole codec : json config : warmup_secs : 10 stop_after_secs : 100 significant_figures : 2 The pipeline and binding configuration will vary by benchmark, for the real world throughput benchmark they are structured as follows: pipeline : - id : main interface : inputs : - in outputs : - out nodes : - id : runtime op : runtime::tremor config : script : | match event.application of case \"app1\" => let $class = \"applog_app1\", let $rate = 1250, let $dimension = event.application, emit event case \"app2\" => let $class = \"applog_app1\", let $rate = 2500, let $dimension = event.application, emit event case \"app3\" => let $class = \"applog_app1\", let $rate = 18750, let $dimension = event.application, emit event case \"app4\" => let $class = \"applog_app1\", let $rate = 750, let $dimension = event.application, emit event case \"app5\" => let $class = \"applog_app1\", let $rate = 18750, let $dimension = event.application, emit event default => null end; match event.index_type of case \"applog_app6\" => let $class = \"applog_app6\", let $rate = 4500, let $dimensions = event.logger_name, emit event case \"syslog_app1\" => let $class = \"syslog_app1\", let $rate = 2500, let $dimensions = event.syslog_hostname, emit event default => null end; match array::contains(event.tags, \"tag1\") of case true => let $class = \"syslog_app2\", let $rate = 125, let $dimensions = event.syslog_hostname, emit event default => null end; match event.index_type of case \"syslog_app3\" => let $class = \"syslog_app3\", let $rate = 1750, let $dimensions = event.syslog_hostname case \"syslog_app4\" => let $class = \"syslog_app4\", let $rate = 7500, let $dimensions = event.syslog_hostname case \"syslog_app5\" => let $class = \"syslog_app5\", let $rate = 125, let $dimensions = event.syslog_hostname case \"syslog_app6\" => let $class = \"syslog_app6\", let $rate = 3750, let $dimensions = event.syslog_hostname default => let $class = \"default\", let $rate = 250 end; event; - id : group op : grouper::bucket links : in : [ runtime ] runtime : [ group ] group : [ out ] group/overflow : [ out ]","title":"Example blackhole offramp configuration"},{"location":"development/benchmarking/#other","text":"All the above configuration are provided in a single yaml file and evaluated through the run script. The make target bench calls the run script for each known benchmark file and redirects test / benchmark output into a file.","title":"Other"},{"location":"development/benchmarking/#recommendations","text":"To account for run-on-run variance ( difference in measured or recorded performance from one run to another ) we typically run benchmarks repeatedly on development machines with non-essential services such as docker or other services not engaged in the benchmark such as IDEs shut down during benchmarking. Even then, development laptops are not lab quality environments so results should be taken as indicative and with a grain of salt.","title":"Recommendations"},{"location":"development/debugging/","text":"Debugging Tremor \u00b6 This is a short canned synopsis of debugging tremor. rust-lldb \u00b6 We use rust-lldb, to drive breakpoint debugging in tremor. Alternately, rust integration with IntelliJ CLION also offers interactive breakpoint debugging in an IDE environment. Setup on Mac OS X \u00b6 rust-lldb ships with rust so no added tooling is required. Preparing tremor for debugging \u00b6 $ rust-lldb target/debug/tremor ( lldb ) command script import \"/Users/dennis/.rustup/toolchains/stable-x86_64-apple-darwin/lib/rustlib/etc/lldb_rust_formatters.py\" ( lldb ) type summary add --no-value --python-function lldb_rust_formatters.print_val -x \".*\" --category Rust ( lldb ) type category enable Rust ( lldb ) target create \"target/debug/tremor\" Current executable set to 'target/debug/tremor' ( x86_64 ) . ( lldb ) Run tremor under the debugger \u00b6 ( lldb ) run Run to breakpoint for malloc related issues \u00b6 ( lldb ) br set -n malloc_error_break ( lldb ) run Take a backtrace ( stacktrace ) upon hitting a breakpoint \u00b6 ( lldb ) bt List breakpoints \u00b6 ( lldb ) br l Quit lldb \u00b6 ( lldb ) quit References \u00b6 For a more detailed synopsis check out lldb project documentation or the lldb cheatsheet .","title":"Debugging with LLDB"},{"location":"development/debugging/#debugging-tremor","text":"This is a short canned synopsis of debugging tremor.","title":"Debugging Tremor"},{"location":"development/debugging/#rust-lldb","text":"We use rust-lldb, to drive breakpoint debugging in tremor. Alternately, rust integration with IntelliJ CLION also offers interactive breakpoint debugging in an IDE environment.","title":"rust-lldb"},{"location":"development/debugging/#setup-on-mac-os-x","text":"rust-lldb ships with rust so no added tooling is required.","title":"Setup on Mac OS X"},{"location":"development/debugging/#preparing-tremor-for-debugging","text":"$ rust-lldb target/debug/tremor ( lldb ) command script import \"/Users/dennis/.rustup/toolchains/stable-x86_64-apple-darwin/lib/rustlib/etc/lldb_rust_formatters.py\" ( lldb ) type summary add --no-value --python-function lldb_rust_formatters.print_val -x \".*\" --category Rust ( lldb ) type category enable Rust ( lldb ) target create \"target/debug/tremor\" Current executable set to 'target/debug/tremor' ( x86_64 ) . ( lldb )","title":"Preparing tremor for debugging"},{"location":"development/debugging/#run-tremor-under-the-debugger","text":"( lldb ) run","title":"Run tremor under the debugger"},{"location":"development/debugging/#run-to-breakpoint-for-malloc-related-issues","text":"( lldb ) br set -n malloc_error_break ( lldb ) run","title":"Run to breakpoint for malloc related issues"},{"location":"development/debugging/#take-a-backtrace-stacktrace-upon-hitting-a-breakpoint","text":"( lldb ) bt","title":"Take a backtrace ( stacktrace ) upon hitting a breakpoint"},{"location":"development/debugging/#list-breakpoints","text":"( lldb ) br l","title":"List breakpoints"},{"location":"development/debugging/#quit-lldb","text":"( lldb ) quit","title":"Quit lldb"},{"location":"development/debugging/#references","text":"For a more detailed synopsis check out lldb project documentation or the lldb cheatsheet .","title":"References"},{"location":"development/issue-investigation/","text":"Investigations \u00b6 Tools \u00b6 Tools for investigating issues in a production environment. Logs & Metrics (prod) \u00b6 Logs and metrics are the first and most fundamental tool when starting to investigate an issue. While they do not provide a deep insight they allow correlating events from the system under investigation and it is dependants and dependencies. htop (prod) \u00b6 Not really a debugging tool but a nice starting point to see values like memory consumption, CPU load both per process and per thread. Things to look out for are: Memory consumption CPU load Number of processes System Load ( careful this can be misleading !) To a degree top can be used in its place if htop isn't available. perf (prod: Linux) \u00b6 Perf is a tool that is used to profile processes, it gives an overview over functions that CPU time is spend on. While it doesn't give a full trace but it is quite handy in situations where we have a hot function to find where a program is spending it time. It can be run to find where a program spends time: perf record -p <pid> # ctrl-c after a while to stop recording perf report # will pick up the data that record generated It can do a lot more than this. A full tutorial on perf can be found here . netstat (prod) \u00b6 netstat can be helpful to investigate networking state of connections and identify connection issues. Again it can be limited in the scope of docker unless run inside of the container. Please look at the man pages ( man netstat ) for details, this tutorial also covers the basics and useful commands. It also allows to investigate routing tables which can be very handy for connection issues. ping (prod) \u00b6 This isn't exciting but if network related issues are suspected \"can I ping this address\" can cover a lot of ground and is always worth to check. This can also be used to investigate MTU lldb / lldb-rust (prod) \u00b6 Both lldb and for rust more specifically lldb-rust are debuggers. They can attach to a program and step through the application step by step. A good starting guide can be found here . It needs to be said that using lldb should be done with care, it will stop the program and has the potential to crash or terminate it. So far running lldb against a docker contained often lead to the process hanging or crashing. valgrind (dev: Linux) \u00b6 valgrind is used to debug and analyze memory leaks. While originally developed for C/C++ it mostly works with rust code as well - however on OS X it has shown to crash code and as the time of writing this is not a usable tool. More information can be found in the quick start . dtrace (dev: OS X / Windows / BSD) \u00b6 dtrace\u0001 is a tool that allows low impact introspection of running systems. It allows putting probes on specific points in either kernel or user land code and perform analytics on the results. It is a very powerful tool but it requires learning to use efficiently. The dtrace toolkit is a good starting point. Instruments (dev: OS X) \u00b6 OS X comes with a user interface around dtrace that abstracts a lot of the complication away and presents some of the core functionality in bite-sized portions that can be used without requiring to understand the whole functionality of dtrace. It is installed alongside with XCode. Some of the more interesting profiling templates are: Leaks - for finding memory leaks Time Profiler - for finding 'hot' functions System Usage (I/O) - for finding I/O bottle necks strace (prod: Linux) \u00b6 strace is a tool that allows tracing sys calls in Linux, it can be helpful to determine what system calls do occur during an observed issue. For example this can rule in our out specific kernel calls such as IO, muteness, threading, networking and so on. websocat (prod: nay) \u00b6 websocat is a WebSocket client that can be used to interact with trmeors WebSocket onramp and offramp for testing. Methodology \u00b6 There is no 'one way' to investigate an issue as what you find during the process will guide the next steps. Having seen and investigated other problems will help as it gives you a set of heuristics to go \"oh I've seen this before last time it was X\" but it is not required. However there are general methods that have shown to be efficient in trying to investigate an issue. The mot basic approach consists of three steps: Look at the current state, the indicators that have made the issue visible such as logs or htop . Formulate a theory what caused the issue - ideally write it down along with the factors of what promoted you to form this theory. Decide on what would prove your theory - this is where you decide what the next debugging step is, it should ideally either completely confirm, or rule out your theory, that however isn't always possible. If the steps decided on in step 3 fully confirm the issue you're done. If they fully disprove the theory go back to step 1 with the new information. If you neither have prove or disprove for your theory return to 3 and re-formulate what is required to prove it. Especially initially it helps to document each step as you progress along with any changes you made. This helps to prevent double-checking the same theory but also serves as a good learning exercise. Note Don't be shy to re-visit a theory if you found new evidence for it that were not visible in the first attempt. Note It is very helpful to talk to someone, formulating the thoughts in sentences and words often springs new ideas and a second perspective helps with. Logs \u00b6 Out of memory 'issue' 01 \u00b6 Initial observation(source: Metrics & logs) \u00b6 On one of the busier boxes the memory of the process kept growing until the system started swapping. This happened repeatedly, after restarting the process the memory would slowly grow again. 1st theory: a memory leak in tremor-script \u00b6 Most of tremor was written in rust which makes it very hard to create memory leaks, however we integrated two pieces of C code: tremor script and librdkafka those two pipieceses could either directly or by interfacing with rust introduce memory leaks. disprove of 1st theory (oom) \u00b6 We ran tremor extensively using the Leak profiler of Instruments to see if it could spot a memory leak. Any amount of memory profiling we did would not show any unusual allocations and missing deallocations. That made the 1st theory unlikely. However this did show a lot of allocation happening inside librdkafka. 2nd theory: Misconfigured librdkafka operations \u00b6 With the hint gained from the last investigation we looked at the configuration of the system. It showed that a number instances of Kafka onramps were used and the hosts it running on were under spaced regarding to what was communicated to us. Looking at the manual for librdkafka showed that by default each librdkafka instance would allocate up to 1GB of memory as a buffer. Running 4 instances at a 4GB of memory box with additional processes this would eventually lead to running out of memory. prove of 2nd theory \u00b6 Re-deploying tremor with the librdkafka settings set to only use 100MB of memory for buffers we saw the growth disappear after the expected 400MB proving our theory. Infinite loop in librdkafka \u00b6 Initial observation (source: Metrics & logs) \u00b6 On GCP a node jumped to 75% (100% on a single core) and stopped processing data. No Kafka partitions were assigned to that node until it was restarted. This bug hunt carries some complications. We have not been able to replicate the bug outside of a production environment. The environment runs on an outdated Linux, has no internet access to download tools and does not have many of the usual debugging tools available. The bug is rare enough that it take between one and two weeks to reproduce it. 1st theory: network problems on GCP \u00b6 Since this was first and only observed directly after the migration to GCP the initial theory was that networking issues on GCP cause this problem. disprove of 1st theory (lib kafka) \u00b6 To validate the theory we installed the same version of tremor on-premises - and in parallel to working installations and observed if the issue would surface outside of GCP. If it wouldn't we could have reduced it to a GCP related issue. Result : After two weeks of on-prem load we were able to recreate the issue locally, this invalidated our 1st theory. 2nd theory: a bug in our Kafka onramp \u00b6 Inspecting the code that run Kafka we identified a possible issue that we didn't abort on a bad return value to force a reconnect as we suspected the librdkafak wrapper to handle these situations. disprove of 2nd theory \u00b6 We patched our Kafka onramp to explicitly handle this bad returns and provide logs in that case (as it should be rare). We updated to see if this would resolve the issue. Result : After a week the issue re-surfaced without the related logs printed. 3rd theory: incompatible versions of librdkafka and Kafka \u00b6 During the update of tremor we also updated the version of librdkakfa - the Kafka version however remained quite old. We theorized that incompatibility of the newer librdkafka and the old Kafka could cause undefined behavior such as the busy loop. disprove of 3rd theory \u00b6 We prepared a build of tremor with the same version of librdkafka we have been using in the past and updated the boxes in question to see if the bug would re-produce. Result : After a week and a half the bug re-surfaced, making a version conflict between Kafka and librdkafka unlikely. 4th theory: unhandled mutex locks in librdkafka \u00b6 Inspecting a broken process with perf we would observe heavy load in pthread related code and the function rd_kafka_q_pop_serve 36.35% poll libpthread-2.17.so [.] pthread_cond_timedwait@@GLIBC_2.3.2 19.88% poll tremor-server [.] cnd_timedwait 19.14% poll tremor-server [.] rd_kafka_q_pop_serve 15.09% poll tremor-server [.] cnd_timedwait_abs 8.37% poll tremor-server [.] 0x000000000009fcc0 0.27% poll [kernel.kallsyms] [k] __do_softirq After inspecting the code we noticed that the lock obtained in the function did not check if the lock was successful. The else condition in the function could lead to an infinite loop calling the function over and over again. disprove of 4th theory \u00b6 We isolated the hot thread using stop - looking for the tremor thread that was using 100% CPU. We then traced system calls using strace . If librdkafka would attempt to fetch a mutex lock we should see related system calls in the strace output. However observing the process for an hour didn't show a single system call to be made on the hot thread - this ruled out any mutex/kernel-related code to be run in the hot loop. 5th theory: a different librdkafka bug \u00b6 Inspecting the code and the perf output further we noticed that cnd_timedwait_abs was part of the last condition in a while (1) loop. It is reasonable to assume that if we spend time in cnd_timedwait_abs that we hit that part of the loop - if this call would fail the loop would re-run possibly creating an infinite loop. In addition we found a related kafka issue that pointed to this particular location. proving 5th theory \u00b6 We deployed half the nodes with a version of tremor that the newest version of librdkafka (1.0.0) that includes a fix for the issue mentioned above while. We expect the patched nodes to keep stable and the unpatched nodes to eventually fail with the bug. This never happened again. UDP GELF messages issue \u00b6 Issues: Undocumented and nonstandard conform use of Decompress after chunking, these messages were discarded, this was falsely attributed to tremor Setup was spanning the WAN using local Logstas and WAN connected Tremor causing MTU issues that was not documented and falsely attributed to tremor Using a nonstandard conform GELF header for 'uncompressed' caused those datapoints to be discarded, this was falsely attributed to tremor the UDP buffer on the Tremor and logstash hosts were configured differently causing the tremor host to have significantly less buffer causing some messages to be discarded in the OS UDP stack, this was falsely attributed to tremor A tool called udp_replay was used to copy data from a logstash host to a tremor host, this tool truncated the UDP payload, this payload could no longer be decompressed making this packages fail, this was falsely attributed to tremor Some clients send a empty tailing message with a bad segment index (n+1) causing error messages to appear in the logs, this is valid behavior but were flagged as a 'bug' in tremor because logstash does silently drop those. Some clients reuse the sequence number - this can lead to bad messages when UDP packages interleave, tremor reports those incidents and will likely be flagged as buggy because of it. MIO's UDP with edge-poll stops receiving data ticket we switched to level poll which solves this. PCAP files seem to include lots of invalid gelfs when replaying When replaying the pacap the tool shows the following error. thread 'main' panicked at 'called Result::unwrap() on an Err value: Error(WrongField(\"PacketHeader.incl_len (288) > PacketHeader.orig_len (272)\"), State { next_error: None, backtrace: None })', src/libcore/result.rs:999:5","title":"Investigation"},{"location":"development/issue-investigation/#investigations","text":"","title":"Investigations"},{"location":"development/issue-investigation/#tools","text":"Tools for investigating issues in a production environment.","title":"Tools"},{"location":"development/issue-investigation/#logs-metrics-prod","text":"Logs and metrics are the first and most fundamental tool when starting to investigate an issue. While they do not provide a deep insight they allow correlating events from the system under investigation and it is dependants and dependencies.","title":"Logs &amp; Metrics (prod)"},{"location":"development/issue-investigation/#htop-prod","text":"Not really a debugging tool but a nice starting point to see values like memory consumption, CPU load both per process and per thread. Things to look out for are: Memory consumption CPU load Number of processes System Load ( careful this can be misleading !) To a degree top can be used in its place if htop isn't available.","title":"htop (prod)"},{"location":"development/issue-investigation/#perf-prod-linux","text":"Perf is a tool that is used to profile processes, it gives an overview over functions that CPU time is spend on. While it doesn't give a full trace but it is quite handy in situations where we have a hot function to find where a program is spending it time. It can be run to find where a program spends time: perf record -p <pid> # ctrl-c after a while to stop recording perf report # will pick up the data that record generated It can do a lot more than this. A full tutorial on perf can be found here .","title":"perf (prod: Linux)"},{"location":"development/issue-investigation/#netstat-prod","text":"netstat can be helpful to investigate networking state of connections and identify connection issues. Again it can be limited in the scope of docker unless run inside of the container. Please look at the man pages ( man netstat ) for details, this tutorial also covers the basics and useful commands. It also allows to investigate routing tables which can be very handy for connection issues.","title":"netstat (prod)"},{"location":"development/issue-investigation/#ping-prod","text":"This isn't exciting but if network related issues are suspected \"can I ping this address\" can cover a lot of ground and is always worth to check. This can also be used to investigate MTU","title":"ping (prod)"},{"location":"development/issue-investigation/#lldb-lldb-rust-prod","text":"Both lldb and for rust more specifically lldb-rust are debuggers. They can attach to a program and step through the application step by step. A good starting guide can be found here . It needs to be said that using lldb should be done with care, it will stop the program and has the potential to crash or terminate it. So far running lldb against a docker contained often lead to the process hanging or crashing.","title":"lldb / lldb-rust (prod)"},{"location":"development/issue-investigation/#valgrind-dev-linux","text":"valgrind is used to debug and analyze memory leaks. While originally developed for C/C++ it mostly works with rust code as well - however on OS X it has shown to crash code and as the time of writing this is not a usable tool. More information can be found in the quick start .","title":"valgrind (dev: Linux)"},{"location":"development/issue-investigation/#dtrace-dev-os-x-windows-bsd","text":"dtrace\u0001 is a tool that allows low impact introspection of running systems. It allows putting probes on specific points in either kernel or user land code and perform analytics on the results. It is a very powerful tool but it requires learning to use efficiently. The dtrace toolkit is a good starting point.","title":"dtrace (dev: OS X / Windows / BSD)"},{"location":"development/issue-investigation/#instruments-dev-os-x","text":"OS X comes with a user interface around dtrace that abstracts a lot of the complication away and presents some of the core functionality in bite-sized portions that can be used without requiring to understand the whole functionality of dtrace. It is installed alongside with XCode. Some of the more interesting profiling templates are: Leaks - for finding memory leaks Time Profiler - for finding 'hot' functions System Usage (I/O) - for finding I/O bottle necks","title":"Instruments (dev: OS X)"},{"location":"development/issue-investigation/#strace-prod-linux","text":"strace is a tool that allows tracing sys calls in Linux, it can be helpful to determine what system calls do occur during an observed issue. For example this can rule in our out specific kernel calls such as IO, muteness, threading, networking and so on.","title":"strace (prod: Linux)"},{"location":"development/issue-investigation/#websocat-prod-nay","text":"websocat is a WebSocket client that can be used to interact with trmeors WebSocket onramp and offramp for testing.","title":"websocat (prod: nay)"},{"location":"development/issue-investigation/#methodology","text":"There is no 'one way' to investigate an issue as what you find during the process will guide the next steps. Having seen and investigated other problems will help as it gives you a set of heuristics to go \"oh I've seen this before last time it was X\" but it is not required. However there are general methods that have shown to be efficient in trying to investigate an issue. The mot basic approach consists of three steps: Look at the current state, the indicators that have made the issue visible such as logs or htop . Formulate a theory what caused the issue - ideally write it down along with the factors of what promoted you to form this theory. Decide on what would prove your theory - this is where you decide what the next debugging step is, it should ideally either completely confirm, or rule out your theory, that however isn't always possible. If the steps decided on in step 3 fully confirm the issue you're done. If they fully disprove the theory go back to step 1 with the new information. If you neither have prove or disprove for your theory return to 3 and re-formulate what is required to prove it. Especially initially it helps to document each step as you progress along with any changes you made. This helps to prevent double-checking the same theory but also serves as a good learning exercise. Note Don't be shy to re-visit a theory if you found new evidence for it that were not visible in the first attempt. Note It is very helpful to talk to someone, formulating the thoughts in sentences and words often springs new ideas and a second perspective helps with.","title":"Methodology"},{"location":"development/issue-investigation/#logs","text":"","title":"Logs"},{"location":"development/issue-investigation/#out-of-memory-issue-01","text":"","title":"Out of memory 'issue' 01"},{"location":"development/issue-investigation/#initial-observationsource-metrics-logs","text":"On one of the busier boxes the memory of the process kept growing until the system started swapping. This happened repeatedly, after restarting the process the memory would slowly grow again.","title":"Initial observation(source: Metrics &amp; logs)"},{"location":"development/issue-investigation/#1st-theory-a-memory-leak-in-tremor-script","text":"Most of tremor was written in rust which makes it very hard to create memory leaks, however we integrated two pieces of C code: tremor script and librdkafka those two pipieceses could either directly or by interfacing with rust introduce memory leaks.","title":"1st theory: a memory leak in tremor-script"},{"location":"development/issue-investigation/#disprove-of-1st-theory-oom","text":"We ran tremor extensively using the Leak profiler of Instruments to see if it could spot a memory leak. Any amount of memory profiling we did would not show any unusual allocations and missing deallocations. That made the 1st theory unlikely. However this did show a lot of allocation happening inside librdkafka.","title":"disprove of 1st theory (oom)"},{"location":"development/issue-investigation/#2nd-theory-misconfigured-librdkafka-operations","text":"With the hint gained from the last investigation we looked at the configuration of the system. It showed that a number instances of Kafka onramps were used and the hosts it running on were under spaced regarding to what was communicated to us. Looking at the manual for librdkafka showed that by default each librdkafka instance would allocate up to 1GB of memory as a buffer. Running 4 instances at a 4GB of memory box with additional processes this would eventually lead to running out of memory.","title":"2nd theory: Misconfigured librdkafka operations"},{"location":"development/issue-investigation/#prove-of-2nd-theory","text":"Re-deploying tremor with the librdkafka settings set to only use 100MB of memory for buffers we saw the growth disappear after the expected 400MB proving our theory.","title":"prove of 2nd theory"},{"location":"development/issue-investigation/#infinite-loop-in-librdkafka","text":"","title":"Infinite loop in librdkafka"},{"location":"development/issue-investigation/#initial-observation-source-metrics-logs","text":"On GCP a node jumped to 75% (100% on a single core) and stopped processing data. No Kafka partitions were assigned to that node until it was restarted. This bug hunt carries some complications. We have not been able to replicate the bug outside of a production environment. The environment runs on an outdated Linux, has no internet access to download tools and does not have many of the usual debugging tools available. The bug is rare enough that it take between one and two weeks to reproduce it.","title":"Initial observation (source: Metrics &amp; logs)"},{"location":"development/issue-investigation/#1st-theory-network-problems-on-gcp","text":"Since this was first and only observed directly after the migration to GCP the initial theory was that networking issues on GCP cause this problem.","title":"1st theory: network problems on GCP"},{"location":"development/issue-investigation/#disprove-of-1st-theory-lib-kafka","text":"To validate the theory we installed the same version of tremor on-premises - and in parallel to working installations and observed if the issue would surface outside of GCP. If it wouldn't we could have reduced it to a GCP related issue. Result : After two weeks of on-prem load we were able to recreate the issue locally, this invalidated our 1st theory.","title":"disprove of 1st theory (lib kafka)"},{"location":"development/issue-investigation/#2nd-theory-a-bug-in-our-kafka-onramp","text":"Inspecting the code that run Kafka we identified a possible issue that we didn't abort on a bad return value to force a reconnect as we suspected the librdkafak wrapper to handle these situations.","title":"2nd theory: a bug in our Kafka onramp"},{"location":"development/issue-investigation/#disprove-of-2nd-theory","text":"We patched our Kafka onramp to explicitly handle this bad returns and provide logs in that case (as it should be rare). We updated to see if this would resolve the issue. Result : After a week the issue re-surfaced without the related logs printed.","title":"disprove of 2nd theory"},{"location":"development/issue-investigation/#3rd-theory-incompatible-versions-of-librdkafka-and-kafka","text":"During the update of tremor we also updated the version of librdkakfa - the Kafka version however remained quite old. We theorized that incompatibility of the newer librdkafka and the old Kafka could cause undefined behavior such as the busy loop.","title":"3rd theory: incompatible versions of librdkafka and Kafka"},{"location":"development/issue-investigation/#disprove-of-3rd-theory","text":"We prepared a build of tremor with the same version of librdkafka we have been using in the past and updated the boxes in question to see if the bug would re-produce. Result : After a week and a half the bug re-surfaced, making a version conflict between Kafka and librdkafka unlikely.","title":"disprove of 3rd theory"},{"location":"development/issue-investigation/#4th-theory-unhandled-mutex-locks-in-librdkafka","text":"Inspecting a broken process with perf we would observe heavy load in pthread related code and the function rd_kafka_q_pop_serve 36.35% poll libpthread-2.17.so [.] pthread_cond_timedwait@@GLIBC_2.3.2 19.88% poll tremor-server [.] cnd_timedwait 19.14% poll tremor-server [.] rd_kafka_q_pop_serve 15.09% poll tremor-server [.] cnd_timedwait_abs 8.37% poll tremor-server [.] 0x000000000009fcc0 0.27% poll [kernel.kallsyms] [k] __do_softirq After inspecting the code we noticed that the lock obtained in the function did not check if the lock was successful. The else condition in the function could lead to an infinite loop calling the function over and over again.","title":"4th theory: unhandled mutex locks in librdkafka"},{"location":"development/issue-investigation/#disprove-of-4th-theory","text":"We isolated the hot thread using stop - looking for the tremor thread that was using 100% CPU. We then traced system calls using strace . If librdkafka would attempt to fetch a mutex lock we should see related system calls in the strace output. However observing the process for an hour didn't show a single system call to be made on the hot thread - this ruled out any mutex/kernel-related code to be run in the hot loop.","title":"disprove of 4th theory"},{"location":"development/issue-investigation/#5th-theory-a-different-librdkafka-bug","text":"Inspecting the code and the perf output further we noticed that cnd_timedwait_abs was part of the last condition in a while (1) loop. It is reasonable to assume that if we spend time in cnd_timedwait_abs that we hit that part of the loop - if this call would fail the loop would re-run possibly creating an infinite loop. In addition we found a related kafka issue that pointed to this particular location.","title":"5th theory: a different librdkafka bug"},{"location":"development/issue-investigation/#proving-5th-theory","text":"We deployed half the nodes with a version of tremor that the newest version of librdkafka (1.0.0) that includes a fix for the issue mentioned above while. We expect the patched nodes to keep stable and the unpatched nodes to eventually fail with the bug. This never happened again.","title":"proving 5th theory"},{"location":"development/issue-investigation/#udp-gelf-messages-issue","text":"Issues: Undocumented and nonstandard conform use of Decompress after chunking, these messages were discarded, this was falsely attributed to tremor Setup was spanning the WAN using local Logstas and WAN connected Tremor causing MTU issues that was not documented and falsely attributed to tremor Using a nonstandard conform GELF header for 'uncompressed' caused those datapoints to be discarded, this was falsely attributed to tremor the UDP buffer on the Tremor and logstash hosts were configured differently causing the tremor host to have significantly less buffer causing some messages to be discarded in the OS UDP stack, this was falsely attributed to tremor A tool called udp_replay was used to copy data from a logstash host to a tremor host, this tool truncated the UDP payload, this payload could no longer be decompressed making this packages fail, this was falsely attributed to tremor Some clients send a empty tailing message with a bad segment index (n+1) causing error messages to appear in the logs, this is valid behavior but were flagged as a 'bug' in tremor because logstash does silently drop those. Some clients reuse the sequence number - this can lead to bad messages when UDP packages interleave, tremor reports those incidents and will likely be flagged as buggy because of it. MIO's UDP with edge-poll stops receiving data ticket we switched to level poll which solves this. PCAP files seem to include lots of invalid gelfs when replaying When replaying the pacap the tool shows the following error. thread 'main' panicked at 'called Result::unwrap() on an Err value: Error(WrongField(\"PacketHeader.incl_len (288) > PacketHeader.orig_len (272)\"), State { next_error: None, backtrace: None })', src/libcore/result.rs:999:5","title":"UDP GELF messages issue"},{"location":"development/profiling/","text":"Profiling Tremor \u00b6 This is a short canned synopsis of profiling tremor. Valgrind \u00b6 We use valgrind, specifically callgrind, to drive call graph optimizations in tremor. Setup on Mac OS X \u00b6 Install valgrind via homebrew brew install valgrind Dependent utilities \u00b6 The Google Performance Toolkit, gprof2dot, qcachegrind are also required / useful. brew install gprof2dot brew install gperftools brew install qcachegrind Basic profiling via tremor \u00b6 This is good enough for initial high-level exploration. For example, run a tremor pipeline against recorded data in data.json valgrind --tool = callgrind target/debug/tremor run tests/configs/ut.combine3-op.yaml data.json Analysing results via google perf toolkit and graphviz for static call flow diagrams gprof2dot -f callgrind callgrind.out.93972 > pipe.dot dot -Tpng pipe.dot -o pipe.png && open pipe.png Interactive analysis via QCachegrind / KCachegrind qcachegrind callgrind.out.93972 The profiling ( sampling ) frequency is tunable and SHOULD be tuned for each run, eg: RUST_BACKTRACE = 1 PROFILEFREQUENCY = 1000 valgrind --tool = callgrind \\ target/release/tremor run examples/config-spike5.tremor data.json Note When using a release build make sure debug symbols are configured in Cargo.toml and enable link time optimisations ( LTO ). Flamegraphs \u00b6 Install rust flamegraph support: cargo install flamegraph Perform a benchmark run with flamegraph support ( on Mac OS X ): flamegraph target/release/tremor server run -f bench/real-workflow-througput-json.yaml bench/link.yaml This generates a flamegraph.svg file which can be opened from a browser. open flamegraph.svg","title":"Profiling"},{"location":"development/profiling/#profiling-tremor","text":"This is a short canned synopsis of profiling tremor.","title":"Profiling Tremor"},{"location":"development/profiling/#valgrind","text":"We use valgrind, specifically callgrind, to drive call graph optimizations in tremor.","title":"Valgrind"},{"location":"development/profiling/#setup-on-mac-os-x","text":"Install valgrind via homebrew brew install valgrind","title":"Setup on Mac OS X"},{"location":"development/profiling/#dependent-utilities","text":"The Google Performance Toolkit, gprof2dot, qcachegrind are also required / useful. brew install gprof2dot brew install gperftools brew install qcachegrind","title":"Dependent utilities"},{"location":"development/profiling/#basic-profiling-via-tremor","text":"This is good enough for initial high-level exploration. For example, run a tremor pipeline against recorded data in data.json valgrind --tool = callgrind target/debug/tremor run tests/configs/ut.combine3-op.yaml data.json Analysing results via google perf toolkit and graphviz for static call flow diagrams gprof2dot -f callgrind callgrind.out.93972 > pipe.dot dot -Tpng pipe.dot -o pipe.png && open pipe.png Interactive analysis via QCachegrind / KCachegrind qcachegrind callgrind.out.93972 The profiling ( sampling ) frequency is tunable and SHOULD be tuned for each run, eg: RUST_BACKTRACE = 1 PROFILEFREQUENCY = 1000 valgrind --tool = callgrind \\ target/release/tremor run examples/config-spike5.tremor data.json Note When using a release build make sure debug symbols are configured in Cargo.toml and enable link time optimisations ( LTO ).","title":"Basic profiling via tremor"},{"location":"development/profiling/#flamegraphs","text":"Install rust flamegraph support: cargo install flamegraph Perform a benchmark run with flamegraph support ( on Mac OS X ): flamegraph target/release/tremor server run -f bench/real-workflow-througput-json.yaml bench/link.yaml This generates a flamegraph.svg file which can be opened from a browser. open flamegraph.svg","title":"Flamegraphs"},{"location":"development/quick-start/","text":"Quick Start Guide \u00b6 This page explains how to get tremor running on a local system for development or testing. There are 2 ways of installing tremor: Installing tremor on the system without docker Using docker Without Docker \u00b6 Install Rust \u00b6 Tremor can be run on any platform without using docker by installing the rust ecosystem. To install the rust ecosystem, you can use rustup which is a toolchain installer. Rustup will install all the necessary tools required for rust, which includes rustc (the compiler) and cargo (package manager). Tremor is built using the latest stable toolchain, so when asked to select the toolchain during installation, select stable. macOS/Linux \u00b6 Run the following command and follow the on-screen instructions: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Now activate it by adding source $HOME/.cargo/env to your .rc file and open a new console. For building tremor on macOS, you also need to install xcode and the commandline tools. Windows \u00b6 Pre-requisite: Rust requires the Microsoft C++ build tools for Visual Studio 2013 or later. You can get those from: https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019. During installation, make sure Windows 10 SDK is selected (should be on by default). Now download the rustup installer by clicking here , run it and follow the on-screen instructions. Additional Libraries \u00b6 macOS \u00b6 brew install openssl brew install autoconf brew install re2c brew install bison #make sure to follow the printed instructions! Ubuntu \u00b6 sudo apt install libssl-dev libclang-dev cmake Windows \u00b6 cmake : choose the latest stable release (3.16 at the time of writing) clang : choose windows pre-built binaries for the latest release that has it (9.0.0 at the time of writing) Make sure the cmake and llvm binaries are added to the system path for at least the current user (if not all), as part of the installation process. Since openssl does not provided official builds, you can get it via vcpkg . First, set up vcpkg: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg .\\bootstrap-vcpkg.bat .\\vcpkg integrate install # hook up user-wide integration Now install openssl with: .\\vcpkg install openssl:x64-windows-static To pick up the openssl libs during tremor build, you also have to set the OPENSSL_DIR environment variable right now. Example: set OPENSSL_DIR=C:\\Users\\juju\\TREMOR\\vcpkg\\installed\\x64-windows-static Technically, the rust openssl crate will try to discover the openssl libs via vcpkg (as long as env var VCPKGRS_DYNAMIC is set), but that is not working for the recent openssl libs supplied by vcpkg. There's a fix for it and once that lands in a release for rust-openssl (and also starts getting used by tremor dependencies), we won't have to rely on the OPENSSL_DIR var. Running Tremor \u00b6 After installing rust and cloning the repository, you can start tremor server by running the following from the root ( tremor-runtime ) directory: cargo run -p tremor-cli -- server run And to run the test suite, you can run: cargo test This will run all the tests in the suite, except those which are feature-gated and not needed to quickly test tremor. Rustfmt \u00b6 Rustfmt is a tool for formatting rust code according to style guidelines. It maintains consistency in the style in the entire project. To install rustfmt run: rustup component add rustfmt To run rustfmt on the project, run the following command: cargo fmt Clippy \u00b6 Clippy is a linting tool that catches common mistakes and improves the rust code. It is available as a toolchain component and can be installed by running: rustup component add clippy To run clippy , run the following command: cargo clippy Rustfix \u00b6 Rustfix automatically applies suggestions made by rustc. There are two ways of using rustfix - either by adding it as a library to Cargo.toml or by installing it as a cargo subcommand by running: cargo install cargo-fix To run it, you can run: cargo fix Tree \u00b6 Cargo tree is a subcommand that visualizes a crate's dependency-graph and display a tree structure of them. To install it: cargo install cargo-tree To run it: cargo tree Flamegraph \u00b6 Flamegraph is a profiling tool that visualises where time is spent in a program. It generates a SVG image based on the current location of the code and the function that were called to get there. To install it: cargo install cargo-flamegraph To run it: cargo flamegraph Integration Tests \u00b6 Tremor contains integration tests that tests it from a user's perspective. To run the integration tests you can run: make it With Docker \u00b6 Tremor contains a Dockerfile which makes it easier to run and build using docker. It also contains a makefile so that common docker commands. Make sure docker has at least 4GB of memory. To build tremor you can run: make image To run the images: make demo","title":"Quick Start Guide"},{"location":"development/quick-start/#quick-start-guide","text":"This page explains how to get tremor running on a local system for development or testing. There are 2 ways of installing tremor: Installing tremor on the system without docker Using docker","title":"Quick Start Guide"},{"location":"development/quick-start/#without-docker","text":"","title":"Without Docker"},{"location":"development/quick-start/#install-rust","text":"Tremor can be run on any platform without using docker by installing the rust ecosystem. To install the rust ecosystem, you can use rustup which is a toolchain installer. Rustup will install all the necessary tools required for rust, which includes rustc (the compiler) and cargo (package manager). Tremor is built using the latest stable toolchain, so when asked to select the toolchain during installation, select stable.","title":"Install Rust"},{"location":"development/quick-start/#macoslinux","text":"Run the following command and follow the on-screen instructions: curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh Now activate it by adding source $HOME/.cargo/env to your .rc file and open a new console. For building tremor on macOS, you also need to install xcode and the commandline tools.","title":"macOS/Linux"},{"location":"development/quick-start/#windows","text":"Pre-requisite: Rust requires the Microsoft C++ build tools for Visual Studio 2013 or later. You can get those from: https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019. During installation, make sure Windows 10 SDK is selected (should be on by default). Now download the rustup installer by clicking here , run it and follow the on-screen instructions.","title":"Windows"},{"location":"development/quick-start/#additional-libraries","text":"","title":"Additional Libraries"},{"location":"development/quick-start/#macos","text":"brew install openssl brew install autoconf brew install re2c brew install bison #make sure to follow the printed instructions!","title":"macOS"},{"location":"development/quick-start/#ubuntu","text":"sudo apt install libssl-dev libclang-dev cmake","title":"Ubuntu"},{"location":"development/quick-start/#windows_1","text":"cmake : choose the latest stable release (3.16 at the time of writing) clang : choose windows pre-built binaries for the latest release that has it (9.0.0 at the time of writing) Make sure the cmake and llvm binaries are added to the system path for at least the current user (if not all), as part of the installation process. Since openssl does not provided official builds, you can get it via vcpkg . First, set up vcpkg: git clone https://github.com/Microsoft/vcpkg.git cd vcpkg .\\bootstrap-vcpkg.bat .\\vcpkg integrate install # hook up user-wide integration Now install openssl with: .\\vcpkg install openssl:x64-windows-static To pick up the openssl libs during tremor build, you also have to set the OPENSSL_DIR environment variable right now. Example: set OPENSSL_DIR=C:\\Users\\juju\\TREMOR\\vcpkg\\installed\\x64-windows-static Technically, the rust openssl crate will try to discover the openssl libs via vcpkg (as long as env var VCPKGRS_DYNAMIC is set), but that is not working for the recent openssl libs supplied by vcpkg. There's a fix for it and once that lands in a release for rust-openssl (and also starts getting used by tremor dependencies), we won't have to rely on the OPENSSL_DIR var.","title":"Windows"},{"location":"development/quick-start/#running-tremor","text":"After installing rust and cloning the repository, you can start tremor server by running the following from the root ( tremor-runtime ) directory: cargo run -p tremor-cli -- server run And to run the test suite, you can run: cargo test This will run all the tests in the suite, except those which are feature-gated and not needed to quickly test tremor.","title":"Running Tremor"},{"location":"development/quick-start/#rustfmt","text":"Rustfmt is a tool for formatting rust code according to style guidelines. It maintains consistency in the style in the entire project. To install rustfmt run: rustup component add rustfmt To run rustfmt on the project, run the following command: cargo fmt","title":"Rustfmt"},{"location":"development/quick-start/#clippy","text":"Clippy is a linting tool that catches common mistakes and improves the rust code. It is available as a toolchain component and can be installed by running: rustup component add clippy To run clippy , run the following command: cargo clippy","title":"Clippy"},{"location":"development/quick-start/#rustfix","text":"Rustfix automatically applies suggestions made by rustc. There are two ways of using rustfix - either by adding it as a library to Cargo.toml or by installing it as a cargo subcommand by running: cargo install cargo-fix To run it, you can run: cargo fix","title":"Rustfix"},{"location":"development/quick-start/#tree","text":"Cargo tree is a subcommand that visualizes a crate's dependency-graph and display a tree structure of them. To install it: cargo install cargo-tree To run it: cargo tree","title":"Tree"},{"location":"development/quick-start/#flamegraph","text":"Flamegraph is a profiling tool that visualises where time is spent in a program. It generates a SVG image based on the current location of the code and the function that were called to get there. To install it: cargo install cargo-flamegraph To run it: cargo flamegraph","title":"Flamegraph"},{"location":"development/quick-start/#integration-tests","text":"Tremor contains integration tests that tests it from a user's perspective. To run the integration tests you can run: make it","title":"Integration Tests"},{"location":"development/quick-start/#with-docker","text":"Tremor contains a Dockerfile which makes it easier to run and build using docker. It also contains a makefile so that common docker commands. Make sure docker has at least 4GB of memory. To build tremor you can run: make image To run the images: make demo","title":"With Docker"},{"location":"development/testing/","text":"Testing \u00b6 This is a short canned synopsis of testing in the tremor project. Running Internal tests \u00b6 cargo run -p tremor-cli -- test all tremor-cli/tests EQC \u00b6 EQC or 'QuickCheck' is a specification-based testing tool for Erlang supporting a test methodology called property-based testing. Programs are tested by writing properties - preconditions, postconditions and invariants. QuickCheck uses random generation to create constructive ( should pass ) and destructive ( should fail ) tests given the specified properties. This allows suitably defined specifications to cover a far greater set of use cases than would ordinarily be possible to write manually. Further to this, QuickCheck can reduce a set of failing test cases to the minimal test case that forces any failing test to fail its specification. This drastically reduces the amount of QA and developer time required to verify or prove a piece of code works given a suitably defined specification. Start tremor \u00b6 You need to start the tremor to run the tests: cargo run -p tremor -- server run Running EQC \u00b6 In tremor-runtime/tremor-erl run: rebar3 as eqc eqc","title":"Testing"},{"location":"development/testing/#testing","text":"This is a short canned synopsis of testing in the tremor project.","title":"Testing"},{"location":"development/testing/#running-internal-tests","text":"cargo run -p tremor-cli -- test all tremor-cli/tests","title":"Running Internal tests"},{"location":"development/testing/#eqc","text":"EQC or 'QuickCheck' is a specification-based testing tool for Erlang supporting a test methodology called property-based testing. Programs are tested by writing properties - preconditions, postconditions and invariants. QuickCheck uses random generation to create constructive ( should pass ) and destructive ( should fail ) tests given the specified properties. This allows suitably defined specifications to cover a far greater set of use cases than would ordinarily be possible to write manually. Further to this, QuickCheck can reduce a set of failing test cases to the minimal test case that forces any failing test to fail its specification. This drastically reduces the amount of QA and developer time required to verify or prove a piece of code works given a suitably defined specification.","title":"EQC"},{"location":"development/testing/#start-tremor","text":"You need to start the tremor to run the tests: cargo run -p tremor -- server run","title":"Start tremor"},{"location":"development/testing/#running-eqc","text":"In tremor-runtime/tremor-erl run: rebar3 as eqc eqc","title":"Running EQC"},{"location":"development/benchmarks/LogstashBenchmark/","text":"Logstash Benchmark \u00b6 This section documents how to setup logstash for benchmarking to compare against tremor-script. The following benchmarks were taken with tremor v0.5.0 on June 2019. Logstash \u00b6 Assert that JDK 8 or higher is on your system path $ java -version java version \"1.8.0_192-ea\" Java ( TM ) SE Runtime Environment ( build 1 .8.0_192-ea-b04 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 25 .192-b04, mixed mode ) $ javac -version javac 1 .8.0_192-ea Assert that JRuby 9.2 or higher is on your system path $ jruby -v jruby 9 .2.6.0 ( 2 .5.3 ) 2019 -02-11 15ba00b Java HotSpot ( TM ) 64 -Bit Server VM 25 .192-b04 on 1 .8.0_192-ea-b04 +jit [ darwin-x86_64 ] Assert that the rake and bundler ruby tools are installed $ gem install rake bundler Fetching: rake-12.3.2.gem ( 100 % ) Successfully installed rake-12.3.2 Fetching: bundler-2.0.2.gem ( 100 % ) Successfully installed bundler-2.0.2 2 gems installed Clone and build logstash and its benchmark tool git clone https://github.com/elastic/logstash cd logstash gradle clean assemble cd tools/benchmark-cli gradle clean assemble Benchmark methodology \u00b6 In the first ( this ) version, this was a manual process Identify performance related tunables for logstash \u00b6 Number of logstash workers ( default 2 ) Batch size for micro-batching queues in logstash workers ( default 128 ) Identity baseline benchmark \u00b6 Literally named baseline and is an equivalent in functionality to tremor's empty-passthrough-json benchmark For each logstash tunable, run a benchmark \u00b6 Given the baseline logstash benchmark and record results and initial analysis Document command line arguments required to evaluated each run/iteration $ java -cp build/libs/benchmark-cli.jar org.logstash.benchmark.cli.Main \\ --testcase = baseline --distribution-version = 5 .5.0 \\ --ls-workers <num-workers> \\ --ls-batch-size <batch-size> For each tremor tunable, run a benchmark \u00b6 Given the empty-passthrough-json equivalent benchmark. Tremor has no performance-related tunables. Tremor's benchmark framework has no performance-related tunables for the empty-passthrough-json equivalent benchmark Record results Benchmark conditioning and environment \u00b6 The benchmark-cli tool that ships with logstash suffers from a number of issues. The number of events in each run is fixed and limited to 1 million events per run. No accommodation is made for warmup to ensure that the JVM has reached a stable state before results recording begins. The framework also incorrectly terminates after each run once 1 million results have been submitted. It should not complete until all workers have drained their respective queues and the benchmark reaches a quiescent state. Quiescence is not asserted. This means that the number of recorded processed events can be less than the configured target by a significant margin. As these are micro-benchmarks and we are concerned with Order of Magnitude differences in performance we have not expended effort resolving these issues. A further issue with the logstash benchmark-cli tool is that the results suffer from the coordinated omission problem. The coordinated omission problem, is a term first-coined by Gil Tene based on the observations in benchmarking the Azul Vega hardware and Zing JVM with C4 garbage collector and related ZTS subsystems). In a nutshell, coordinated-omission is where a benchmarking tool ( usually unintentionally ) incorrectly records events under measurement time spans by failing to record the intended verses actual time to record. Specifically, it is insufficient to record the start time and end time of a particular event of interest. Capturing the start and end time allows the delta or servicing time to be computed. It does not capture any synchronization overhead, waiting time, delays or other system-induced hiccups introduced between up to the point the event should have started. We fail to capture unintentional drift introduced artificially by the benchmark framework when CO is in force. Well-designed benchmarks should be CO-free. Tremor's benchmarking facility allows events to be injected at a fixed frequency. This tactic ( which more specifically, pins the intended commencement time for an event to begin processing to a starting epoch ) is sufficient to practically account for any hiccups or drift in expected verses actual inter-arrival based on designed constraints in a benchmark framework. By selecting a fixed static frequency any measured hiccups should be outside of the control of the benchmark framework - they are either artifacts of the scenario under test, or the system upon which it is being tested. These conditions are optimal in all benchmark testing, but an absolute necessity for any latency-sensitive testing, especially where fine-grained statistic quartiles are being computed if they are to impart results that are fit for low-level analysis and interpretation. Coordinated-omission-free benchmarks are important for micro-benchmarking and latency-driven benchmarking. However, as we are interested in Order of Magnitude ( finger in the air ) characteristics rather than isolating long-tail latencies or understanding fine-grained latency characteristics on a per-event basis ( say, at the long-tail of performance beyond the 99.99th percentile ) In short, tremor makes some effort to account for coordinated-omission where relevant, but logstash's benchmark framework does not. However, as we are taking a 50,000-foot view of characteristic throughput and are not focusing on specific per-event latency characteristics the identified issues are negligible for our analysis. It would be incorrect however, to focus on per-event performance characteristics or focus in on specific latency quartiles or throughput quartiles to derive any significance. Such an analysis would require more effort and would not necessarily deliver any greater value. We have not used lab-quality environments to run any of the benchmarks. All benchmarks were run on the same development grade laptop ( not ideal ) with the same background processes active on an intel / Mac OS X x86_64 environment. As such we consider the results indicative of characteristics and good enough for high-level analysis. It should, however, be a small task to follow this report to replicate the characteristic results detailed in this report and accompanying evidence and to replicate the same on similar resources. Baseline Analysis \u00b6 Logstash's benchmark-cli was put through 40 variations of its two tunable parameters. We tested with 1, 2, 4 and 8 logstash workers. We tested each worker configuration with queue batch size bounds of 1, 2, 4, 8, 16, 32, 64, 128, 256 and 512. The best of 3 runs was recorded. As tremor has no tunables we recorded the best of 3 runs. Logstash is configured out of the box for 2 workers and a batch size of 128. There are marginally better configurations possible with 4 or 8 workers showing marginal throughput benefits given the use-case selected. Configuring a single worker has a significant negative impact on performance. As such logstash is well-configured out of the box, at least for typical development or non-production activities. The optimal configuration on the test machine was configured with 8 workers ( default 2 ) and a batch size of 256 ( default 128 ). This is less than 1% of a difference. Generally speaking, batch sizes of less than 32 tend to significantly reduce throughput for any number of configured workers. Also, generally speaking, the improvement from 2 worker threads to more does not demonstrate any improvement in scaling. As such multi-core scaling with logstash does not seem to be of much benefit beyond 2 workers ( threads ). It should be noted that the benchmark creates artificial conditions and that the baseline working-set is atypical of production working sets. On the other hand, this conditioning is the same with respect to tremor whose baseline benchmark is equivalent. In both cases we ingest, forward and publish an event or pass it through the system under test. As logstash is configured for a fixed ceiling of events ( 1 million ) for a benchmark run, and tremor is configured for a specific test duration ( as many events as possible in 40 seconds ) we need to baseline the results. We bias in favor of tremor as normative and compute the effective throughput at 40 seconds for each logstash run. So a 63 second run with 8 workes and a batch size of 1 38 seconds run at batch size 256 respectively for logstash is counted as the equivalent 40 seconds run as follows: Logstash Default (W2 W128) Logstash Worst (W1 B1) Logstash Best (W8 B256) Tremor (baseline) 1033811 466708 1041943 21402853 Note that we truncate / floor round the equivalent logstash 40-second results for each selected configuration. Relative to the worst case logstash benchmark run performance: Logstash W2 W128 ( default ) Logstash W8 B1 Logstash W8 B256 Tremor ( baseline ) 2.22 1 2.23 45.86 Logstash can itself benefit from at least a 2x improvement, and this is consistent with the default out of the box configuration. Tremor, however, is a factor of 45 better than the logstash worst case. Tremor, compared to the logstash best case, is still a factor of 20.54 higher throughput. So the total effective range of improvement for the given benchmark ( all other things considered equal ) is somewhere between a 20x to 45x increase in throughput favoring tremor over logstash for the baseline use case based on experimental conditions detailed in this report. Production Analysis \u00b6 We don't typically deploy logstash or tremor into production as a distribution proxy or interconnect that passes through events. Logstash would be a pretty bad choice compared to tremor. But tremor, although it has excellent conditioning and is designed for elegantly handling back-pressure and saturation conditions for log shipping and distribution - it does not provide the delivery semantics, retention and feature-set of technologies such as Kafka. For the level 1 traffic limiting, traffic shaping and rate-limiting use cases in Level 3 Logging at Wayfair for which tremor was originally designed we have seen a 7x-8x improvement in density compared to logstash for v0.4 of tremor. v0.5 should increase this to the 10x ballpark as we benefit from SIMD vectorization of JSON deserialization. However, there is further room for improvement as the tremor-script language has evolved to handle level 2 logging to replace ruby and logstash with far richer configuration than level 1. In v0.5 performance is a non-goal; as such there are many optimisations to the new tremor-script domain-specific-langauge that we have yet to undertake - so in practice for level 2 logging we won't see the full benefit of SIMD vectorization as some of those gains are ammortised by additional essential complexity of providing a richer DSL to support replacing logstash in level 2. Indicatively, we stand to see a range of 20x-40x improvement. In production we have observed closer to a 7x-8x improvement in density with the L3 replacement, and with the v0.4 upgrade to L1 in GCP pre-live. We expect a further modest incremental improvement in L1 with v0.5, and a good ~10x over logstash for L2 this ( v0.5 ) release. Apache Logs Scenario \u00b6 Logstash ships with a benchmark where logstash is configured for Apache log processing and elementization. We have ported this benchmark to tremor for coparative purposes. Tremor processes 44109658 messages in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 751 seconds to process the same 6900000 log records. This is 48x speedup in favour of tremor. Logging Level 2 Replacement Scenario \u00b6 Tremor ships with a benchmark where the enrichment component of logstash replacement running in production has been used to drive the implementation of tremor-script features in the v0.5 release. We have minimally modified the logstash benchmark client to support running logstash equivalent configuration of tremor ( or vice-versa ) to get as close to possible to a fair apples-to-apples comparison. We have stopped short of fixing fundemental issues with the logstash benchmark framework itself. Tremor process 2262222 records in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 66 seconds to process 200000 records. This equates to a ballpark 7.5x speedup in favour of tremor. Logging Level 2 Replacement bad case \u00b6 Benchmarking the validation/transformation scenario involves a more complex script with loops and deeper nested expressions. Here some of the shortcuts we took to implement the new tremor script in a reasonable timeframe surface as performance bottlenecks. Again ran the transformation script with both Logstash as well tremor with configurations as close to equivalent as possible. Tremor processes 73132 records in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 87 seconds to process 150000 records. In this case tremor is about 2.3x slower then a comparable logstasv configuration.","title":"Logstash"},{"location":"development/benchmarks/LogstashBenchmark/#logstash-benchmark","text":"This section documents how to setup logstash for benchmarking to compare against tremor-script. The following benchmarks were taken with tremor v0.5.0 on June 2019.","title":"Logstash Benchmark"},{"location":"development/benchmarks/LogstashBenchmark/#logstash","text":"Assert that JDK 8 or higher is on your system path $ java -version java version \"1.8.0_192-ea\" Java ( TM ) SE Runtime Environment ( build 1 .8.0_192-ea-b04 ) Java HotSpot ( TM ) 64 -Bit Server VM ( build 25 .192-b04, mixed mode ) $ javac -version javac 1 .8.0_192-ea Assert that JRuby 9.2 or higher is on your system path $ jruby -v jruby 9 .2.6.0 ( 2 .5.3 ) 2019 -02-11 15ba00b Java HotSpot ( TM ) 64 -Bit Server VM 25 .192-b04 on 1 .8.0_192-ea-b04 +jit [ darwin-x86_64 ] Assert that the rake and bundler ruby tools are installed $ gem install rake bundler Fetching: rake-12.3.2.gem ( 100 % ) Successfully installed rake-12.3.2 Fetching: bundler-2.0.2.gem ( 100 % ) Successfully installed bundler-2.0.2 2 gems installed Clone and build logstash and its benchmark tool git clone https://github.com/elastic/logstash cd logstash gradle clean assemble cd tools/benchmark-cli gradle clean assemble","title":"Logstash"},{"location":"development/benchmarks/LogstashBenchmark/#benchmark-methodology","text":"In the first ( this ) version, this was a manual process","title":"Benchmark methodology"},{"location":"development/benchmarks/LogstashBenchmark/#identify-performance-related-tunables-for-logstash","text":"Number of logstash workers ( default 2 ) Batch size for micro-batching queues in logstash workers ( default 128 )","title":"Identify performance related tunables for logstash"},{"location":"development/benchmarks/LogstashBenchmark/#identity-baseline-benchmark","text":"Literally named baseline and is an equivalent in functionality to tremor's empty-passthrough-json benchmark","title":"Identity baseline benchmark"},{"location":"development/benchmarks/LogstashBenchmark/#for-each-logstash-tunable-run-a-benchmark","text":"Given the baseline logstash benchmark and record results and initial analysis Document command line arguments required to evaluated each run/iteration $ java -cp build/libs/benchmark-cli.jar org.logstash.benchmark.cli.Main \\ --testcase = baseline --distribution-version = 5 .5.0 \\ --ls-workers <num-workers> \\ --ls-batch-size <batch-size>","title":"For each logstash tunable, run a benchmark"},{"location":"development/benchmarks/LogstashBenchmark/#for-each-tremor-tunable-run-a-benchmark","text":"Given the empty-passthrough-json equivalent benchmark. Tremor has no performance-related tunables. Tremor's benchmark framework has no performance-related tunables for the empty-passthrough-json equivalent benchmark Record results","title":"For each tremor tunable, run a benchmark"},{"location":"development/benchmarks/LogstashBenchmark/#benchmark-conditioning-and-environment","text":"The benchmark-cli tool that ships with logstash suffers from a number of issues. The number of events in each run is fixed and limited to 1 million events per run. No accommodation is made for warmup to ensure that the JVM has reached a stable state before results recording begins. The framework also incorrectly terminates after each run once 1 million results have been submitted. It should not complete until all workers have drained their respective queues and the benchmark reaches a quiescent state. Quiescence is not asserted. This means that the number of recorded processed events can be less than the configured target by a significant margin. As these are micro-benchmarks and we are concerned with Order of Magnitude differences in performance we have not expended effort resolving these issues. A further issue with the logstash benchmark-cli tool is that the results suffer from the coordinated omission problem. The coordinated omission problem, is a term first-coined by Gil Tene based on the observations in benchmarking the Azul Vega hardware and Zing JVM with C4 garbage collector and related ZTS subsystems). In a nutshell, coordinated-omission is where a benchmarking tool ( usually unintentionally ) incorrectly records events under measurement time spans by failing to record the intended verses actual time to record. Specifically, it is insufficient to record the start time and end time of a particular event of interest. Capturing the start and end time allows the delta or servicing time to be computed. It does not capture any synchronization overhead, waiting time, delays or other system-induced hiccups introduced between up to the point the event should have started. We fail to capture unintentional drift introduced artificially by the benchmark framework when CO is in force. Well-designed benchmarks should be CO-free. Tremor's benchmarking facility allows events to be injected at a fixed frequency. This tactic ( which more specifically, pins the intended commencement time for an event to begin processing to a starting epoch ) is sufficient to practically account for any hiccups or drift in expected verses actual inter-arrival based on designed constraints in a benchmark framework. By selecting a fixed static frequency any measured hiccups should be outside of the control of the benchmark framework - they are either artifacts of the scenario under test, or the system upon which it is being tested. These conditions are optimal in all benchmark testing, but an absolute necessity for any latency-sensitive testing, especially where fine-grained statistic quartiles are being computed if they are to impart results that are fit for low-level analysis and interpretation. Coordinated-omission-free benchmarks are important for micro-benchmarking and latency-driven benchmarking. However, as we are interested in Order of Magnitude ( finger in the air ) characteristics rather than isolating long-tail latencies or understanding fine-grained latency characteristics on a per-event basis ( say, at the long-tail of performance beyond the 99.99th percentile ) In short, tremor makes some effort to account for coordinated-omission where relevant, but logstash's benchmark framework does not. However, as we are taking a 50,000-foot view of characteristic throughput and are not focusing on specific per-event latency characteristics the identified issues are negligible for our analysis. It would be incorrect however, to focus on per-event performance characteristics or focus in on specific latency quartiles or throughput quartiles to derive any significance. Such an analysis would require more effort and would not necessarily deliver any greater value. We have not used lab-quality environments to run any of the benchmarks. All benchmarks were run on the same development grade laptop ( not ideal ) with the same background processes active on an intel / Mac OS X x86_64 environment. As such we consider the results indicative of characteristics and good enough for high-level analysis. It should, however, be a small task to follow this report to replicate the characteristic results detailed in this report and accompanying evidence and to replicate the same on similar resources.","title":"Benchmark conditioning and environment"},{"location":"development/benchmarks/LogstashBenchmark/#baseline-analysis","text":"Logstash's benchmark-cli was put through 40 variations of its two tunable parameters. We tested with 1, 2, 4 and 8 logstash workers. We tested each worker configuration with queue batch size bounds of 1, 2, 4, 8, 16, 32, 64, 128, 256 and 512. The best of 3 runs was recorded. As tremor has no tunables we recorded the best of 3 runs. Logstash is configured out of the box for 2 workers and a batch size of 128. There are marginally better configurations possible with 4 or 8 workers showing marginal throughput benefits given the use-case selected. Configuring a single worker has a significant negative impact on performance. As such logstash is well-configured out of the box, at least for typical development or non-production activities. The optimal configuration on the test machine was configured with 8 workers ( default 2 ) and a batch size of 256 ( default 128 ). This is less than 1% of a difference. Generally speaking, batch sizes of less than 32 tend to significantly reduce throughput for any number of configured workers. Also, generally speaking, the improvement from 2 worker threads to more does not demonstrate any improvement in scaling. As such multi-core scaling with logstash does not seem to be of much benefit beyond 2 workers ( threads ). It should be noted that the benchmark creates artificial conditions and that the baseline working-set is atypical of production working sets. On the other hand, this conditioning is the same with respect to tremor whose baseline benchmark is equivalent. In both cases we ingest, forward and publish an event or pass it through the system under test. As logstash is configured for a fixed ceiling of events ( 1 million ) for a benchmark run, and tremor is configured for a specific test duration ( as many events as possible in 40 seconds ) we need to baseline the results. We bias in favor of tremor as normative and compute the effective throughput at 40 seconds for each logstash run. So a 63 second run with 8 workes and a batch size of 1 38 seconds run at batch size 256 respectively for logstash is counted as the equivalent 40 seconds run as follows: Logstash Default (W2 W128) Logstash Worst (W1 B1) Logstash Best (W8 B256) Tremor (baseline) 1033811 466708 1041943 21402853 Note that we truncate / floor round the equivalent logstash 40-second results for each selected configuration. Relative to the worst case logstash benchmark run performance: Logstash W2 W128 ( default ) Logstash W8 B1 Logstash W8 B256 Tremor ( baseline ) 2.22 1 2.23 45.86 Logstash can itself benefit from at least a 2x improvement, and this is consistent with the default out of the box configuration. Tremor, however, is a factor of 45 better than the logstash worst case. Tremor, compared to the logstash best case, is still a factor of 20.54 higher throughput. So the total effective range of improvement for the given benchmark ( all other things considered equal ) is somewhere between a 20x to 45x increase in throughput favoring tremor over logstash for the baseline use case based on experimental conditions detailed in this report.","title":"Baseline Analysis"},{"location":"development/benchmarks/LogstashBenchmark/#production-analysis","text":"We don't typically deploy logstash or tremor into production as a distribution proxy or interconnect that passes through events. Logstash would be a pretty bad choice compared to tremor. But tremor, although it has excellent conditioning and is designed for elegantly handling back-pressure and saturation conditions for log shipping and distribution - it does not provide the delivery semantics, retention and feature-set of technologies such as Kafka. For the level 1 traffic limiting, traffic shaping and rate-limiting use cases in Level 3 Logging at Wayfair for which tremor was originally designed we have seen a 7x-8x improvement in density compared to logstash for v0.4 of tremor. v0.5 should increase this to the 10x ballpark as we benefit from SIMD vectorization of JSON deserialization. However, there is further room for improvement as the tremor-script language has evolved to handle level 2 logging to replace ruby and logstash with far richer configuration than level 1. In v0.5 performance is a non-goal; as such there are many optimisations to the new tremor-script domain-specific-langauge that we have yet to undertake - so in practice for level 2 logging we won't see the full benefit of SIMD vectorization as some of those gains are ammortised by additional essential complexity of providing a richer DSL to support replacing logstash in level 2. Indicatively, we stand to see a range of 20x-40x improvement. In production we have observed closer to a 7x-8x improvement in density with the L3 replacement, and with the v0.4 upgrade to L1 in GCP pre-live. We expect a further modest incremental improvement in L1 with v0.5, and a good ~10x over logstash for L2 this ( v0.5 ) release.","title":"Production Analysis"},{"location":"development/benchmarks/LogstashBenchmark/#apache-logs-scenario","text":"Logstash ships with a benchmark where logstash is configured for Apache log processing and elementization. We have ported this benchmark to tremor for coparative purposes. Tremor processes 44109658 messages in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 751 seconds to process the same 6900000 log records. This is 48x speedup in favour of tremor.","title":"Apache Logs Scenario"},{"location":"development/benchmarks/LogstashBenchmark/#logging-level-2-replacement-scenario","text":"Tremor ships with a benchmark where the enrichment component of logstash replacement running in production has been used to drive the implementation of tremor-script features in the v0.5 release. We have minimally modified the logstash benchmark client to support running logstash equivalent configuration of tremor ( or vice-versa ) to get as close to possible to a fair apples-to-apples comparison. We have stopped short of fixing fundemental issues with the logstash benchmark framework itself. Tremor process 2262222 records in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 66 seconds to process 200000 records. This equates to a ballpark 7.5x speedup in favour of tremor.","title":"Logging Level 2 Replacement Scenario"},{"location":"development/benchmarks/LogstashBenchmark/#logging-level-2-replacement-bad-case","text":"Benchmarking the validation/transformation scenario involves a more complex script with loops and deeper nested expressions. Here some of the shortcuts we took to implement the new tremor script in a reasonable timeframe surface as performance bottlenecks. Again ran the transformation script with both Logstash as well tremor with configurations as close to equivalent as possible. Tremor processes 73132 records in 100 seconds fixed interval ( continuously replaying ), whereas logstash takes 87 seconds to process 150000 records. In this case tremor is about 2.3x slower then a comparable logstasv configuration.","title":"Logging Level 2 Replacement bad case"},{"location":"development/process/contributions/","text":"","title":"Contributions"},{"location":"development/process/patches/","text":"","title":"Patches"},{"location":"development/process/release/","text":"","title":"Release"},{"location":"development/process/reporting-bugs/","text":"","title":"Reporting bugs"},{"location":"development/process/rfc/","text":"Requests for Proposals \u00b6 Tremor request for proposals TBD TODO","title":"Requests for Proposals"},{"location":"development/process/rfc/#requests-for-proposals","text":"Tremor request for proposals TBD TODO","title":"Requests for Proposals"},{"location":"development/process/running-tests/","text":"Running Tests \u00b6 Tremor includes a number of test suites that may be used to TBD TODO","title":"Running Tests"},{"location":"development/process/running-tests/#running-tests","text":"Tremor includes a number of test suites that may be used to TBD TODO","title":"Running Tests"},{"location":"development/process/security-bugs/","text":"","title":"Security bugs"},{"location":"internal/LocalMkdocsSetup/","text":"Local MkDocs Testing \u00b6 This is a short canned synopsis for running MkDdocs locally while editing the docs. Install mkdocs \u00b6 # mkdocs is python-based pip3 install mkdocs # or on mac systems brew install mkdocs Install extensions \u00b6 pip3 install -r requirements.txt pip3 install -r python_scripts/requirements.txt This installs the mkdocs-material theme which we use, along with other dependencies. To enable syntax-highlighting for tremor-script/trickle code snippets, install our mkdocs-specifc tremor lexer as well: .github/scripts/install-lexer.sh Generate dynamic documentation and configuration \u00b6 make This auto-generates the doc files for tremor stdlib and cli, and also produces the default config file for mkdocs ( mkdocs.yml ) file at the end (with the right navigation references to the generated stdlib docs). Build documentation \u00b6 mkdocs build Local doc service \u00b6 mkdocs serve The doc site will be available at http://127.0.0.1:8000/ for reviewing (supports live-reload, as you edit).","title":"Local MkDocs Testing"},{"location":"internal/LocalMkdocsSetup/#local-mkdocs-testing","text":"This is a short canned synopsis for running MkDdocs locally while editing the docs.","title":"Local MkDocs Testing"},{"location":"internal/LocalMkdocsSetup/#install-mkdocs","text":"# mkdocs is python-based pip3 install mkdocs # or on mac systems brew install mkdocs","title":"Install mkdocs"},{"location":"internal/LocalMkdocsSetup/#install-extensions","text":"pip3 install -r requirements.txt pip3 install -r python_scripts/requirements.txt This installs the mkdocs-material theme which we use, along with other dependencies. To enable syntax-highlighting for tremor-script/trickle code snippets, install our mkdocs-specifc tremor lexer as well: .github/scripts/install-lexer.sh","title":"Install extensions"},{"location":"internal/LocalMkdocsSetup/#generate-dynamic-documentation-and-configuration","text":"make This auto-generates the doc files for tremor stdlib and cli, and also produces the default config file for mkdocs ( mkdocs.yml ) file at the end (with the right navigation references to the generated stdlib docs).","title":"Generate dynamic documentation and configuration"},{"location":"internal/LocalMkdocsSetup/#build-documentation","text":"mkdocs build","title":"Build documentation"},{"location":"internal/LocalMkdocsSetup/#local-doc-service","text":"mkdocs serve The doc site will be available at http://127.0.0.1:8000/ for reviewing (supports live-reload, as you edit).","title":"Local doc service"},{"location":"internal/transform/","text":"match to functional \u00b6 A useful step by step followup on transforming match style to functional style. pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ); match res0 { Ok ( res ) => match res { Some ( res ) => reply ( req , data , Ok ( res . binding ), false , 200 ), None => Err ( error :: ErrorNotFound ( \"Binding not found\" )), }, Err ( e ) => Err ( error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))), } } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) ? . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" )) ? ; reply ( req , data , Ok ( res0 . binding ), false , 200 ) } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ). map_err ( | e | { error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e )) }) ? ; match res { Some ( res ) => reply ( req , data , Ok ( res . binding ), false , 200 ), None => Err ( error :: ErrorNotFound ( \"Binding not found\" )), } } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) ? . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" )) . and_then ( | result | reply ( req , data , Ok ( result . binding ), false , 200 )) } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) . and_then ( | result | result . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" ))) . and_then ( | result | reply ( req , data , Ok ( result . binding ), false , 200 )) }","title":"match to functional"},{"location":"internal/transform/#match-to-functional","text":"A useful step by step followup on transforming match style to functional style. pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ); match res0 { Ok ( res ) => match res { Some ( res ) => reply ( req , data , Ok ( res . binding ), false , 200 ), None => Err ( error :: ErrorNotFound ( \"Binding not found\" )), }, Err ( e ) => Err ( error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))), } } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) ? . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" )) ? ; reply ( req , data , Ok ( res0 . binding ), false , 200 ) } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; let res0 = data . world . reg . find_binding ( & url ). map_err ( | e | { error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e )) }) ? ; match res { Some ( res ) => reply ( req , data , Ok ( res . binding ), false , 200 ), None => Err ( error :: ErrorNotFound ( \"Binding not found\" )), } } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) ? . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" )) . and_then ( | result | reply ( req , data , Ok ( result . binding ), false , 200 )) } pub fn get_servant ( ( req , data , path ) : ( HttpRequest , Data < State > , Path < ( String , String ) > ), ) -> HTTPResult { let url = build_url ( & [ \"binding\" , & path . 0 , & path . 1 ]) ? ; data . world . reg . find_binding ( & url ) . map_err ( | e | error :: ErrorInternalServerError ( format! ( \"Internal server error: {}\" , e ))) . and_then ( | result | result . ok_or_else ( || error :: ErrorNotFound ( \"Binding not found\" ))) . and_then ( | result | reply ( req , data , Ok ( result . binding ), false , 200 )) }","title":"match to functional"},{"location":"operations/cli/","text":"Tremor cli v0.11 \u00b6 Tremor cli - Command Line Interface Scope \u00b6 This document summarises the tremor cli commands. Audience \u00b6 Tremor operators and developers General flags and switches \u00b6 Arguments Name Switch Kind Multiple Description verbose None switch/flag yes Sets the level of verbosity instance None switch/flag no Instance identifier Commands \u00b6 Subcommands Command Description completions Generate shell completions to stdout. Tries to guess the shell if no subcommand is given. server Tremor server test Testing facilities dbg Advanced debugging commands run Run tremor script or query files against stdin or a json data archive, the data will be read from STDIN or an archive and written to STDOUT. doc Generates documention from tremor script files api Tremor API client completions \u00b6 Generate shell completions to stdout. Tries to guess the shell if no subcommand is given. Subcommands Command Description guess Generate completion based on active shell bash Generate bash shell completions zsh Generate zsh shell completions elvish Generate elvish shell completions fish Generate fish shell completions powershell Generate powershell shell completions completions guess \u00b6 Generate completion based on active shell Usage tremor completions guess completions bash \u00b6 Generate bash shell completions Usage tremor completions bash completions zsh \u00b6 Generate zsh shell completions Usage tremor completions zsh completions elvish \u00b6 Generate elvish shell completions Usage tremor completions elvish completions fish \u00b6 Generate fish shell completions Usage tremor completions fish completions powershell \u00b6 Generate powershell shell completions Usage tremor completions powershell server \u00b6 Tremor server Subcommands Command Description run Runs the tremor server process server run \u00b6 Runs the tremor server process Usage tremor server run Arguments Name Switch Kind Multiple Description artefacts None switch/flag yes Paths to files containing pipelines, onramps, offramps to provision storage-directory None switch/flag no Directory to cache/store runtime type information pid None switch/flag no Captures process id if set and stores in a file no-api None switch/flag no Disable the API api-host None switch/flag no The host:port to listen for the API logger-config None switch/flag no log4rs config recursion-limit None switch/flag no function tail-recursion stack depth limit test \u00b6 Testing facilities Usage tremor test [<MODE>] [<PATH>] Arguments Name Switch Kind Multiple Description MODE None switch/flag no One of all , api , bench , command , integration , rest , or unit PATH None switch/flag no The root test path REPORT None switch/flag no Should generate a test report to specified path INCLUDES None switch/flag yes Optional tags to filter test executions by EXCLUDES None switch/flag yes Optional tags to filter test executions by QUIET None switch/flag no do not print skipped tests dbg \u00b6 Advanced debugging commands Arguments Name Switch Kind Multiple Description no-banner None switch/flag no do not print the banner no-highlight None switch/flag no do not highlight output Subcommands Command Description dot prints the .dot representation for a trickle file (you can use | dot -Tpng -oout.png to generate a picture) ast prints the AST of the source preprocess prints the preprocessed source lex prints lexemes src prints source dbg dot \u00b6 prints the .dot representation for a trickle file (you can use | dot -Tpng -oout.png to generate a picture) Usage tremor dbg dot [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no trickle script filename dbg ast \u00b6 prints the AST of the source Usage tremor dbg ast [<SCRIPT>] Arguments Name Switch Kind Multiple Description exprs-only None switch/flag no only prints the expressions SCRIPT None switch/flag no tremor/json/trickle script filename dbg preprocess \u00b6 prints the preprocessed source Usage tremor dbg preprocess [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename dbg lex \u00b6 prints lexemes Usage tremor dbg lex [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename dbg src \u00b6 prints source Usage tremor dbg src [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename run \u00b6 Run tremor script or query files against stdin or a json data archive, the data will be read from STDIN or an archive and written to STDOUT. Usage tremor run [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no filename to run the data through interactive None switch/flag no Should not output to consumed source / produced synthetic data or errors pretty None switch/flag no Should not pretty print data [ when in interactive mode ] ENCODER None switch/flag no The codec to use for encoding the data DECODER None switch/flag no The codec to use for decoding the data INFILE None switch/flag no input file OUTFILE None switch/flag no output file PREPROCESSOR None switch/flag no preprocessor to pass data through before decoding POSTPROCESSOR None switch/flag no postprocessor to pass data through after encoding output-port None switch/flag no selects the port to pull output doc \u00b6 Generates documention from tremor script files Usage tremor doc [<DIR>] [<OUTDIR>] Arguments Name Switch Kind Multiple Description interactive None switch/flag no generates output to standard output DIR None switch/flag no directory or source to generate documents for OUTDIR None switch/flag no directory to generate documents into api \u00b6 Tremor API client Arguments Name Switch Kind Multiple Description FORMAT None switch/flag no Sets the output format CONFIG None switch/flag no Sets a custom config file Subcommands Command Description version Get tremor version target Target one or many tremor server instances binding Query/update binding specification repository pipeline Query/update pipeline specification repository onramp Query/update onramp specification repository offramp Query/update offramp specification repository api version \u00b6 Get tremor version Usage tremor api version api target \u00b6 Target one or many tremor server instances Subcommands Command Description list List registered targets create Create a new API target delete Delete an existing API target api target list \u00b6 List registered targets Usage tremor api target list api target create \u00b6 Create a new API target Usage tremor api target create [<TARGET_ID>] [<SOURCE>] Arguments Name Switch Kind Multiple Description TARGET_ID None switch/flag no The unique target id for the targetted tremor servers SOURCE None switch/flag no JSON or YAML file request body api target delete \u00b6 Delete an existing API target Usage tremor api target delete [<TARGET_ID>] Arguments Name Switch Kind Multiple Description TARGET_ID None switch/flag no The unique target id for the targetted tremor servers api binding \u00b6 Query/update binding specification repository Subcommands Command Description list List registered binding specifications fetch Fetch a binding by artefact id delete Delete a binding by artefact id create Create and register a binding specification instance Fetch an binding instance by artefact id and instance id activate Activate a binding by artefact id and servant instance id deactivate Activate a binding by artefact id and servant instance id api binding list \u00b6 List registered binding specifications Usage tremor api binding list api binding fetch \u00b6 Fetch a binding by artefact id Usage tremor api binding fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification api binding delete \u00b6 Delete a binding by artefact id Usage tremor api binding delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification api binding create \u00b6 Create and register a binding specification Usage tremor api binding create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body api binding instance \u00b6 Fetch an binding instance by artefact id and instance id Usage tremor api binding instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification api binding activate \u00b6 Activate a binding by artefact id and servant instance id Usage tremor api binding activate [<ARTEFACT_ID>] [<INSTANCE_ID>] [<SOURCE>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification SOURCE None switch/flag no JSON -r YAML file request body api binding deactivate \u00b6 Activate a binding by artefact id and servant instance id Usage tremor api binding deactivate [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification api pipeline \u00b6 Query/update pipeline specification repository Subcommands Command Description list List registered pipeline specifications fetch Fetch a pipeline by artefact id delete Delete a pipeline by artefact id create Create and register a pipeline specification instance Fetch an pipeline instance by artefact id and instance id api pipeline list \u00b6 List registered pipeline specifications Usage tremor api pipeline list api pipeline fetch \u00b6 Fetch a pipeline by artefact id Usage tremor api pipeline fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification api pipeline delete \u00b6 Delete a pipeline by artefact id Usage tremor api pipeline delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification api pipeline create \u00b6 Create and register a pipeline specification Usage tremor api pipeline create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body api pipeline instance \u00b6 Fetch an pipeline instance by artefact id and instance id Usage tremor api pipeline instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification INSTANCE_ID None switch/flag no The unique instance id for the pipeline specification api onramp \u00b6 Query/update onramp specification repository Subcommands Command Description list List registered onramp specifications fetch Fetch an onramp by artefact id delete Delete an onramp by artefact id create Create and register an onramp specification instance Fetch an onramp instance by artefact id and instance id api onramp list \u00b6 List registered onramp specifications Usage tremor api onramp list api onramp fetch \u00b6 Fetch an onramp by artefact id Usage tremor api onramp fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification api onramp delete \u00b6 Delete an onramp by artefact id Usage tremor api onramp delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification api onramp create \u00b6 Create and register an onramp specification Usage tremor api onramp create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body api onramp instance \u00b6 Fetch an onramp instance by artefact id and instance id Usage tremor api onramp instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification INSTANCE_ID None switch/flag no The unique instance id for the onramp specification api offramp \u00b6 Query/update offramp specification repository Subcommands Command Description list List registered offramp specifications fetch Fetch an offramp by artefact id delete Delete an offramp by artefact id create Create and register an offramp specification instance Fetch an offramp instance by artefact id and instance id api offramp list \u00b6 List registered offramp specifications Usage tremor api offramp list api offramp fetch \u00b6 Fetch an offramp by artefact id Usage tremor api offramp fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification api offramp delete \u00b6 Delete an offramp by artefact id Usage tremor api offramp delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification api offramp create \u00b6 Create and register an offramp specification Usage tremor api offramp create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body api offramp instance \u00b6 Fetch an offramp instance by artefact id and instance id Usage tremor api offramp instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification INSTANCE_ID None switch/flag no The unique instance id for the offramp specification","title":"Tremor Tool"},{"location":"operations/cli/#tremor-cli-v011","text":"Tremor cli - Command Line Interface","title":"Tremor cli v0.11"},{"location":"operations/cli/#scope","text":"This document summarises the tremor cli commands.","title":"Scope"},{"location":"operations/cli/#audience","text":"Tremor operators and developers","title":"Audience"},{"location":"operations/cli/#general-flags-and-switches","text":"Arguments Name Switch Kind Multiple Description verbose None switch/flag yes Sets the level of verbosity instance None switch/flag no Instance identifier","title":"General flags and switches"},{"location":"operations/cli/#commands","text":"Subcommands Command Description completions Generate shell completions to stdout. Tries to guess the shell if no subcommand is given. server Tremor server test Testing facilities dbg Advanced debugging commands run Run tremor script or query files against stdin or a json data archive, the data will be read from STDIN or an archive and written to STDOUT. doc Generates documention from tremor script files api Tremor API client","title":"Commands"},{"location":"operations/cli/#completions","text":"Generate shell completions to stdout. Tries to guess the shell if no subcommand is given. Subcommands Command Description guess Generate completion based on active shell bash Generate bash shell completions zsh Generate zsh shell completions elvish Generate elvish shell completions fish Generate fish shell completions powershell Generate powershell shell completions","title":"completions"},{"location":"operations/cli/#completions-guess","text":"Generate completion based on active shell Usage tremor completions guess","title":"completions guess"},{"location":"operations/cli/#completions-bash","text":"Generate bash shell completions Usage tremor completions bash","title":"completions bash"},{"location":"operations/cli/#completions-zsh","text":"Generate zsh shell completions Usage tremor completions zsh","title":"completions zsh"},{"location":"operations/cli/#completions-elvish","text":"Generate elvish shell completions Usage tremor completions elvish","title":"completions elvish"},{"location":"operations/cli/#completions-fish","text":"Generate fish shell completions Usage tremor completions fish","title":"completions fish"},{"location":"operations/cli/#completions-powershell","text":"Generate powershell shell completions Usage tremor completions powershell","title":"completions powershell"},{"location":"operations/cli/#server","text":"Tremor server Subcommands Command Description run Runs the tremor server process","title":"server"},{"location":"operations/cli/#server-run","text":"Runs the tremor server process Usage tremor server run Arguments Name Switch Kind Multiple Description artefacts None switch/flag yes Paths to files containing pipelines, onramps, offramps to provision storage-directory None switch/flag no Directory to cache/store runtime type information pid None switch/flag no Captures process id if set and stores in a file no-api None switch/flag no Disable the API api-host None switch/flag no The host:port to listen for the API logger-config None switch/flag no log4rs config recursion-limit None switch/flag no function tail-recursion stack depth limit","title":"server run"},{"location":"operations/cli/#test","text":"Testing facilities Usage tremor test [<MODE>] [<PATH>] Arguments Name Switch Kind Multiple Description MODE None switch/flag no One of all , api , bench , command , integration , rest , or unit PATH None switch/flag no The root test path REPORT None switch/flag no Should generate a test report to specified path INCLUDES None switch/flag yes Optional tags to filter test executions by EXCLUDES None switch/flag yes Optional tags to filter test executions by QUIET None switch/flag no do not print skipped tests","title":"test"},{"location":"operations/cli/#dbg","text":"Advanced debugging commands Arguments Name Switch Kind Multiple Description no-banner None switch/flag no do not print the banner no-highlight None switch/flag no do not highlight output Subcommands Command Description dot prints the .dot representation for a trickle file (you can use | dot -Tpng -oout.png to generate a picture) ast prints the AST of the source preprocess prints the preprocessed source lex prints lexemes src prints source","title":"dbg"},{"location":"operations/cli/#dbg-dot","text":"prints the .dot representation for a trickle file (you can use | dot -Tpng -oout.png to generate a picture) Usage tremor dbg dot [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no trickle script filename","title":"dbg dot"},{"location":"operations/cli/#dbg-ast","text":"prints the AST of the source Usage tremor dbg ast [<SCRIPT>] Arguments Name Switch Kind Multiple Description exprs-only None switch/flag no only prints the expressions SCRIPT None switch/flag no tremor/json/trickle script filename","title":"dbg ast"},{"location":"operations/cli/#dbg-preprocess","text":"prints the preprocessed source Usage tremor dbg preprocess [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename","title":"dbg preprocess"},{"location":"operations/cli/#dbg-lex","text":"prints lexemes Usage tremor dbg lex [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename","title":"dbg lex"},{"location":"operations/cli/#dbg-src","text":"prints source Usage tremor dbg src [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no tremor/json/trickle script filename","title":"dbg src"},{"location":"operations/cli/#run","text":"Run tremor script or query files against stdin or a json data archive, the data will be read from STDIN or an archive and written to STDOUT. Usage tremor run [<SCRIPT>] Arguments Name Switch Kind Multiple Description SCRIPT None switch/flag no filename to run the data through interactive None switch/flag no Should not output to consumed source / produced synthetic data or errors pretty None switch/flag no Should not pretty print data [ when in interactive mode ] ENCODER None switch/flag no The codec to use for encoding the data DECODER None switch/flag no The codec to use for decoding the data INFILE None switch/flag no input file OUTFILE None switch/flag no output file PREPROCESSOR None switch/flag no preprocessor to pass data through before decoding POSTPROCESSOR None switch/flag no postprocessor to pass data through after encoding output-port None switch/flag no selects the port to pull output","title":"run"},{"location":"operations/cli/#doc","text":"Generates documention from tremor script files Usage tremor doc [<DIR>] [<OUTDIR>] Arguments Name Switch Kind Multiple Description interactive None switch/flag no generates output to standard output DIR None switch/flag no directory or source to generate documents for OUTDIR None switch/flag no directory to generate documents into","title":"doc"},{"location":"operations/cli/#api","text":"Tremor API client Arguments Name Switch Kind Multiple Description FORMAT None switch/flag no Sets the output format CONFIG None switch/flag no Sets a custom config file Subcommands Command Description version Get tremor version target Target one or many tremor server instances binding Query/update binding specification repository pipeline Query/update pipeline specification repository onramp Query/update onramp specification repository offramp Query/update offramp specification repository","title":"api"},{"location":"operations/cli/#api-version","text":"Get tremor version Usage tremor api version","title":"api version"},{"location":"operations/cli/#api-target","text":"Target one or many tremor server instances Subcommands Command Description list List registered targets create Create a new API target delete Delete an existing API target","title":"api target"},{"location":"operations/cli/#api-target-list","text":"List registered targets Usage tremor api target list","title":"api target list"},{"location":"operations/cli/#api-target-create","text":"Create a new API target Usage tremor api target create [<TARGET_ID>] [<SOURCE>] Arguments Name Switch Kind Multiple Description TARGET_ID None switch/flag no The unique target id for the targetted tremor servers SOURCE None switch/flag no JSON or YAML file request body","title":"api target create"},{"location":"operations/cli/#api-target-delete","text":"Delete an existing API target Usage tremor api target delete [<TARGET_ID>] Arguments Name Switch Kind Multiple Description TARGET_ID None switch/flag no The unique target id for the targetted tremor servers","title":"api target delete"},{"location":"operations/cli/#api-binding","text":"Query/update binding specification repository Subcommands Command Description list List registered binding specifications fetch Fetch a binding by artefact id delete Delete a binding by artefact id create Create and register a binding specification instance Fetch an binding instance by artefact id and instance id activate Activate a binding by artefact id and servant instance id deactivate Activate a binding by artefact id and servant instance id","title":"api binding"},{"location":"operations/cli/#api-binding-list","text":"List registered binding specifications Usage tremor api binding list","title":"api binding list"},{"location":"operations/cli/#api-binding-fetch","text":"Fetch a binding by artefact id Usage tremor api binding fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification","title":"api binding fetch"},{"location":"operations/cli/#api-binding-delete","text":"Delete a binding by artefact id Usage tremor api binding delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification","title":"api binding delete"},{"location":"operations/cli/#api-binding-create","text":"Create and register a binding specification Usage tremor api binding create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body","title":"api binding create"},{"location":"operations/cli/#api-binding-instance","text":"Fetch an binding instance by artefact id and instance id Usage tremor api binding instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification","title":"api binding instance"},{"location":"operations/cli/#api-binding-activate","text":"Activate a binding by artefact id and servant instance id Usage tremor api binding activate [<ARTEFACT_ID>] [<INSTANCE_ID>] [<SOURCE>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification SOURCE None switch/flag no JSON -r YAML file request body","title":"api binding activate"},{"location":"operations/cli/#api-binding-deactivate","text":"Activate a binding by artefact id and servant instance id Usage tremor api binding deactivate [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the binding specification INSTANCE_ID None switch/flag no The unique instance id for the binding specification","title":"api binding deactivate"},{"location":"operations/cli/#api-pipeline","text":"Query/update pipeline specification repository Subcommands Command Description list List registered pipeline specifications fetch Fetch a pipeline by artefact id delete Delete a pipeline by artefact id create Create and register a pipeline specification instance Fetch an pipeline instance by artefact id and instance id","title":"api pipeline"},{"location":"operations/cli/#api-pipeline-list","text":"List registered pipeline specifications Usage tremor api pipeline list","title":"api pipeline list"},{"location":"operations/cli/#api-pipeline-fetch","text":"Fetch a pipeline by artefact id Usage tremor api pipeline fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification","title":"api pipeline fetch"},{"location":"operations/cli/#api-pipeline-delete","text":"Delete a pipeline by artefact id Usage tremor api pipeline delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification","title":"api pipeline delete"},{"location":"operations/cli/#api-pipeline-create","text":"Create and register a pipeline specification Usage tremor api pipeline create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body","title":"api pipeline create"},{"location":"operations/cli/#api-pipeline-instance","text":"Fetch an pipeline instance by artefact id and instance id Usage tremor api pipeline instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the pipeline specification INSTANCE_ID None switch/flag no The unique instance id for the pipeline specification","title":"api pipeline instance"},{"location":"operations/cli/#api-onramp","text":"Query/update onramp specification repository Subcommands Command Description list List registered onramp specifications fetch Fetch an onramp by artefact id delete Delete an onramp by artefact id create Create and register an onramp specification instance Fetch an onramp instance by artefact id and instance id","title":"api onramp"},{"location":"operations/cli/#api-onramp-list","text":"List registered onramp specifications Usage tremor api onramp list","title":"api onramp list"},{"location":"operations/cli/#api-onramp-fetch","text":"Fetch an onramp by artefact id Usage tremor api onramp fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification","title":"api onramp fetch"},{"location":"operations/cli/#api-onramp-delete","text":"Delete an onramp by artefact id Usage tremor api onramp delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification","title":"api onramp delete"},{"location":"operations/cli/#api-onramp-create","text":"Create and register an onramp specification Usage tremor api onramp create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body","title":"api onramp create"},{"location":"operations/cli/#api-onramp-instance","text":"Fetch an onramp instance by artefact id and instance id Usage tremor api onramp instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the onramp specification INSTANCE_ID None switch/flag no The unique instance id for the onramp specification","title":"api onramp instance"},{"location":"operations/cli/#api-offramp","text":"Query/update offramp specification repository Subcommands Command Description list List registered offramp specifications fetch Fetch an offramp by artefact id delete Delete an offramp by artefact id create Create and register an offramp specification instance Fetch an offramp instance by artefact id and instance id","title":"api offramp"},{"location":"operations/cli/#api-offramp-list","text":"List registered offramp specifications Usage tremor api offramp list","title":"api offramp list"},{"location":"operations/cli/#api-offramp-fetch","text":"Fetch an offramp by artefact id Usage tremor api offramp fetch [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification","title":"api offramp fetch"},{"location":"operations/cli/#api-offramp-delete","text":"Delete an offramp by artefact id Usage tremor api offramp delete [<ARTEFACT_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification","title":"api offramp delete"},{"location":"operations/cli/#api-offramp-create","text":"Create and register an offramp specification Usage tremor api offramp create [<SOURCE>] Arguments Name Switch Kind Multiple Description SOURCE None switch/flag no JSON or YAML file request body","title":"api offramp create"},{"location":"operations/cli/#api-offramp-instance","text":"Fetch an offramp instance by artefact id and instance id Usage tremor api offramp instance [<ARTEFACT_ID>] [<INSTANCE_ID>] Arguments Name Switch Kind Multiple Description ARTEFACT_ID None switch/flag no The unique artefact id for the offramp specification INSTANCE_ID None switch/flag no The unique instance id for the offramp specification","title":"api offramp instance"},{"location":"operations/configuration-walkthrough/","text":"Configuration Walkthrough \u00b6 A short canned synopsis of configuration tremor. This guide walks through configuring tremor via its API directly and via its command line tool 'tremor'. For the API, we use 'curl' on the command line. Introduction \u00b6 In this walkthrough we will deploy a tremor pipeline that generates a periodic sequence of messages or heartbeats every second. The solution is composed of the following tremor artefacts: A metronome onramp - our periodic message generator A stdout offramp - an offramp that serializes to standard output useful for debugging A pipeline - we pass the input ( metronome events ) to our output In this walkthrough we configure a single onramp, offramp and pipeline but many other configurations are possible. Prerequisites \u00b6 Write an onramp specification \u00b6 This creates a config specification for an onramp that can be referenced by the unique id metronome . It does not create an instance of it. # File: metronome-onramp.yaml id : metronome type : metronome config : interval : 1000 Write an offramp specification \u00b6 This creates a config specification for an offramp that can be referenced by the unique id stdout . It does not create an instance of it. Think of this as a blueprint. # File: metronome-offramp.yaml id : stdout type : stdout Write a pipeline specification \u00b6 # File: metronome-pipeline.yaml --- id : main interface : inputs : [ in ] outputs : [ out ] links : in : [ out ] Write a binding specification \u00b6 In tremor pipelines have no non-deterministic side-effects. By design, tremor does not allow onramps or offramps to be specified as a part of a pipeline. This would couple running pipelines to external connections. For example, to an external kafka broker and topic. This isn't bad per se, but it would allow a configuration or programming style that allows pipelines that are hard to distribute, clusterable or scalable. To be clear, therefore: All data processed by a tremor pipeline is always ingested via an event All events arrive into pipelines via 'input streams', operators that link a pipeline to the outside world All events leave a pipeline via 'output streams', operators that link a pipeline to the outside world Events always traverse a pipeline in graph order ( Depth-First-Search ) Where there is no imposed ordering ( in a branch ), tremor imposes declaration order Synthetic events ( signals from the tremor system runtime, or contraflow that derive from events already in-flight in a pipeline ) follow the same rules, without exception. All in-flight events in a pipeline are processed to completion before queued events are processed. As a result, in order to connect onramps, offramps and pipelines , we need to link them together. We call this set of ( required ) links a 'binding specification'. It is ok not to connect a pipeline input stream or output stream. But it is not ok to not connect the subset exposed in a binding specification. For our scenario, the following will suffice: # File: metronome-binding.yaml id : default links : \"/onramp/metronome/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/stdout/{instance}/in\" ] Ths creates a binding specification. Again this does not instantiate the referenced onramps, offramps or pipelines. This is also just a blueprint of a connected topology with a unique identifier default . Publish via the REST API / curl \u00b6 Publish onramp specification \u00b6 curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-onramp.yaml http://localhost:9898/onramp Check that it published ok: $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/onramp - metronome Publish offramp specification \u00b6 curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-offramp.yaml http://localhost:9898/offramp Check that it published ok: curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/offramp - stdout Publish pipeline specification \u00b6 curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-pipeline.yaml http://localhost:9898/pipeline Check that it published ok: $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/pipeline - main Publish binding specification \u00b6 curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-binding.yaml http://localhost:9898/binding $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/binding - default Publish metronome offramp specification \u00b6 curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-offramp.yaml http://localhost:9898/offramp Check that it published ok: curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/offramp - default Publish via tremor \u00b6 The tremor command allows the exact sample set of interactions as above. For brevity we simpilify the examples in this section but the steps are the same. Tremor tool, however, makes it easier to switch between JSON and YAML Publish all specifications \u00b6 Publish onramp, offramp, pipeline and binding: tremor api onramp create metronome-onramp.yaml tremor api offramp create metronome-offramp.yaml tremor api pipeline create metronome-pipeline.yaml tremor api binding create metronome-binding.yaml Check all our artefacts have published ok: tremor api onramp list tremor api offramp list tremor api pipeline list tremor api binding list Limitations \u00b6 Live deployments via the API only work with a single entity and passing a list using the API isn't supported. In order to achieve that you can use 'Static or Bootstrap Deployments Deployment \u00b6 Once all artefacts are published into the tremor repository we are ready to deploy. We deploy instances, via bindings, through mapping specifications. In all steps to this point, we have been populating the tremor repository. Like a git repository the tremor repository stores artefacts, like git stores code. When we publish a mapping we are deploying live instances of onramps, offramps, and pipelines, in our case, we want to: Deploy a single metronome onramp instance Deploy a single stdout offramp instance Deploy a single passthrough pipeline We want the onramp to connect to the pipeline We want the offramp to connect to the pipeline In our final step we specify: We want to call our instance 'walkthrough' A Mapping specification contains values for the placeholders (with curly braces, e.g. {instance} ) in the binding specification. # File: metronome-mapping.yaml instance : \"walkthrough\" We do not deploy or publish a mapping, we rather instantiate a binding specification and provide a mapping with the placeholder values. Deploy via curl: curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-mapping.yaml http://localhost:9898/binding/default/walkthrough Deploy via tremor: tremor api binding activate default walkthrough metronome-mapping.yaml The result is that all referenced onramps, offramps and pipelines specified in the binding are live and linked and events flow through them.","title":"Configuration Walkthrough"},{"location":"operations/configuration-walkthrough/#configuration-walkthrough","text":"A short canned synopsis of configuration tremor. This guide walks through configuring tremor via its API directly and via its command line tool 'tremor'. For the API, we use 'curl' on the command line.","title":"Configuration Walkthrough"},{"location":"operations/configuration-walkthrough/#introduction","text":"In this walkthrough we will deploy a tremor pipeline that generates a periodic sequence of messages or heartbeats every second. The solution is composed of the following tremor artefacts: A metronome onramp - our periodic message generator A stdout offramp - an offramp that serializes to standard output useful for debugging A pipeline - we pass the input ( metronome events ) to our output In this walkthrough we configure a single onramp, offramp and pipeline but many other configurations are possible.","title":"Introduction"},{"location":"operations/configuration-walkthrough/#prerequisites","text":"","title":"Prerequisites"},{"location":"operations/configuration-walkthrough/#write-an-onramp-specification","text":"This creates a config specification for an onramp that can be referenced by the unique id metronome . It does not create an instance of it. # File: metronome-onramp.yaml id : metronome type : metronome config : interval : 1000","title":"Write an onramp specification"},{"location":"operations/configuration-walkthrough/#write-an-offramp-specification","text":"This creates a config specification for an offramp that can be referenced by the unique id stdout . It does not create an instance of it. Think of this as a blueprint. # File: metronome-offramp.yaml id : stdout type : stdout","title":"Write an offramp specification"},{"location":"operations/configuration-walkthrough/#write-a-pipeline-specification","text":"# File: metronome-pipeline.yaml --- id : main interface : inputs : [ in ] outputs : [ out ] links : in : [ out ]","title":"Write a pipeline specification"},{"location":"operations/configuration-walkthrough/#write-a-binding-specification","text":"In tremor pipelines have no non-deterministic side-effects. By design, tremor does not allow onramps or offramps to be specified as a part of a pipeline. This would couple running pipelines to external connections. For example, to an external kafka broker and topic. This isn't bad per se, but it would allow a configuration or programming style that allows pipelines that are hard to distribute, clusterable or scalable. To be clear, therefore: All data processed by a tremor pipeline is always ingested via an event All events arrive into pipelines via 'input streams', operators that link a pipeline to the outside world All events leave a pipeline via 'output streams', operators that link a pipeline to the outside world Events always traverse a pipeline in graph order ( Depth-First-Search ) Where there is no imposed ordering ( in a branch ), tremor imposes declaration order Synthetic events ( signals from the tremor system runtime, or contraflow that derive from events already in-flight in a pipeline ) follow the same rules, without exception. All in-flight events in a pipeline are processed to completion before queued events are processed. As a result, in order to connect onramps, offramps and pipelines , we need to link them together. We call this set of ( required ) links a 'binding specification'. It is ok not to connect a pipeline input stream or output stream. But it is not ok to not connect the subset exposed in a binding specification. For our scenario, the following will suffice: # File: metronome-binding.yaml id : default links : \"/onramp/metronome/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/stdout/{instance}/in\" ] Ths creates a binding specification. Again this does not instantiate the referenced onramps, offramps or pipelines. This is also just a blueprint of a connected topology with a unique identifier default .","title":"Write a binding specification"},{"location":"operations/configuration-walkthrough/#publish-via-the-rest-api-curl","text":"","title":"Publish via the REST API / curl"},{"location":"operations/configuration-walkthrough/#publish-onramp-specification","text":"curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-onramp.yaml http://localhost:9898/onramp Check that it published ok: $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/onramp - metronome","title":"Publish onramp specification"},{"location":"operations/configuration-walkthrough/#publish-offramp-specification","text":"curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-offramp.yaml http://localhost:9898/offramp Check that it published ok: curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/offramp - stdout","title":"Publish offramp specification"},{"location":"operations/configuration-walkthrough/#publish-pipeline-specification","text":"curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-pipeline.yaml http://localhost:9898/pipeline Check that it published ok: $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/pipeline - main","title":"Publish pipeline specification"},{"location":"operations/configuration-walkthrough/#publish-binding-specification","text":"curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-binding.yaml http://localhost:9898/binding $ curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/binding - default","title":"Publish binding specification"},{"location":"operations/configuration-walkthrough/#publish-metronome-offramp-specification","text":"curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-offramp.yaml http://localhost:9898/offramp Check that it published ok: curl -vs --stderr - -H \"Accept: application/yaml\" http://localhost:9898/offramp - default","title":"Publish metronome offramp specification"},{"location":"operations/configuration-walkthrough/#publish-via-tremor","text":"The tremor command allows the exact sample set of interactions as above. For brevity we simpilify the examples in this section but the steps are the same. Tremor tool, however, makes it easier to switch between JSON and YAML","title":"Publish via tremor"},{"location":"operations/configuration-walkthrough/#publish-all-specifications","text":"Publish onramp, offramp, pipeline and binding: tremor api onramp create metronome-onramp.yaml tremor api offramp create metronome-offramp.yaml tremor api pipeline create metronome-pipeline.yaml tremor api binding create metronome-binding.yaml Check all our artefacts have published ok: tremor api onramp list tremor api offramp list tremor api pipeline list tremor api binding list","title":"Publish all specifications"},{"location":"operations/configuration-walkthrough/#limitations","text":"Live deployments via the API only work with a single entity and passing a list using the API isn't supported. In order to achieve that you can use 'Static or Bootstrap Deployments","title":"Limitations"},{"location":"operations/configuration-walkthrough/#deployment","text":"Once all artefacts are published into the tremor repository we are ready to deploy. We deploy instances, via bindings, through mapping specifications. In all steps to this point, we have been populating the tremor repository. Like a git repository the tremor repository stores artefacts, like git stores code. When we publish a mapping we are deploying live instances of onramps, offramps, and pipelines, in our case, we want to: Deploy a single metronome onramp instance Deploy a single stdout offramp instance Deploy a single passthrough pipeline We want the onramp to connect to the pipeline We want the offramp to connect to the pipeline In our final step we specify: We want to call our instance 'walkthrough' A Mapping specification contains values for the placeholders (with curly braces, e.g. {instance} ) in the binding specification. # File: metronome-mapping.yaml instance : \"walkthrough\" We do not deploy or publish a mapping, we rather instantiate a binding specification and provide a mapping with the placeholder values. Deploy via curl: curl -vs -stderr -X POST -H \"Content-Type: application/yaml\" --data-binary @metronome-mapping.yaml http://localhost:9898/binding/default/walkthrough Deploy via tremor: tremor api binding activate default walkthrough metronome-mapping.yaml The result is that all referenced onramps, offramps and pipelines specified in the binding are live and linked and events flow through them.","title":"Deployment"},{"location":"operations/configuration/","text":"Configuring Tremor \u00b6 This is a short canned synopsis of tremor configuration. Tremor supports dynamic reconfiguration since v0.4. Introduction \u00b6 The tremor runtime is internally structured with repositories and registries of configurable elements or artefacts. These artefacts can be one of: Onramp - Specify to tremor how to connect to the outside world to ingest or consume external data. For example, the Kafka onramp consumes data from Kafka topics. Offramp - Specify to tremor how to connect to the outside world to publish data to external systems For example, the Elastic offramp pushes data to ElasticSearch via its bulk upload REST/HTTP API endpoint Pipeline - Specify to tremor what operations to perform on data ingested ( from any connected upstream source ) and what to contribute or publish downstream ( to any connected downstream target ). Binding - A binding is a specification of how to interconnect Onramps, Offramps and Pipelines. Binding specifications can be thought of a type of wiring harness Mapping - A mapping instantiated multiple bindings and fills in template variables. Specifications for onramps, offramps, pipelines and bindings in tremor should be considered templates. They are stored in an internal tremor repository . A tremor repository stores artefacts, much like git repositories code. Live onramps, offramps and pipelines in tremor are in a runnable state. They consume typically network bandwidth and some compute in the case of onramps and offramps. They consume compute bandwidth in the case of pipelines. In order to create live instances of onramps, offramps and pipelines, a binding must be deployed . This is done by specifying a mapping which consists of a mapping from binding instance url to the template values to fill in placeholders in the Binding specification. For each entry in the mapping a binding instance is created and all referenced connected onramps, pipelines and offramps are instantiated with it. Live instances of tremor artefacts are stored in a registry . A tremor registry can be thought of similarly to the Domain Name Service or DNS. All 'live' or 'deployed' instances in tremor ( onramps, offramps, pipelines ) are managed by a finite state machine. Deployment Types \u00b6 In this section we explore the two basic types of deployment in tremor. +------------+ \"publish\" +--------------+ \"bind/deploy\" +--------------+ | +-------------------->+ +---------------------->+ | | | | | | | | Artefact | onramp, pipeline | Artefact | mapping | Instance | | | offramp, binding | Repository | | Registry | | | | | | | | +<--------------------+ +<----------------------+ | +------------+ \"find\" +--------------+ \"unbind/undeploy\" +--------------+ Tremor leverages the registry/repository and publish-find-bind Service Oriented Architecture patterns to drive its configuration model. Onramp, Offramp and Pipeline configurations can be published as template specifications along with how they should be interconnected as binding specifications. As all artefacts in tremor are named, when a mapping is published, it deploys all the required onramps, offramps and pipelines automatically. This also means that when a mapping is deleted, that the corresponding live instances are undeployed. All live or running artefacts have a corresponding state machine that manages its deployment lifecycle. The FSM is a simplified version of the POA worker activator lifecycle from CORBA and other Application Server Platforms. Using the docker image \u00b6 When using the tremor docker image configuration is loaded from the folder /etc/tremor this folder should be mounted into the docker container to propagate the data. The following files are looked for: /etc/tremor/logger.yaml a log4rs configuration file to control logging in tremor. /etc/tremor/config/*.trickle all files will be loaded as trickle pipelines - trickle pipelines are always loaded before yaml configuration! The file name in this case (the part before .trickle ) is the pipeline id which you can use in places like the binding configuration. The #!config id = \"name\" preprocessor directive can be used to overwrite the naming. /etc/tremor/config/*.yaml all files will be loaded as configuration files and evaluated in order (so mappings cannot refer to artefacts in later files!) - NOTE: defining pipelines in yaml is deprecated and trickle pipelines should be used. Static or Bootstrap deployments \u00b6 Static or Bootstrap deployment allows tremor to be configured at startup with its registry and repository pre-populated with out of the box user defined configuration. For example, in the following example, tremor is started with a registry and repository that runs a micro benchmark on startup target/debug/tremor server run -f repo.yaml reg.yaml The repository: # File: repo.yaml # define a blaster that replays data from an archived JSON log file onramp : - id : blaster type : blaster config : source : ./demo/data/data.json.xz # define a blackhole that runs a 40 second benchmark and then stops offramp : - id : blackhole type : blackhole config : warmup_secs : 10 stop_after_secs : 40 significant_figures : 2 # define a passthrough pipeline for benchmarking pipeline : - id : main interface : inputs : - in outputs : - out nodes : - id : passthrough op : passthrough links : in : [ passthrough ] passthrough : [ out ] # define how blaster, the pipeline under test, and blackhole interconnect binding : - id : default links : \"/onramp/blaster/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/blackhole/{instance}/in\" ] Static deployments support multiple configurations. You can specify multiple configurations by using a list in the config instead of specifying one item as illustrated below: onramp : - id : kafka-in type : kafka codec : json config : brokers : - kafka:9092 topics : - demo group_id : demo - id : kafka-again type : kafka codec : json config : brokers : - kafka:9092 topics : - snotbadger group_id : demo This file 'loads' the repository on startup with various specifications or templates, but it doesn't do anything. For that, we need to define one or many instances for tremor to deploy. This is done in the reg.yaml file: mapping : /binding/default/01 : # deployment '01' instance : \"01\" # .. deploys, blaster/01, blackhole/01, pipeline/main/01 Interactive or Operational deployments \u00b6 Tremor's registry and repository can be configured dynamically via tremor's REST API, and via the tremor-cli tool. For example: tremor api onramp publish blaster.yaml tremor api offramp publish offramp.yaml tremor api pipeline publish main.yaml tremor api binding publish benchmark.yaml Or via curl: curl --data-binary @deployment.yaml http://localhost:9898/binding/default/01","title":"Configuration"},{"location":"operations/configuration/#configuring-tremor","text":"This is a short canned synopsis of tremor configuration. Tremor supports dynamic reconfiguration since v0.4.","title":"Configuring Tremor"},{"location":"operations/configuration/#introduction","text":"The tremor runtime is internally structured with repositories and registries of configurable elements or artefacts. These artefacts can be one of: Onramp - Specify to tremor how to connect to the outside world to ingest or consume external data. For example, the Kafka onramp consumes data from Kafka topics. Offramp - Specify to tremor how to connect to the outside world to publish data to external systems For example, the Elastic offramp pushes data to ElasticSearch via its bulk upload REST/HTTP API endpoint Pipeline - Specify to tremor what operations to perform on data ingested ( from any connected upstream source ) and what to contribute or publish downstream ( to any connected downstream target ). Binding - A binding is a specification of how to interconnect Onramps, Offramps and Pipelines. Binding specifications can be thought of a type of wiring harness Mapping - A mapping instantiated multiple bindings and fills in template variables. Specifications for onramps, offramps, pipelines and bindings in tremor should be considered templates. They are stored in an internal tremor repository . A tremor repository stores artefacts, much like git repositories code. Live onramps, offramps and pipelines in tremor are in a runnable state. They consume typically network bandwidth and some compute in the case of onramps and offramps. They consume compute bandwidth in the case of pipelines. In order to create live instances of onramps, offramps and pipelines, a binding must be deployed . This is done by specifying a mapping which consists of a mapping from binding instance url to the template values to fill in placeholders in the Binding specification. For each entry in the mapping a binding instance is created and all referenced connected onramps, pipelines and offramps are instantiated with it. Live instances of tremor artefacts are stored in a registry . A tremor registry can be thought of similarly to the Domain Name Service or DNS. All 'live' or 'deployed' instances in tremor ( onramps, offramps, pipelines ) are managed by a finite state machine.","title":"Introduction"},{"location":"operations/configuration/#deployment-types","text":"In this section we explore the two basic types of deployment in tremor. +------------+ \"publish\" +--------------+ \"bind/deploy\" +--------------+ | +-------------------->+ +---------------------->+ | | | | | | | | Artefact | onramp, pipeline | Artefact | mapping | Instance | | | offramp, binding | Repository | | Registry | | | | | | | | +<--------------------+ +<----------------------+ | +------------+ \"find\" +--------------+ \"unbind/undeploy\" +--------------+ Tremor leverages the registry/repository and publish-find-bind Service Oriented Architecture patterns to drive its configuration model. Onramp, Offramp and Pipeline configurations can be published as template specifications along with how they should be interconnected as binding specifications. As all artefacts in tremor are named, when a mapping is published, it deploys all the required onramps, offramps and pipelines automatically. This also means that when a mapping is deleted, that the corresponding live instances are undeployed. All live or running artefacts have a corresponding state machine that manages its deployment lifecycle. The FSM is a simplified version of the POA worker activator lifecycle from CORBA and other Application Server Platforms.","title":"Deployment Types"},{"location":"operations/configuration/#using-the-docker-image","text":"When using the tremor docker image configuration is loaded from the folder /etc/tremor this folder should be mounted into the docker container to propagate the data. The following files are looked for: /etc/tremor/logger.yaml a log4rs configuration file to control logging in tremor. /etc/tremor/config/*.trickle all files will be loaded as trickle pipelines - trickle pipelines are always loaded before yaml configuration! The file name in this case (the part before .trickle ) is the pipeline id which you can use in places like the binding configuration. The #!config id = \"name\" preprocessor directive can be used to overwrite the naming. /etc/tremor/config/*.yaml all files will be loaded as configuration files and evaluated in order (so mappings cannot refer to artefacts in later files!) - NOTE: defining pipelines in yaml is deprecated and trickle pipelines should be used.","title":"Using the docker image"},{"location":"operations/configuration/#static-or-bootstrap-deployments","text":"Static or Bootstrap deployment allows tremor to be configured at startup with its registry and repository pre-populated with out of the box user defined configuration. For example, in the following example, tremor is started with a registry and repository that runs a micro benchmark on startup target/debug/tremor server run -f repo.yaml reg.yaml The repository: # File: repo.yaml # define a blaster that replays data from an archived JSON log file onramp : - id : blaster type : blaster config : source : ./demo/data/data.json.xz # define a blackhole that runs a 40 second benchmark and then stops offramp : - id : blackhole type : blackhole config : warmup_secs : 10 stop_after_secs : 40 significant_figures : 2 # define a passthrough pipeline for benchmarking pipeline : - id : main interface : inputs : - in outputs : - out nodes : - id : passthrough op : passthrough links : in : [ passthrough ] passthrough : [ out ] # define how blaster, the pipeline under test, and blackhole interconnect binding : - id : default links : \"/onramp/blaster/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/blackhole/{instance}/in\" ] Static deployments support multiple configurations. You can specify multiple configurations by using a list in the config instead of specifying one item as illustrated below: onramp : - id : kafka-in type : kafka codec : json config : brokers : - kafka:9092 topics : - demo group_id : demo - id : kafka-again type : kafka codec : json config : brokers : - kafka:9092 topics : - snotbadger group_id : demo This file 'loads' the repository on startup with various specifications or templates, but it doesn't do anything. For that, we need to define one or many instances for tremor to deploy. This is done in the reg.yaml file: mapping : /binding/default/01 : # deployment '01' instance : \"01\" # .. deploys, blaster/01, blackhole/01, pipeline/main/01","title":"Static or Bootstrap deployments"},{"location":"operations/configuration/#interactive-or-operational-deployments","text":"Tremor's registry and repository can be configured dynamically via tremor's REST API, and via the tremor-cli tool. For example: tremor api onramp publish blaster.yaml tremor api offramp publish offramp.yaml tremor api pipeline publish main.yaml tremor api binding publish benchmark.yaml Or via curl: curl --data-binary @deployment.yaml http://localhost:9898/binding/default/01","title":"Interactive or Operational deployments"},{"location":"operations/gdcb/","text":"Circuit Breakers and Guaranteed Delivery \u00b6 With tremor 0.9 we introduced logic for Guaranteed Delivery (GD) and Circuit Breakers (CB) so it is worth discussing this addition and what to expect from them. The features are complementary but have different tradeoffs. We start with looking at a short comparison. support perf impact guarantee pre 0.9 all sources & sinks baseline if we can 0.9 CB most sources & sinks virtually none stop on known disconnect 0.9 GD select sources & sinks significant guaranteed where possible Let's add some clarification to this: pre 0.9 \u00b6 guarantee: we send the data with a best effort to deliver it but have no control over malfunctioning downstream systems. 0.9 CB \u00b6 support: Where the underlying transport supports it we support stopping consumption, as an example UDP has no support of stopping ingestion. perf impact: virtually none - there is a theoretical impact on this but it was small enough to be not measurable in our test. guarantee: If a sink sends a CB event the source will stop producing data, however any events send until them might be lost. 0.9 GD \u00b6 support: Unless explicitly noted by sources / sinks notes differently there are no guarantees. perf: As GD requires acknowledgments for delivered messages the impact of this is significant compared to other methods. guarantee: using GD will provide the minimum guarantee that the used sink and source offers. Example \u00b6 To emphasize on those differences we'll go through an example of how those methods work together. With the release we updated our integration tests, including the web-socket test. The test works by creating two tremor instances. One working as a producer, having a file source and a web-socket sink. The other being a consumer having a web-socket source and a file sink. The test is supposed to validate that all the data that the producer sends arrives at the consumer. There is an issue with that, since they are two different processes there is no guarantee the consumer is ready by the time the producer starts sending data. Web-sockets do support circuit breakers so the web-socket sink will report that its counterpart is down and stop reading the file. However for the CB logic to trigger and propagate some time passes so we observed the first 3 messages being 'lost' before the circuit breakers could kick in. This demonstrates the limitations of circuit breakers nicely, they do react to a bad sink but the reaction isn't instantaneous so we risk loosing a few messages. To get around this we can use the qos::wal operator that provides a higher delivery guarantee then the web-socket offramp. With the qos::wal operator we do get all the messages since the web-socket sink never acknowledges sending the message and it is considered failed. There is a caveat in this however, since web-sockets are not fully GD aware we only get a GD error when the web-socket can not send the message. If we had an intermittent failure that drops a few messages the qos::wal won't help as GD does only offer the weakest guarantee of the components involved - in this case the web-socket sink.","title":"Circuit Breakers and Guaranteed Delivery"},{"location":"operations/gdcb/#circuit-breakers-and-guaranteed-delivery","text":"With tremor 0.9 we introduced logic for Guaranteed Delivery (GD) and Circuit Breakers (CB) so it is worth discussing this addition and what to expect from them. The features are complementary but have different tradeoffs. We start with looking at a short comparison. support perf impact guarantee pre 0.9 all sources & sinks baseline if we can 0.9 CB most sources & sinks virtually none stop on known disconnect 0.9 GD select sources & sinks significant guaranteed where possible Let's add some clarification to this:","title":"Circuit Breakers and Guaranteed Delivery"},{"location":"operations/gdcb/#pre-09","text":"guarantee: we send the data with a best effort to deliver it but have no control over malfunctioning downstream systems.","title":"pre 0.9"},{"location":"operations/gdcb/#09-cb","text":"support: Where the underlying transport supports it we support stopping consumption, as an example UDP has no support of stopping ingestion. perf impact: virtually none - there is a theoretical impact on this but it was small enough to be not measurable in our test. guarantee: If a sink sends a CB event the source will stop producing data, however any events send until them might be lost.","title":"0.9 CB"},{"location":"operations/gdcb/#09-gd","text":"support: Unless explicitly noted by sources / sinks notes differently there are no guarantees. perf: As GD requires acknowledgments for delivered messages the impact of this is significant compared to other methods. guarantee: using GD will provide the minimum guarantee that the used sink and source offers.","title":"0.9 GD"},{"location":"operations/gdcb/#example","text":"To emphasize on those differences we'll go through an example of how those methods work together. With the release we updated our integration tests, including the web-socket test. The test works by creating two tremor instances. One working as a producer, having a file source and a web-socket sink. The other being a consumer having a web-socket source and a file sink. The test is supposed to validate that all the data that the producer sends arrives at the consumer. There is an issue with that, since they are two different processes there is no guarantee the consumer is ready by the time the producer starts sending data. Web-sockets do support circuit breakers so the web-socket sink will report that its counterpart is down and stop reading the file. However for the CB logic to trigger and propagate some time passes so we observed the first 3 messages being 'lost' before the circuit breakers could kick in. This demonstrates the limitations of circuit breakers nicely, they do react to a bad sink but the reaction isn't instantaneous so we risk loosing a few messages. To get around this we can use the qos::wal operator that provides a higher delivery guarantee then the web-socket offramp. With the qos::wal operator we do get all the messages since the web-socket sink never acknowledges sending the message and it is considered failed. There is a caveat in this however, since web-sockets are not fully GD aware we only get a GD error when the web-socket can not send the message. If we had an intermittent failure that drops a few messages the qos::wal won't help as GD does only offer the weakest guarantee of the components involved - in this case the web-socket sink.","title":"Example"},{"location":"operations/linked-transports/","text":"Linked Transports \u00b6 Attention Linked transports are in alpha status as of v0.9.0 and we recommend its use only for exploratory projects. Details around it (including any on this page) are likely to change, as the feature set matures. Tremor supports ingestion of events from external sources ( onramps ) and after processing them from pipelines, they can be written to external sinks ( offramps ). Since v0.9, Tremor also supports Linked Transports (LT): a mechanism that allows linking of source and sink nature into one ramp artefact. In other words -- once this mechanism is turned on -- a Tremor onramp can behave as an offramp (i.e. send events to the outside world) and similarly, a Tremor offramp can behave as an onramp (i.e. receive events from the outside world). This is specifically useful for onramps and offramps like REST and websocket, where the protocol already provides facility for responding to events, and as such, the mechanism is currently supported for those onramps and offramps only. With the addition of linked transports and the whole new possibilities for event-flow that comes with it, Tremor has become a platform for implementing a wider variety of applications -- think servers, proxies, bridges etc., and not just ETL-style use cases. Moreover, in combination with other Tremor features and the composability that is Tremor's signature, operators can create richer applications with linked transports at the center -- think loadbalancers, or custom APIs that dynamically change pipeline behaviour (without the need for pipeline redeploy). This document will describe the feature with concrete examples next, so if the above possibilities seem abstract to you, we hope it will be more clear by the end here. Basic configuration \u00b6 The linked behavior for an onramp or offramp can be turned on by setting the linked config param for the artefact to true (by default, it's false ). A simple Tremor deployment illustrating the feature: onramp : - id : websocket type : ws # turns on linked transport for this ramp linked : true codec : string preprocessors : # events are delimited by new line - lines postprocessors : - lines config : host : localhost port : 8139 binding : - id : echo links : # send incoming event from a websocket connection to the built-In # passthrough pipeline \"/onramp/websocket/{instance}/out\" : [ \"/pipeline/system::passthrough/system/in\" ] # and now send back the event from the same websocket connection \"/pipeline/system::passthrough/system/out\" : [ \"/onramp/websocket/{instance}/in\" ] mapping : /binding/echo/01 : instance : \"01\" This instantiates a websocket server that listens to (new-line delimited) events on port 8139. After an event comes through a client websocket connection, it goes to the built-in passthrough pipeline, and is then sent back out to the client from the same connection. We have now in our hands a classic websocket echo server! You can test the functionality with a tool like websocat . # we see the same message echoed back \\o/ $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello The offramp linking works similarly, with the offramp out port capturing the event coming back as a reply to the event transmitted to the external sink (examples of this in action are detailed below). Supported ramps \u00b6 Ramp artefacts that support linked transports are listed here: REST Onramp REST Offramp Websocket Onramp Websocket Offramp Discord onramp KV offramp As part of the above docs, you will also find event metadata variables that these onramps/offramps set (and use), which can be utilized as part of the wider application built using these aretefacts. Example use cases \u00b6 In the above example, instead of using a passthrough pipeline, you can imagine processing the incoming event from a custom trickle pipeline, with the various operators we have at our disposal. In this vein, more elaborate server examples based on onramp linking (and supporting request/response style interactions) are linked below: HTTP server Websocket server When linked onramps of this sort are coupled with linked offramps, we have proxy applications, where incoming requests from clients can be forwarded to upstream servers and the resulting response can then be returned back to the client which initiated the request. Custom proxying logic (eg: deciding the upstream based on incoming request attributes) can be coded up as part of the runtime script . Some concrete examples demonstrating this pattern: HTTP Proxy Websocket Proxy Example binding for a HTTP proxy - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to upstream \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] # send the response from upstream for processing # (here be linked offramp) \"/offramp/upstream/{instance}/out\" : 9[\"/pipeline/response_processing/{instance}/in\"] # process upstream response and send it back as a response to incoming # (here be linked onramp) \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] And when proxying, if we configure linked onramps and offramps of different types, we have bridges: HTTP -> WS Bridge Or when the proxying use case is combined with some qos operators ( roundrobin and backpressure ), we get a working load-balancer: HTTP Load Balancing These are some example use cases now possible with linked transports at the center, but with the amount of flexibility and composability that Tremor supports for its various capabilities, we can get very creative with what we can do here -- our imagination is the limit. Examples of even more advanced Tremor applications: Quota Service Configurator Error handling \u00b6 The above linked examples also demonstrate typical error handling needed for applications built on top of linked transports (eg: for HTTP-based applications, how to send a proper error response to the client with an appropriate status code on tremor-internal failures like runtime script errors, or codec errors on invalid input). Example error-handling binding for a HTTP proxy - id : error links : \"/onramp/http/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/request_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/response_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] # send back errors as response as well \"/pipeline/internal_error_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] # respond on errors during error processing too \"/pipeline/internal_error_processing/{instance}/err\" : [ \"/onramp/http/{instance The key here is remembering to link the error ports for all the onramp/offramp/pipeline artefacts involved in the main event-flow, ensuring that the end destination for error events (emitted from the err ports) are visible to the client (or user). Correlation \u00b6 If requests to and responses from a linked transport need to be correlated, the special metadata field $correlation can be used. It will be forwarded from the request event to the response event, so that some data from the request can be present in the response context. Example pipeline: define script correlate script let $ correlation = event . correlation_id ; let event = patch event of erase \"correlation_id\" end ; event end ; create script correlate ; select event from in into correlate ; select event from correlate into out ; select event from correlate / err into err ; This script within the pipeline file above will erase correlation_id from the event and put it into the $correlation metadata. It will be available in the response pipeline, also as $correlation . If both events need to be fully present and $correlation is only used for having a common unique identifier to bring them both together, a windowed select statement with a group by clause can help: define tumbling window window_size_2 with size = 2 # eviction_period = ... end ; select aggr :: win :: collect_flattened ( { \"event\" : event , \"meta\" : $ } ) from in [ window_size_2 ] group by $ correlation into out ; This pipeline will output both request and response as 2-element array: [{ \"event\" : {}, \"meta\" : { \"correlation\" : \"123456\" }}, { \"event\" : {}, \"meta\" : { \"correlation\" : \"123456\" }}] Future work \u00b6 v0.9.0 introduces linked transports as a feature preview. There are known rough edges and issues with the current mode of configuring linked transports, and also how the richer, linked-transports-powered capabilities interface with rest of Tremor configuration. Some known items for future work, aimed at improving the overall usability: Improve the LT port linking for onramps/offramps ( onramp/in and offramp/out are not natural there) Resolve the boilerplate involved in various aspects of LT use (eg: error handling, pipeline flow, server code) Simplify scatter-gather workflows Guaranteed delivery of events between pipelines Allow certain meta-variables (eg: rate/cardinality) to be set dynamically Better namespacing/sharing for meta-variables","title":"Linked Transports"},{"location":"operations/linked-transports/#linked-transports","text":"Attention Linked transports are in alpha status as of v0.9.0 and we recommend its use only for exploratory projects. Details around it (including any on this page) are likely to change, as the feature set matures. Tremor supports ingestion of events from external sources ( onramps ) and after processing them from pipelines, they can be written to external sinks ( offramps ). Since v0.9, Tremor also supports Linked Transports (LT): a mechanism that allows linking of source and sink nature into one ramp artefact. In other words -- once this mechanism is turned on -- a Tremor onramp can behave as an offramp (i.e. send events to the outside world) and similarly, a Tremor offramp can behave as an onramp (i.e. receive events from the outside world). This is specifically useful for onramps and offramps like REST and websocket, where the protocol already provides facility for responding to events, and as such, the mechanism is currently supported for those onramps and offramps only. With the addition of linked transports and the whole new possibilities for event-flow that comes with it, Tremor has become a platform for implementing a wider variety of applications -- think servers, proxies, bridges etc., and not just ETL-style use cases. Moreover, in combination with other Tremor features and the composability that is Tremor's signature, operators can create richer applications with linked transports at the center -- think loadbalancers, or custom APIs that dynamically change pipeline behaviour (without the need for pipeline redeploy). This document will describe the feature with concrete examples next, so if the above possibilities seem abstract to you, we hope it will be more clear by the end here.","title":"Linked Transports"},{"location":"operations/linked-transports/#basic-configuration","text":"The linked behavior for an onramp or offramp can be turned on by setting the linked config param for the artefact to true (by default, it's false ). A simple Tremor deployment illustrating the feature: onramp : - id : websocket type : ws # turns on linked transport for this ramp linked : true codec : string preprocessors : # events are delimited by new line - lines postprocessors : - lines config : host : localhost port : 8139 binding : - id : echo links : # send incoming event from a websocket connection to the built-In # passthrough pipeline \"/onramp/websocket/{instance}/out\" : [ \"/pipeline/system::passthrough/system/in\" ] # and now send back the event from the same websocket connection \"/pipeline/system::passthrough/system/out\" : [ \"/onramp/websocket/{instance}/in\" ] mapping : /binding/echo/01 : instance : \"01\" This instantiates a websocket server that listens to (new-line delimited) events on port 8139. After an event comes through a client websocket connection, it goes to the built-in passthrough pipeline, and is then sent back out to the client from the same connection. We have now in our hands a classic websocket echo server! You can test the functionality with a tool like websocat . # we see the same message echoed back \\o/ $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello The offramp linking works similarly, with the offramp out port capturing the event coming back as a reply to the event transmitted to the external sink (examples of this in action are detailed below).","title":"Basic configuration"},{"location":"operations/linked-transports/#supported-ramps","text":"Ramp artefacts that support linked transports are listed here: REST Onramp REST Offramp Websocket Onramp Websocket Offramp Discord onramp KV offramp As part of the above docs, you will also find event metadata variables that these onramps/offramps set (and use), which can be utilized as part of the wider application built using these aretefacts.","title":"Supported ramps"},{"location":"operations/linked-transports/#example-use-cases","text":"In the above example, instead of using a passthrough pipeline, you can imagine processing the incoming event from a custom trickle pipeline, with the various operators we have at our disposal. In this vein, more elaborate server examples based on onramp linking (and supporting request/response style interactions) are linked below: HTTP server Websocket server When linked onramps of this sort are coupled with linked offramps, we have proxy applications, where incoming requests from clients can be forwarded to upstream servers and the resulting response can then be returned back to the client which initiated the request. Custom proxying logic (eg: deciding the upstream based on incoming request attributes) can be coded up as part of the runtime script . Some concrete examples demonstrating this pattern: HTTP Proxy Websocket Proxy Example binding for a HTTP proxy - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to upstream \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] # send the response from upstream for processing # (here be linked offramp) \"/offramp/upstream/{instance}/out\" : 9[\"/pipeline/response_processing/{instance}/in\"] # process upstream response and send it back as a response to incoming # (here be linked onramp) \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] And when proxying, if we configure linked onramps and offramps of different types, we have bridges: HTTP -> WS Bridge Or when the proxying use case is combined with some qos operators ( roundrobin and backpressure ), we get a working load-balancer: HTTP Load Balancing These are some example use cases now possible with linked transports at the center, but with the amount of flexibility and composability that Tremor supports for its various capabilities, we can get very creative with what we can do here -- our imagination is the limit. Examples of even more advanced Tremor applications: Quota Service Configurator","title":"Example use cases"},{"location":"operations/linked-transports/#error-handling","text":"The above linked examples also demonstrate typical error handling needed for applications built on top of linked transports (eg: for HTTP-based applications, how to send a proper error response to the client with an appropriate status code on tremor-internal failures like runtime script errors, or codec errors on invalid input). Example error-handling binding for a HTTP proxy - id : error links : \"/onramp/http/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/request_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/response_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] # send back errors as response as well \"/pipeline/internal_error_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] # respond on errors during error processing too \"/pipeline/internal_error_processing/{instance}/err\" : [ \"/onramp/http/{instance The key here is remembering to link the error ports for all the onramp/offramp/pipeline artefacts involved in the main event-flow, ensuring that the end destination for error events (emitted from the err ports) are visible to the client (or user).","title":"Error handling"},{"location":"operations/linked-transports/#correlation","text":"If requests to and responses from a linked transport need to be correlated, the special metadata field $correlation can be used. It will be forwarded from the request event to the response event, so that some data from the request can be present in the response context. Example pipeline: define script correlate script let $ correlation = event . correlation_id ; let event = patch event of erase \"correlation_id\" end ; event end ; create script correlate ; select event from in into correlate ; select event from correlate into out ; select event from correlate / err into err ; This script within the pipeline file above will erase correlation_id from the event and put it into the $correlation metadata. It will be available in the response pipeline, also as $correlation . If both events need to be fully present and $correlation is only used for having a common unique identifier to bring them both together, a windowed select statement with a group by clause can help: define tumbling window window_size_2 with size = 2 # eviction_period = ... end ; select aggr :: win :: collect_flattened ( { \"event\" : event , \"meta\" : $ } ) from in [ window_size_2 ] group by $ correlation into out ; This pipeline will output both request and response as 2-element array: [{ \"event\" : {}, \"meta\" : { \"correlation\" : \"123456\" }}, { \"event\" : {}, \"meta\" : { \"correlation\" : \"123456\" }}]","title":"Correlation"},{"location":"operations/linked-transports/#future-work","text":"v0.9.0 introduces linked transports as a feature preview. There are known rough edges and issues with the current mode of configuring linked transports, and also how the richer, linked-transports-powered capabilities interface with rest of Tremor configuration. Some known items for future work, aimed at improving the overall usability: Improve the LT port linking for onramps/offramps ( onramp/in and offramp/out are not natural there) Resolve the boilerplate involved in various aspects of LT use (eg: error handling, pipeline flow, server code) Simplify scatter-gather workflows Guaranteed delivery of events between pipelines Allow certain meta-variables (eg: rate/cardinality) to be set dynamically Better namespacing/sharing for meta-variables","title":"Future work"},{"location":"operations/monitoring/","text":"Monitoring \u00b6 Monitoring tremor is done using tremor itself. This has some interesting implications. Each onramp, pipeline and offramp emits metrics on its operations into a pipeline called /pipeline/system::metrics/system . This allows to both write these messages to a destination system such as InfluxDB, as well as to a message queue such as Kafka. Metrics are published to the system::metrics pipeline every 10 second or if an event was received by the pipeline (whatever happened later). To enable monitoring, the metrics_interval_s key must be specified as part of onramp, pipeline and offramp config (depending on which metrics you want to capture), followed by the amount of seconds that should pass between emits. Metrics are formatted following the same structure as the Influx Codec . Pipeline metrics \u00b6 Example: { \"measurement\" : \"events\" , \"tags\" : { \"direction\" : \"output\" , \"node\" : \"in\" , \"pipeline\" : \"tremor:///pipeline/main/01\" , \"port\" : \"out\" }, \"fields\" : { \"count\" : 20 }, \"timestamp\" : 1553077007898214000 } In influx format: events,port=out,direction=output,node=in,pipeline=tremor:///pipeline/main/01 count=20 1553077007898214000 In this structure measurement is always events as that is what this is measuring. The number of events is always in the field count as we are counting them. The tags section explains where this measurement was taken: direction means if this event came into the node \"input\" or came out of the node \"output\" node is the id of the node in a given pipeline pipeline is the id of the pipeline port is the point the event was received or send from The example above measures all events that left the in of pipeline main . In addition to the general pipeline metrics, some operators do generate their own metrics, for details please check on the documentation for the operator in question. Ramp metrics \u00b6 { \"measurement\" : \"ramp_events\" , \"tags\" : { \"port\" : \"out\" , \"ramp\" : \"tremor:///offramp/main/01/in\" }, \"fields\" : { \"count\" : 42 }, \"timestamp\" : 1576215344378248634 } In influx format: ramp_events,port=out,ramp=tremor:///offramp/main/01 count=42 1576215344378248634 In this structure measurement is always ramp_events as that is what this is measuring. The number of events is always in the field count as we are counting them. The tags section explains where this measurement was taken: ramp is the id of the onramp/offramp port is one of in , out and error The example above measures all events that were emitted out by the offramp main . Notes: Preprocessor and codec level errors count as errors for onramp metrics. For onramps, count for in port is always zero since an event in tremor is something concrete only after the initial onramp processing. Furthermore, for stream-based onramps like tcp, the idea of counting in events does not make sense. If your pipeline is using the batch operator and offramp is receiving events from it, no of events tracked at offramp is going to be dictated by the batching config. Operator level metrics \u00b6 In addition to the metrics provided by the pipeline itself, some operators can generate additional metrics. The details are documented on a per operator level. Currently the following operators provide custom metrics: grouper::bucket Enriching \u00b6 The metrics event holds the minimum required data to identify the event inside the instance. This has the purpose of allowing enrichment using tremors existing facilities. An example of enriching the metrics with the hostname would be the configuration: pipeline : - id : enrich interface : inputs : - in outputs : - out nodes : - id : enrich op : runtime::tremor config : script : | let event.tags.host = system::hostname(); event; links : in : [ enrich ] enrich : [ out ] binding : - id : enrich links : \"/pipeline/system::metrics/system/out\" : [ \"/pipeline/enrich/system/in\" ] \"/pipeline/enrich/system/out\" : [ \"/offramp/system::stdout/system/in\" ] This will take the metrics from the metrics pipeline and run a tremor script to add the host tag to each event. Example \u00b6 Lets walk through a example to see how where and why metrics are generated. Lets configure the following configuration: onramp : - id : kafka-in-stores type : kafka codec : json config : # abbirivated offramp : - id : elastic-out-stores type : elastic config : # abbirivated pipeline : - id : main metrics_interval_s : 10 interface : inputs : - in outputs : - out nodes : - id : runtime op : runtime::tremor config : script : | export class, rate, index, doc_type; _ { $index := string::format(\"{}_tremor\", index_type); $doc_type := \"_doc\"; } logger_name=\"log_info_level\" { $class := \"logger_info\"; $rate := 1875; return; } logger_name { $class := \"logger\"; $rate := 250; return; } index_type { $class := \"default\"; $rate := 25; return; } - id : bucket op : grouper::bucket - id : bp op : generic::backpressure config : timeout : 100 - id : batch op : generic::batch config : count : 20 links : in : [ runtime ] runtime : [ bucket ] bucket : [ bp ] bp : [ batch ] batch : [ out ] binding : - id : stores-test links : \"/onramp/kafka-in-stores/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/elastic-out-stores/{instance}/in\" ] +---------+ +---------+ | Kafka | | elastic | +---------+ +---------+ | ^ v | +---------+ +---------+ | in | | out | +----a----+ +----l----+ | ^ v | +----b----+ +---------+ +---------+ +----k----+ | runtime c----->d bucket f----->g bp i----->j batch | +---------+ +----e----+ +----h----+ +---------+ | | | | v v Tremor instruments this pipeline on the points a to l . As follows (timestamp abbreviated by \u2026 , assuming 40 events were passed in): a as events,node=in,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=40 \u2026 counting the events entering the pipeline b as events,node=runtime,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it to the runtime (should be equal to a ) c as events,node=runtime,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it out of the runtime (should be equal to b ) d as events,node=bucket,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it to the bucketer (should be equal to c ) e as events,node=bucket,port=overflow,direction=output,pipeline=tremor:///pipeline/main/01 count=10 \u2026 as the events that the overflowed from the bucketer (aka they exceeded the limits in $rate ) - we assume 10 of the 40 messages hit this criteria f as events,node=bucket,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=30 \u2026 as the events that make it out of the bucketer (we assume 30 here since in e we had 10 events overflow) d as events,node=bp,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=30 \u2026 as the events that make it to the back-pressure ( bp ) step (should be equal to f ) h as events,node=bp,port=overflow,direction=output,pipeline=tremor:///pipeline/main/01 count=5 \u2026 as the events that the overflowed from the back-pressure step (aka the offramp asked is to back off a bit) - we assume 5 events fell into the back-pressure period. i as events,node=bp,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=25 \u2026 as the events that make it out of the back pressure step (we assume 25 here since in h we had 5 events overflow) j as events,node=batch,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=25 \u2026 as the events that make it to the batch step (should be equal to i ) k as events,node=batch,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=1 \u2026 this is a tricky one, we got 25 events into this, however we batch by 20 events, so the first 20 events that come in get send out as a batch so we have 1 as a count - it should be noted that at the time of this snapshot 5 more events are currently held by the batch step but not send out. If we had a batch size of 30 defined we would see no output events here. b as events,node=out,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=1 \u2026 the events that make it to the output to be written, since we wrote only a single batch we get a count of 1 here as in k","title":"Monitoring"},{"location":"operations/monitoring/#monitoring","text":"Monitoring tremor is done using tremor itself. This has some interesting implications. Each onramp, pipeline and offramp emits metrics on its operations into a pipeline called /pipeline/system::metrics/system . This allows to both write these messages to a destination system such as InfluxDB, as well as to a message queue such as Kafka. Metrics are published to the system::metrics pipeline every 10 second or if an event was received by the pipeline (whatever happened later). To enable monitoring, the metrics_interval_s key must be specified as part of onramp, pipeline and offramp config (depending on which metrics you want to capture), followed by the amount of seconds that should pass between emits. Metrics are formatted following the same structure as the Influx Codec .","title":"Monitoring"},{"location":"operations/monitoring/#pipeline-metrics","text":"Example: { \"measurement\" : \"events\" , \"tags\" : { \"direction\" : \"output\" , \"node\" : \"in\" , \"pipeline\" : \"tremor:///pipeline/main/01\" , \"port\" : \"out\" }, \"fields\" : { \"count\" : 20 }, \"timestamp\" : 1553077007898214000 } In influx format: events,port=out,direction=output,node=in,pipeline=tremor:///pipeline/main/01 count=20 1553077007898214000 In this structure measurement is always events as that is what this is measuring. The number of events is always in the field count as we are counting them. The tags section explains where this measurement was taken: direction means if this event came into the node \"input\" or came out of the node \"output\" node is the id of the node in a given pipeline pipeline is the id of the pipeline port is the point the event was received or send from The example above measures all events that left the in of pipeline main . In addition to the general pipeline metrics, some operators do generate their own metrics, for details please check on the documentation for the operator in question.","title":"Pipeline metrics"},{"location":"operations/monitoring/#ramp-metrics","text":"{ \"measurement\" : \"ramp_events\" , \"tags\" : { \"port\" : \"out\" , \"ramp\" : \"tremor:///offramp/main/01/in\" }, \"fields\" : { \"count\" : 42 }, \"timestamp\" : 1576215344378248634 } In influx format: ramp_events,port=out,ramp=tremor:///offramp/main/01 count=42 1576215344378248634 In this structure measurement is always ramp_events as that is what this is measuring. The number of events is always in the field count as we are counting them. The tags section explains where this measurement was taken: ramp is the id of the onramp/offramp port is one of in , out and error The example above measures all events that were emitted out by the offramp main . Notes: Preprocessor and codec level errors count as errors for onramp metrics. For onramps, count for in port is always zero since an event in tremor is something concrete only after the initial onramp processing. Furthermore, for stream-based onramps like tcp, the idea of counting in events does not make sense. If your pipeline is using the batch operator and offramp is receiving events from it, no of events tracked at offramp is going to be dictated by the batching config.","title":"Ramp metrics"},{"location":"operations/monitoring/#operator-level-metrics","text":"In addition to the metrics provided by the pipeline itself, some operators can generate additional metrics. The details are documented on a per operator level. Currently the following operators provide custom metrics: grouper::bucket","title":"Operator level metrics"},{"location":"operations/monitoring/#enriching","text":"The metrics event holds the minimum required data to identify the event inside the instance. This has the purpose of allowing enrichment using tremors existing facilities. An example of enriching the metrics with the hostname would be the configuration: pipeline : - id : enrich interface : inputs : - in outputs : - out nodes : - id : enrich op : runtime::tremor config : script : | let event.tags.host = system::hostname(); event; links : in : [ enrich ] enrich : [ out ] binding : - id : enrich links : \"/pipeline/system::metrics/system/out\" : [ \"/pipeline/enrich/system/in\" ] \"/pipeline/enrich/system/out\" : [ \"/offramp/system::stdout/system/in\" ] This will take the metrics from the metrics pipeline and run a tremor script to add the host tag to each event.","title":"Enriching"},{"location":"operations/monitoring/#example","text":"Lets walk through a example to see how where and why metrics are generated. Lets configure the following configuration: onramp : - id : kafka-in-stores type : kafka codec : json config : # abbirivated offramp : - id : elastic-out-stores type : elastic config : # abbirivated pipeline : - id : main metrics_interval_s : 10 interface : inputs : - in outputs : - out nodes : - id : runtime op : runtime::tremor config : script : | export class, rate, index, doc_type; _ { $index := string::format(\"{}_tremor\", index_type); $doc_type := \"_doc\"; } logger_name=\"log_info_level\" { $class := \"logger_info\"; $rate := 1875; return; } logger_name { $class := \"logger\"; $rate := 250; return; } index_type { $class := \"default\"; $rate := 25; return; } - id : bucket op : grouper::bucket - id : bp op : generic::backpressure config : timeout : 100 - id : batch op : generic::batch config : count : 20 links : in : [ runtime ] runtime : [ bucket ] bucket : [ bp ] bp : [ batch ] batch : [ out ] binding : - id : stores-test links : \"/onramp/kafka-in-stores/{instance}/out\" : [ \"/pipeline/main/{instance}/in\" ] \"/pipeline/main/{instance}/out\" : [ \"/offramp/elastic-out-stores/{instance}/in\" ] +---------+ +---------+ | Kafka | | elastic | +---------+ +---------+ | ^ v | +---------+ +---------+ | in | | out | +----a----+ +----l----+ | ^ v | +----b----+ +---------+ +---------+ +----k----+ | runtime c----->d bucket f----->g bp i----->j batch | +---------+ +----e----+ +----h----+ +---------+ | | | | v v Tremor instruments this pipeline on the points a to l . As follows (timestamp abbreviated by \u2026 , assuming 40 events were passed in): a as events,node=in,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=40 \u2026 counting the events entering the pipeline b as events,node=runtime,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it to the runtime (should be equal to a ) c as events,node=runtime,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it out of the runtime (should be equal to b ) d as events,node=bucket,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=40 \u2026 as the events that make it to the bucketer (should be equal to c ) e as events,node=bucket,port=overflow,direction=output,pipeline=tremor:///pipeline/main/01 count=10 \u2026 as the events that the overflowed from the bucketer (aka they exceeded the limits in $rate ) - we assume 10 of the 40 messages hit this criteria f as events,node=bucket,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=30 \u2026 as the events that make it out of the bucketer (we assume 30 here since in e we had 10 events overflow) d as events,node=bp,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=30 \u2026 as the events that make it to the back-pressure ( bp ) step (should be equal to f ) h as events,node=bp,port=overflow,direction=output,pipeline=tremor:///pipeline/main/01 count=5 \u2026 as the events that the overflowed from the back-pressure step (aka the offramp asked is to back off a bit) - we assume 5 events fell into the back-pressure period. i as events,node=bp,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=25 \u2026 as the events that make it out of the back pressure step (we assume 25 here since in h we had 5 events overflow) j as events,node=batch,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=25 \u2026 as the events that make it to the batch step (should be equal to i ) k as events,node=batch,port=out,direction=output,pipeline=tremor:///pipeline/main/01 count=1 \u2026 this is a tricky one, we got 25 events into this, however we batch by 20 events, so the first 20 events that come in get send out as a batch so we have 1 as a count - it should be noted that at the time of this snapshot 5 more events are currently held by the batch step but not send out. If we had a batch size of 30 defined we would see no output events here. b as events,node=out,port=in,direction=input,pipeline=tremor:///pipeline/main/01 count=1 \u2026 the events that make it to the output to be written, since we wrote only a single batch we get a count of 1 here as in k","title":"Example"},{"location":"policies/security/","text":"Security \u00b6 The tremor project follows strict coding practices that help to reduce the incidence, surface and likelihood of direct or indirect security risks to users of the software. Specifically: Tremor favors safe over unsafe rust code. Safe code is generally considered the better option Unless, performance critical concerns on the hot path suggest otherwise Over time, unsafe code should be displaced with safe code Tremor is careful on matters of code health. Clippy is pedantic mode is mandated for all rust code Property based testing, model-based testing and fuzz-testing are used Additional audits for code quality are in force Static analysis Tremor analyses external library dependencies for all direct and indirect dependencies Tremor logs and reports all LICENSE, CVE and other violations both in our CI code and using other tools Additional dynamic and static analysis methods can be added to broaden/deepen coverage Non Recommendation \u00b6 We do not recommend running tremor outside of corporate firewalls at this time. Although every care is taken to ensure there are no security flaws within the code-base tremor, to date, has been designed with deployment in secure, well-defended environments with active intrusion detection and defenses run by security professionals. Recommendation \u00b6 We do recommend running tremor in secured environments following the best practices of the organization and deployment platform. For example, the security blueprints for deploying secure services to Amazon's infrastructure should be followed when deploying to AWS. The security blueprints for the Google Cloud Platform should be followed when deploying to GCP. Where tremor is deployed into bespoke bare metal data centers, tremor should be treated as software that is not secure in and of itself. A secured environment should be provided for it to run within. Future \u00b6 Contributions to tremor security are very welcome, highly encouraged and we would be delighted to accept contributions that move our security roadmap priority. Disclosing Security Issues \u00b6 Safety is a core principle of Tremor, and one of the prime reasons why we adopted the Rust Programming language to write Tremor. To that end, we would like to ensure that Tremor has a secure implementation with no inherent security issues. Thank you for taking the time to responsibly disclose any issues you find. All security bugs in the tremor project should be reported by email to opensource@wayfair.com . This list is delivered to a small team. Your email will be acknowledged within 24 hours, and you\u2019ll receive a more detailed response to your email within 48 hours indicating the next steps in handling your report. If you would like, you can encrypt your report using a public key. This key can be requested from the security team via direct email, or through contacting us on our slack community and requesting same. Be sure to use a descriptive subject line in email to avoid having your report missed. After the initial reply to your report, the security team will endeavor to keep you informed of the progress being made towards a fix and full announcement. Public keys for disclosures \u00b6 If you have not received a reply to your email within 48 hours, or have not heard from the security team for the past five days, there are a few steps you can take (in order): Contact the current security coordinator (Darach Ennis) directly PGP key . Contact the back-up contact (Heinz Gies) directly PGP key . Post a friendly reminder on the community slack. Please note that discussion forums are public areas. When escalating in these venues, please do not discuss your issue. Say that you\u2019re trying to get a hold of someone from the security team.","title":"Security"},{"location":"policies/security/#security","text":"The tremor project follows strict coding practices that help to reduce the incidence, surface and likelihood of direct or indirect security risks to users of the software. Specifically: Tremor favors safe over unsafe rust code. Safe code is generally considered the better option Unless, performance critical concerns on the hot path suggest otherwise Over time, unsafe code should be displaced with safe code Tremor is careful on matters of code health. Clippy is pedantic mode is mandated for all rust code Property based testing, model-based testing and fuzz-testing are used Additional audits for code quality are in force Static analysis Tremor analyses external library dependencies for all direct and indirect dependencies Tremor logs and reports all LICENSE, CVE and other violations both in our CI code and using other tools Additional dynamic and static analysis methods can be added to broaden/deepen coverage","title":"Security"},{"location":"policies/security/#non-recommendation","text":"We do not recommend running tremor outside of corporate firewalls at this time. Although every care is taken to ensure there are no security flaws within the code-base tremor, to date, has been designed with deployment in secure, well-defended environments with active intrusion detection and defenses run by security professionals.","title":"Non Recommendation"},{"location":"policies/security/#recommendation","text":"We do recommend running tremor in secured environments following the best practices of the organization and deployment platform. For example, the security blueprints for deploying secure services to Amazon's infrastructure should be followed when deploying to AWS. The security blueprints for the Google Cloud Platform should be followed when deploying to GCP. Where tremor is deployed into bespoke bare metal data centers, tremor should be treated as software that is not secure in and of itself. A secured environment should be provided for it to run within.","title":"Recommendation"},{"location":"policies/security/#future","text":"Contributions to tremor security are very welcome, highly encouraged and we would be delighted to accept contributions that move our security roadmap priority.","title":"Future"},{"location":"policies/security/#disclosing-security-issues","text":"Safety is a core principle of Tremor, and one of the prime reasons why we adopted the Rust Programming language to write Tremor. To that end, we would like to ensure that Tremor has a secure implementation with no inherent security issues. Thank you for taking the time to responsibly disclose any issues you find. All security bugs in the tremor project should be reported by email to opensource@wayfair.com . This list is delivered to a small team. Your email will be acknowledged within 24 hours, and you\u2019ll receive a more detailed response to your email within 48 hours indicating the next steps in handling your report. If you would like, you can encrypt your report using a public key. This key can be requested from the security team via direct email, or through contacting us on our slack community and requesting same. Be sure to use a descriptive subject line in email to avoid having your report missed. After the initial reply to your report, the security team will endeavor to keep you informed of the progress being made towards a fix and full announcement.","title":"Disclosing Security Issues"},{"location":"policies/security/#public-keys-for-disclosures","text":"If you have not received a reply to your email within 48 hours, or have not heard from the security team for the past five days, there are a few steps you can take (in order): Contact the current security coordinator (Darach Ennis) directly PGP key . Contact the back-up contact (Heinz Gies) directly PGP key . Post a friendly reminder on the community slack. Please note that discussion forums are public areas. When escalating in these venues, please do not discuss your issue. Say that you\u2019re trying to get a hold of someone from the security team.","title":"Public keys for disclosures"},{"location":"tremor-query/","text":"Tremor-Query \u00b6 The tremor query language, tremor-query or trickle is an interpreted statement-oriented language designed for continuous online structured queries with support for filtering, extraction, transformation and streaming of structured data in a stream or event-based processing system. At its core, tremor-query supports the definition of windows operators, stream definitions and operations on those streams such as select , operator and script . Structured queries in trickle consume unstructured data that are in and of themselves at least well-formed ( eg: such as JSON ) and produce synthetic events that are also well-formed and in and of themselves unstructured. The language does not impose schema based constraints on data flowing through the system, although this may produce runtime errors for badly written queries. The query language interpreter constructs a directed-acyclic-graph or DAG by analysing the dependencies of operators in a user defined query. The input and output streams are then calculated. The DAG model is the same as the pipeline model in previous versions of tremor, but with a rich query language replacing the deprecated tremor yaml format for pipelines. Principles \u00b6 Safety \u00b6 The language is explicitly not Turing-complete: there are no unstructured goto grammar forms there are no unbounded for , while or do..while looping constructs the language is built on top of Rust , inheriting its robustness and safety features, without the development overheads Developer friendly \u00b6 The language adopts a SQL-like syntax for key statement forms and has a path-like syntax for indexing into records and arrays. The statement-oriented query language prefers nested data-structures to tables and columns from traditional ANSI-ISO SQL as most of the data processed through tremor is structured and JSON-like. The expression language in trickle is based on tremor-script . Stream-oriented / event-based \u00b6 Tremor-query is designed to process unstructured ( but well-formed ) data events. Event data can be JSON, MsgPack or any other form supported by the tremor event processing system. Self-documenting \u00b6 The SQL-like syntax enables a natural and familiar style whilst allowing the resulting statements to be compiled into a formal DAG for compile-time checks and to ensure correctness. Leveraging the tremor-query expression syntax allows rich filtering, transformation, patching, merging and object/array comprehensions to be programmed. The addition of aggregate functions, and windowing allows batches or a slice in time of events to be summarised or processed together to derive useful synthetic events. Like its sibling language tremor-script , tremor-query supports the same data-types and is entirely event-driven. It has many parallels for existing tremor users to leverage while learning, yet it is powerful and flexible in its own right. Extensibility \u00b6 The SQL-based nature of tremor-query means that complex branching, combining or powerful constructs such as in-memory tables can be added following a familiar SQL-like syntax. The expression-based expression language derived from tremor-script allows computational forms to be extended. The language core is designed for reuse - currently the expression language is reused in the query language, as are the library of functions available to both. The addition of aggregate functions is currently exclusive to tremor-query as these are only relevant when processing multiple in-flight ( or cached ) events at the same time. In the future, tremor-query may be retargeted as a JIT-compiled language and other domain specific languages may be integrated as the tremor runtime evolves to meet new uses, demands, and stake-holders. Performant \u00b6 Data ingested into tremor-query is vectorized via SIMD-parallel instructions on x86-64 or other Intel processor architectures supporting ssev3/avx extensions. Processing streams of such event-data incurs some allocation overhead at this time, but these event-bound allocations are being written out of the interpreter. The current meaning of performant as documented here means that tremor-query is more efficient at processing metrics-like data than the system it replaces ( telegraf / kapacitor / influx ) which do not support rich proactive and reactive back-pressure mechanisms or efficient asynchronous event-based data distribution between system components. Productive \u00b6 The tremor-query parsing tool-chain has been designed with ease-of-debugging and ease-of-development in mind. It has builtin support for syntax-highlighting on the console with errors annotating highlighted sections of badly written scripts to simplify fixing such scripts. The tool-chain inherits most of its core capabilities from the tremor-script and pipeline components of the tremor runtime. These components are already in large-scale production use and battle-hardened; this, in turn, minimises any associated risks with introducing a query language, whilst offering a migration path away from the far less expressive and far less easy-to-use yaml-based pipeline configuration which is more error-prone and verbose. Language \u00b6 This section details the major components of the tremor-query language. Tremor-Script \u00b6 Comments, Literals, Paths and Expression forms supported in trickle are the same as in tremor-script . Queries \u00b6 Queries are one or many statements separated by ; . Queries are compiled into a DAG of operator nodes and validated at compile time. At runtime, the resulting executable tremor pipeline is evaluated/interpreted. Grammar \u00b6 Stmt: See also Additional Grammar Rules . Statements \u00b6 Statements can be one of: Stream definitions Window definitions Custom Operator definitions Embedded tremor-script definitions Or builtin operations, like the select statement Stream definitions \u00b6 Stream definitions in tremor-query allow private intermediate streams to be named so that they can be used as the source or sinks in other continuous queries. Grammar \u00b6 Example \u00b6 create stream passthrough ; select event from in into passthrough ; # select default public 'in' stream into passthrough select event from passthrough into out ; # select passthrough into default public 'out' stream Window definitions \u00b6 Window definitions in tremor-query can be either tumbling or sliding. A tumbling window is a window configured with a fixed non-overlapping interval of time. The window aggregates events once opened, and continues aggregating until it closes. The window can emit synthetic events upon closing. The window reopens for its next cycle when it closes. Support for sliding windows has not been implemented yet (it has an open RFC and it will be picked up for a future release). Tumbling Windows \u00b6 Tremor supports tumbling windows by number of events or by time. General configuration Parameters: eviction_period : duration in nanoseconds without events arriving, after which to evict / remove the current window data for a single group. max_groups : maximum number of groups to maintain simultaneously in memory. Groups added beyond that number will be ignored. Per default, tremor does not impose any limit on the number of groups kept simultaneously. Each select statement maintains the groups for the current windows in an in memory data-structure. This contains the group values as well as the aggregate states. If your grouping values possibly have a very high cardinality it is possible to end up with runaway memory growth, as per default the group data structures won't be evicted, unless eviction_period is set. Old groups will be discarded after 2 x eviction_period if no event for those groups arrived. To configure an upper bound on the number of groups that should be maintained simultaneously for a window, set max_groups . This will help avoid unbounded memory growth, especially when using emit_empty_windows on time based windows. Windows based on number of events \u00b6 Size based tumbling windows close when a certain number of aggregated events has been reached. Configuration Parameters: size : Number of events until this window closes and emits a downstream event. The size increment for each event defaults to 1 but can be customized by the embedded script in the window definition. This script needs to return an unsigned integer denoting the number of events to use for this event. It is possible to ignore the current event by emitting 0 . Windows based on time \u00b6 Time based tumbling windows close when a certain duration has elapsed. The source for measuring the duration is the ingest timestamp of the events flowing through by default. The provided embedded script can be used to customize the source of time measurement. The embedded script must return a number representing a timestamp in nanoseconds. This way windows using timestamps other than the event ingest time can be built. Only windows using the event ingest timestamp can be closed when the time in interval is elapsed measured by wall-clock time independent from event flow with a granularity of 100ms . It is thus possible that empty windows are emitted. Windows using scripts to determine the window elapsed time are considered to deviate from wall clock time and will only close and emit when events flow through them. Configuration Parameters: interval : Time interval in nanoseconds after which the window closes. emit_empty_windows - By default, time based windows will only emit, if events arrived. By configuring emit_empty_windows as true this window will emit every interval , regardless if events arrived or not. Warning If you use a window with emit_empty_windows in a group by query and the cardinality is likely huge, consider using max_groups and eviction_period to avoid runaway memory growth such a window will one event per interval and group for which we've seen events before. Grammar \u00b6 See also Additional Grammar Rules . Examples \u00b6 For example a 15 second tumbling window based on the event ingest timestamp can be defined as follows define tumbling window fifteen_secs with interval = core :: datetime :: with_seconds ( 15 ), end ; The same window can be defined using a timestamp that is extracted from the message instead of the ingest time: define tumbling window fifteen_secs with interval = core :: datetime :: with_seconds ( 15 ), script event . timestamp end ; A tumbling window based on number of events that will discard windows when 2 hours have been passed: define tumbling window with_size with size = 1000 , eviction_period = core :: datetime :: with_hours ( 2 ) end ; Custom Operator definitions \u00b6 Custom operators allow definition, configuration and usage of legacy operators, that have been around before tremor supported the query language. As the query language and deprecated yaml format share the same DAG model and pipeline formats, they are interoperable at runtime and are backwards compatible: Grammar \u00b6 Operator Definition: Operator Creation: See also Additional Grammar Rules . Example \u00b6 # create a bucketing operator define grouper :: bucket operator kfc ; create operator kfc ; # ... select event from categorize into kfc ; select event from kfc into out ; Embedded script definitions \u00b6 The tremor-script language can be embedded in the query language natively and this mirrors legacy usage (before v0.9) where it was embedded within yaml-based pipeline configuration. However, the tooling that ships with tremor-query understands both the query language and scripting language dialects with better syntax highlighting and error checking built in, for ease of operator productivity over the deprecated yaml syntax. Grammar \u00b6 Script Definition Grammar: Script Creation Grammar: See also Additional Grammar Rules . Example \u00b6 define grouper :: bucket operator kfc ; define script categorize script let $ rate = 1 ; let $ class = event .` group `; { \"event\" : event , \"rate\" : $ rate , \"class\" : $ class } ; end ; create script categorize ; # Stream ingested data into categorize script select event from in into categorize ; create operator kfc ; # Stream scripted events into kfc bucket operator select event from categorize into kfc ; # Stream bucketed events into out stream select event from kfc into out ; Select queries \u00b6 The select query is a builtin operation that is the workhorse of the tremor-query language. An example select operation configured to pass through data from a pipeline's default in stream to a pipeline's default out stream: select event from in into out ; Select operations can filter ingested data with the specification of a where clause . The clause forms a predicate check on the inbound events before any further processing takes place. That means the event available to the where clause is the unprocessed inbound event from the input stream ( in in this case): select event from in where event . is_interesting into out ; Select operations can filter data being forwarded to other operators with the specification of a having clause . The clause forms a predicate check on outbound synthetic events after any other processing has taken place. That means the event available to the having clause is the result of evaluating the select target clause (the expression between select and from ). select event from in into out having event . is_interesting ; Select operations can be windowed by applying a window to the inbound data stream. define tumbling window fifteen_secs with interval = datetime :: with_seconds ( 15 ), end ; select { \"count\" : aggr :: stats :: count () } from in [ fifteen_secs ] into out having event . count > 0 ; In the above operation, we emit a synthetic count every fifteen seconds if at least one event has been witnessed during a 15 second window of time. Select operations can be grouped through defining a group by clause . define tumbling window fifteen_secs with interval = datetime :: with_seconds ( 15 ), end ; select { \"count\" : aggr :: stats :: count () } from in [ fifteen_secs ] group by set ( event . partition ) into out having event . count > 0 ; In the above operation, we partition the ingested events into groups defined by a required event.partition data field on the inbound event. Each of these groups maintains an independent fifteen second tumbling window, and each window upon closing gates outbound synthetic events by a count for that group. The current implementation of select allows set-based and each-based grouping. These can be composed concatenatively. However cube and rollup based grouping dimensions are not currently supported. In windowed queries any event related data can only be referenced in those two cases: it is used as an argument to an aggregate function it is used as expression in the group by clause Here is an example of valid and invalid references: define tumbling window my_window with size = 12 end ; select { \"last\" : aggr :: win :: last ( event . other ), # ok, inside aggregate function \"foo\" : event . foo + 1 , # ok, used inside aggregate function \"bad\" : event . other , # NOT OK \"bad_meta\" : $ my_meta , # NOT OK, same rules apply to event metadata } from in [ my_window ] group by set ( event . foo , event . bar ) into out ; Grammar \u00b6 Select Grammar: FromClause: WhereClause: GroupByClause: GroupByDimension: SetBasedGroup: EachBasedGroup: IntoClause: HavingClause: Additional Grammar Rules \u00b6 These rules are referenced in the main tremor-query grammar rules above and are listed here as extended reference. EmbeddedScript: WithParams: WithPartialParams: Params: Param: ModularId: Id: TiltFrames: WindowKind:","title":"Overview"},{"location":"tremor-query/#tremor-query","text":"The tremor query language, tremor-query or trickle is an interpreted statement-oriented language designed for continuous online structured queries with support for filtering, extraction, transformation and streaming of structured data in a stream or event-based processing system. At its core, tremor-query supports the definition of windows operators, stream definitions and operations on those streams such as select , operator and script . Structured queries in trickle consume unstructured data that are in and of themselves at least well-formed ( eg: such as JSON ) and produce synthetic events that are also well-formed and in and of themselves unstructured. The language does not impose schema based constraints on data flowing through the system, although this may produce runtime errors for badly written queries. The query language interpreter constructs a directed-acyclic-graph or DAG by analysing the dependencies of operators in a user defined query. The input and output streams are then calculated. The DAG model is the same as the pipeline model in previous versions of tremor, but with a rich query language replacing the deprecated tremor yaml format for pipelines.","title":"Tremor-Query"},{"location":"tremor-query/#principles","text":"","title":"Principles"},{"location":"tremor-query/#safety","text":"The language is explicitly not Turing-complete: there are no unstructured goto grammar forms there are no unbounded for , while or do..while looping constructs the language is built on top of Rust , inheriting its robustness and safety features, without the development overheads","title":"Safety"},{"location":"tremor-query/#developer-friendly","text":"The language adopts a SQL-like syntax for key statement forms and has a path-like syntax for indexing into records and arrays. The statement-oriented query language prefers nested data-structures to tables and columns from traditional ANSI-ISO SQL as most of the data processed through tremor is structured and JSON-like. The expression language in trickle is based on tremor-script .","title":"Developer friendly"},{"location":"tremor-query/#stream-oriented-event-based","text":"Tremor-query is designed to process unstructured ( but well-formed ) data events. Event data can be JSON, MsgPack or any other form supported by the tremor event processing system.","title":"Stream-oriented / event-based"},{"location":"tremor-query/#self-documenting","text":"The SQL-like syntax enables a natural and familiar style whilst allowing the resulting statements to be compiled into a formal DAG for compile-time checks and to ensure correctness. Leveraging the tremor-query expression syntax allows rich filtering, transformation, patching, merging and object/array comprehensions to be programmed. The addition of aggregate functions, and windowing allows batches or a slice in time of events to be summarised or processed together to derive useful synthetic events. Like its sibling language tremor-script , tremor-query supports the same data-types and is entirely event-driven. It has many parallels for existing tremor users to leverage while learning, yet it is powerful and flexible in its own right.","title":"Self-documenting"},{"location":"tremor-query/#extensibility","text":"The SQL-based nature of tremor-query means that complex branching, combining or powerful constructs such as in-memory tables can be added following a familiar SQL-like syntax. The expression-based expression language derived from tremor-script allows computational forms to be extended. The language core is designed for reuse - currently the expression language is reused in the query language, as are the library of functions available to both. The addition of aggregate functions is currently exclusive to tremor-query as these are only relevant when processing multiple in-flight ( or cached ) events at the same time. In the future, tremor-query may be retargeted as a JIT-compiled language and other domain specific languages may be integrated as the tremor runtime evolves to meet new uses, demands, and stake-holders.","title":"Extensibility"},{"location":"tremor-query/#performant","text":"Data ingested into tremor-query is vectorized via SIMD-parallel instructions on x86-64 or other Intel processor architectures supporting ssev3/avx extensions. Processing streams of such event-data incurs some allocation overhead at this time, but these event-bound allocations are being written out of the interpreter. The current meaning of performant as documented here means that tremor-query is more efficient at processing metrics-like data than the system it replaces ( telegraf / kapacitor / influx ) which do not support rich proactive and reactive back-pressure mechanisms or efficient asynchronous event-based data distribution between system components.","title":"Performant"},{"location":"tremor-query/#productive","text":"The tremor-query parsing tool-chain has been designed with ease-of-debugging and ease-of-development in mind. It has builtin support for syntax-highlighting on the console with errors annotating highlighted sections of badly written scripts to simplify fixing such scripts. The tool-chain inherits most of its core capabilities from the tremor-script and pipeline components of the tremor runtime. These components are already in large-scale production use and battle-hardened; this, in turn, minimises any associated risks with introducing a query language, whilst offering a migration path away from the far less expressive and far less easy-to-use yaml-based pipeline configuration which is more error-prone and verbose.","title":"Productive"},{"location":"tremor-query/#language","text":"This section details the major components of the tremor-query language.","title":"Language"},{"location":"tremor-query/#tremor-script","text":"Comments, Literals, Paths and Expression forms supported in trickle are the same as in tremor-script .","title":"Tremor-Script"},{"location":"tremor-query/#queries","text":"Queries are one or many statements separated by ; . Queries are compiled into a DAG of operator nodes and validated at compile time. At runtime, the resulting executable tremor pipeline is evaluated/interpreted.","title":"Queries"},{"location":"tremor-query/#grammar","text":"Stmt: See also Additional Grammar Rules .","title":"Grammar"},{"location":"tremor-query/#statements","text":"Statements can be one of: Stream definitions Window definitions Custom Operator definitions Embedded tremor-script definitions Or builtin operations, like the select statement","title":"Statements"},{"location":"tremor-query/#stream-definitions","text":"Stream definitions in tremor-query allow private intermediate streams to be named so that they can be used as the source or sinks in other continuous queries.","title":"Stream definitions"},{"location":"tremor-query/#grammar_1","text":"","title":"Grammar"},{"location":"tremor-query/#example","text":"create stream passthrough ; select event from in into passthrough ; # select default public 'in' stream into passthrough select event from passthrough into out ; # select passthrough into default public 'out' stream","title":"Example"},{"location":"tremor-query/#window-definitions","text":"Window definitions in tremor-query can be either tumbling or sliding. A tumbling window is a window configured with a fixed non-overlapping interval of time. The window aggregates events once opened, and continues aggregating until it closes. The window can emit synthetic events upon closing. The window reopens for its next cycle when it closes. Support for sliding windows has not been implemented yet (it has an open RFC and it will be picked up for a future release).","title":"Window definitions"},{"location":"tremor-query/#tumbling-windows","text":"Tremor supports tumbling windows by number of events or by time. General configuration Parameters: eviction_period : duration in nanoseconds without events arriving, after which to evict / remove the current window data for a single group. max_groups : maximum number of groups to maintain simultaneously in memory. Groups added beyond that number will be ignored. Per default, tremor does not impose any limit on the number of groups kept simultaneously. Each select statement maintains the groups for the current windows in an in memory data-structure. This contains the group values as well as the aggregate states. If your grouping values possibly have a very high cardinality it is possible to end up with runaway memory growth, as per default the group data structures won't be evicted, unless eviction_period is set. Old groups will be discarded after 2 x eviction_period if no event for those groups arrived. To configure an upper bound on the number of groups that should be maintained simultaneously for a window, set max_groups . This will help avoid unbounded memory growth, especially when using emit_empty_windows on time based windows.","title":"Tumbling Windows"},{"location":"tremor-query/#windows-based-on-number-of-events","text":"Size based tumbling windows close when a certain number of aggregated events has been reached. Configuration Parameters: size : Number of events until this window closes and emits a downstream event. The size increment for each event defaults to 1 but can be customized by the embedded script in the window definition. This script needs to return an unsigned integer denoting the number of events to use for this event. It is possible to ignore the current event by emitting 0 .","title":"Windows based on number of events"},{"location":"tremor-query/#windows-based-on-time","text":"Time based tumbling windows close when a certain duration has elapsed. The source for measuring the duration is the ingest timestamp of the events flowing through by default. The provided embedded script can be used to customize the source of time measurement. The embedded script must return a number representing a timestamp in nanoseconds. This way windows using timestamps other than the event ingest time can be built. Only windows using the event ingest timestamp can be closed when the time in interval is elapsed measured by wall-clock time independent from event flow with a granularity of 100ms . It is thus possible that empty windows are emitted. Windows using scripts to determine the window elapsed time are considered to deviate from wall clock time and will only close and emit when events flow through them. Configuration Parameters: interval : Time interval in nanoseconds after which the window closes. emit_empty_windows - By default, time based windows will only emit, if events arrived. By configuring emit_empty_windows as true this window will emit every interval , regardless if events arrived or not. Warning If you use a window with emit_empty_windows in a group by query and the cardinality is likely huge, consider using max_groups and eviction_period to avoid runaway memory growth such a window will one event per interval and group for which we've seen events before.","title":"Windows based on time"},{"location":"tremor-query/#grammar_2","text":"See also Additional Grammar Rules .","title":"Grammar"},{"location":"tremor-query/#examples","text":"For example a 15 second tumbling window based on the event ingest timestamp can be defined as follows define tumbling window fifteen_secs with interval = core :: datetime :: with_seconds ( 15 ), end ; The same window can be defined using a timestamp that is extracted from the message instead of the ingest time: define tumbling window fifteen_secs with interval = core :: datetime :: with_seconds ( 15 ), script event . timestamp end ; A tumbling window based on number of events that will discard windows when 2 hours have been passed: define tumbling window with_size with size = 1000 , eviction_period = core :: datetime :: with_hours ( 2 ) end ;","title":"Examples"},{"location":"tremor-query/#custom-operator-definitions","text":"Custom operators allow definition, configuration and usage of legacy operators, that have been around before tremor supported the query language. As the query language and deprecated yaml format share the same DAG model and pipeline formats, they are interoperable at runtime and are backwards compatible:","title":"Custom Operator definitions"},{"location":"tremor-query/#grammar_3","text":"Operator Definition: Operator Creation: See also Additional Grammar Rules .","title":"Grammar"},{"location":"tremor-query/#example_1","text":"# create a bucketing operator define grouper :: bucket operator kfc ; create operator kfc ; # ... select event from categorize into kfc ; select event from kfc into out ;","title":"Example"},{"location":"tremor-query/#embedded-script-definitions","text":"The tremor-script language can be embedded in the query language natively and this mirrors legacy usage (before v0.9) where it was embedded within yaml-based pipeline configuration. However, the tooling that ships with tremor-query understands both the query language and scripting language dialects with better syntax highlighting and error checking built in, for ease of operator productivity over the deprecated yaml syntax.","title":"Embedded script definitions"},{"location":"tremor-query/#grammar_4","text":"Script Definition Grammar: Script Creation Grammar: See also Additional Grammar Rules .","title":"Grammar"},{"location":"tremor-query/#example_2","text":"define grouper :: bucket operator kfc ; define script categorize script let $ rate = 1 ; let $ class = event .` group `; { \"event\" : event , \"rate\" : $ rate , \"class\" : $ class } ; end ; create script categorize ; # Stream ingested data into categorize script select event from in into categorize ; create operator kfc ; # Stream scripted events into kfc bucket operator select event from categorize into kfc ; # Stream bucketed events into out stream select event from kfc into out ;","title":"Example"},{"location":"tremor-query/#select-queries","text":"The select query is a builtin operation that is the workhorse of the tremor-query language. An example select operation configured to pass through data from a pipeline's default in stream to a pipeline's default out stream: select event from in into out ; Select operations can filter ingested data with the specification of a where clause . The clause forms a predicate check on the inbound events before any further processing takes place. That means the event available to the where clause is the unprocessed inbound event from the input stream ( in in this case): select event from in where event . is_interesting into out ; Select operations can filter data being forwarded to other operators with the specification of a having clause . The clause forms a predicate check on outbound synthetic events after any other processing has taken place. That means the event available to the having clause is the result of evaluating the select target clause (the expression between select and from ). select event from in into out having event . is_interesting ; Select operations can be windowed by applying a window to the inbound data stream. define tumbling window fifteen_secs with interval = datetime :: with_seconds ( 15 ), end ; select { \"count\" : aggr :: stats :: count () } from in [ fifteen_secs ] into out having event . count > 0 ; In the above operation, we emit a synthetic count every fifteen seconds if at least one event has been witnessed during a 15 second window of time. Select operations can be grouped through defining a group by clause . define tumbling window fifteen_secs with interval = datetime :: with_seconds ( 15 ), end ; select { \"count\" : aggr :: stats :: count () } from in [ fifteen_secs ] group by set ( event . partition ) into out having event . count > 0 ; In the above operation, we partition the ingested events into groups defined by a required event.partition data field on the inbound event. Each of these groups maintains an independent fifteen second tumbling window, and each window upon closing gates outbound synthetic events by a count for that group. The current implementation of select allows set-based and each-based grouping. These can be composed concatenatively. However cube and rollup based grouping dimensions are not currently supported. In windowed queries any event related data can only be referenced in those two cases: it is used as an argument to an aggregate function it is used as expression in the group by clause Here is an example of valid and invalid references: define tumbling window my_window with size = 12 end ; select { \"last\" : aggr :: win :: last ( event . other ), # ok, inside aggregate function \"foo\" : event . foo + 1 , # ok, used inside aggregate function \"bad\" : event . other , # NOT OK \"bad_meta\" : $ my_meta , # NOT OK, same rules apply to event metadata } from in [ my_window ] group by set ( event . foo , event . bar ) into out ;","title":"Select queries"},{"location":"tremor-query/#grammar_5","text":"Select Grammar: FromClause: WhereClause: GroupByClause: GroupByDimension: SetBasedGroup: EachBasedGroup: IntoClause: HavingClause:","title":"Grammar"},{"location":"tremor-query/#additional-grammar-rules","text":"These rules are referenced in the main tremor-query grammar rules above and are listed here as extended reference. EmbeddedScript: WithParams: WithPartialParams: Params: Param: ModularId: Id: TiltFrames: WindowKind:","title":"Additional Grammar Rules"},{"location":"tremor-query/functions/","text":"Aggregate Functions \u00b6 Tremor query provides access to a growing number of aggregate functions that allow advanced data manipulation over sets of events delineated by windowed operations. Functions are namespaced to make identification easier.","title":"Overview"},{"location":"tremor-query/functions/#aggregate-functions","text":"Tremor query provides access to a growing number of aggregate functions that allow advanced data manipulation over sets of events delineated by windowed operations. Functions are namespaced to make identification easier.","title":"Aggregate Functions"},{"location":"tremor-query/modules/","text":"Modules \u00b6 Tremor-query supports nested namespaces or modules. Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file, nesting is via the mod clause. Modules can use trickle definitions via the define clause variants to define windows, operators or scripts for reuse, or nested mod sub-modules. Module Path \u00b6 Modules can be defined physically on the file system. For example given the following modular hierarchy on the file system, relative to a root module path: Nested modules can be defined as follows: +-- foo +-- bar +-- snot.trickle +-- baz +-- badger.trickle Assuming this module hierarchy is rooted at /opt/my-project/lib they can be registered with tremor by setting the TREMOR_PATH environment variable export TREMOR_PATH = /opt/my-project/lib The TREMOR_PATH uses ':' on linux/unix to separate multiple module paths. The modules can be used using the use clause as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.trickle' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.trickle' select event from in [ snot :: second , badger :: minute ] # use our imported window definitions into out ; The same modular hierarchy can be defined as nested module declarations as follows: mod foo with mod bar with define tumbling window second with interval = 1000 end ; end ; mod baz with define tumbling window minute with interval = 60000 end ; end ; end ; select event from in [ snot :: second , badger :: minute ] # use our imported window definitions into out ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; select event from in [ fleek :: second ] # use our imported window definitions into out ; It is to be noted that inclusion via use will prevent circular inclusion as in file a.trickle can use b.trickle but beyond that point b.trickle can no longer use a.trickle as this would create a dependency cycle. This is a restriction of the current implementation and may or may not be relaxed in the future.","title":"Modules"},{"location":"tremor-query/modules/#modules","text":"Tremor-query supports nested namespaces or modules. Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file, nesting is via the mod clause. Modules can use trickle definitions via the define clause variants to define windows, operators or scripts for reuse, or nested mod sub-modules.","title":"Modules"},{"location":"tremor-query/modules/#module-path","text":"Modules can be defined physically on the file system. For example given the following modular hierarchy on the file system, relative to a root module path: Nested modules can be defined as follows: +-- foo +-- bar +-- snot.trickle +-- baz +-- badger.trickle Assuming this module hierarchy is rooted at /opt/my-project/lib they can be registered with tremor by setting the TREMOR_PATH environment variable export TREMOR_PATH = /opt/my-project/lib The TREMOR_PATH uses ':' on linux/unix to separate multiple module paths. The modules can be used using the use clause as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.trickle' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.trickle' select event from in [ snot :: second , badger :: minute ] # use our imported window definitions into out ; The same modular hierarchy can be defined as nested module declarations as follows: mod foo with mod bar with define tumbling window second with interval = 1000 end ; end ; mod baz with define tumbling window minute with interval = 60000 end ; end ; end ; select event from in [ snot :: second , badger :: minute ] # use our imported window definitions into out ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; select event from in [ fleek :: second ] # use our imported window definitions into out ; It is to be noted that inclusion via use will prevent circular inclusion as in file a.trickle can use b.trickle but beyond that point b.trickle can no longer use a.trickle as this would create a dependency cycle. This is a restriction of the current implementation and may or may not be relaxed in the future.","title":"Module Path"},{"location":"tremor-query/operators/","text":"Operators \u00b6 Operators are part of the pipeline configuration. Operators process events and signals in the context of a pipeline. An operator, upon receiving an event from an upstream operator or stream, MAY produce one or many events to one or many downstream directly connected operators. An operator MAY drop events which halts any further processing. Operators allow the data processing capabilities of tremor to be extended or specialized without changes to runtime behavior, concurrency, event ordering or other aspects of a running tremor system. Operators are created in the context of a pipeline and configured as part of tremor-query statements . An operator MUST have an identifier that is unique for its owning pipeline. Configuration is of the general form: define module :: operator_name operator my_custom_operator with param1 = \"foo\" , param2 = [ 1 , 2 , 3 ] end ; # create - full form create operator my_custom_operator_instance from my_custom_operator with param1 = \"bar\" , param2 = [ true , false , {} ] end ; # create - short form create operator my_custom_operator ; script \u00b6 An embedded tremor script is created with a special syntax that deviates from the operator creation. For a full reference see the section on tremor-query embedded-scripts . The tremor script runtime allows to modify events or their metadata. To learn more about Tremor Script please see the related section . Outputs : out (default output used with emit ) error - channel for runtime errors <anything else> used when emit event => \"<anything else>\" Examples : # definition define script rt script emit end ; define script add script emit + 1 end ; create script add ; select event from in into add ; select event from add into out ; grouper::bucket \u00b6 Bucket will perform a sliding window rate limiting based on event metadata. Limits are applied for every $class . In a $class each $dimensions is allowed to pass $rate messages per second. This operator does not support configuration. Metadata Variables : $class - The class of an event. (String) $rate - Allowed events per second per class/dimension (Number) (Optional) $dimensions - The dimensions of the event. (Any) (Optional) $cardinality - the maximum number of dimensions kept track of at the same time (Number, default: 1000 ) Outputs : out error - Unprocessable events for example if $class or $rate are not set. overflow - Events that exceed the rate defined for them Example : define grouper :: bucket operator group ; Metrics : The bucket operator generates additional metrics. For each class the following two statistics are generated (as an example): { \"measurement\" : \"bucketing\" , \"tags\" :{ \"action\" : \"pass\" , \"class\" : \"test\" , \"direction\" : \"output\" , \"node\" : \"bucketing\" , \"pipeline\" : \"main\" , \"port\" : \"out\" }, \"fields\" :{ \"count\" : 93 }, \"timestamp\" : 1553012903452340000 } { \"measurement\" : \"bucketing\" , \"tags\" :{ \"action\" : \"overflow\" , \"class\" : \"test\" , \"direction\" : \"output\" , \"node\" : \"bucketing\" , \"pipeline\" : \"main\" , \"port\" : \"out\" }, \"fields\" :{ \"count\" : 127 }, \"timestamp\" : 1553012903452340000 } This tells us the following, up until this measurement was published in the class test : ( pass ) Passed 93 events ( overflow ) Marked 127 events as overflow due to not fitting in the limit generic::backpressure \u00b6 This operator is deprecated please use qos::backpressure instead. qos::backpressure \u00b6 The backpressure operator is used to introduce delays based on downstream systems load. Longer backpressure steps are introduced every time the latency of a downstream system reached timeout , or an error occurs. On a successful transmission within the timeout limit, the delay is reset. Configuration options : timeout - Maximum allowed 'write' time in milliseconds. steps - Array of values to delay when a we detect backpressure. (default: [50, 100, 250, 500, 1000, 5000, 10000] ) Outputs : out overflow - Events that are not let past due to active backpressure Example : define qos :: backpressure operator bp with timeout = 100 end ; qos::percentile \u00b6 An alternative traffic shaping option to backpressure. Instead of all dropping events for a given time we drop a statistical subset with an increasing percentage of events dropped the longer we see errors / timeouts. In general step_up should always be significantly smaller then step_down to ensure we gradually reapproach the ideal state. Configuration options : timeout - Maximum allowed 'write' time in milliseconds. step_down - What additional percentile should be dropped in the case of a timeout (default 5%: 0.05 ) step_up - What percentile should be recovered in case of a good event. (default: 0.1%: 0.001 ) Outputs : out overflow - Events that are not let past due to active backpressure Example : define qos :: percentile operator as perc with timeout = 100 , step_down = 0.1 # 10% end ; qos::roundrobin \u00b6 Evenly distributes events over it's outputs. If a CB trigger event is received from an output this output is skipped until the circuit breaker is restored. If all outputs are triggered the operator itself triggers a CB event. Outputs : * (any named output is possible) Example : define qos :: roundrobin operator roundrobin with outputs = [ \"round\" , \"robin\" , \"outputs\" ] end ; qos::wal \u00b6 A Write Ahead Log that will persist data to disk and feed the following operators from this disk cache. It allows to run onramps that do not provide any support for delivery guarantees with offramps that do. The wal operator will intercept and generate it's own circuit breaker events. You can think about it as a firewall that will protect all operators before itself from issues beyond it. On the other hand it will indiscriminately consume data from sources and operators before itself until it's own circuit breaking conditions are met. At the same time will it interact with tremors guaranteed delivery system, events are only removed from disk once they're acknowledged. In case of delivery failure the WAL operator will replay the failed events. On the same way the WAL operator will acknowledge events that it persists to disk. The WAL operator should be used with caution, since every event that passes through it will be written to the hard drive it has a significant performance impact. Configuration options : read_count - Maximum number of events that are read form the WAL at one time. dir - Directory to store the WAL-file in (optional, if omitted the WAL will remain in memory and not be persisted to disk) max_elements - Maximum number of elements the WAL will cache before triggering a CB event max_bytes - Maximum space on disk the WAL should take (this is a soft limit!) Only one of max_elements or max_bytes is required, setting both is possible. Outputs : out Example : define qos :: wal operator wal with dir = \"./wal\" , read_count = 20 , max_elements = 1000 , max_bytes = 10485760 end ; generic::batch \u00b6 The batch operator is used to batch multiple events and send them in a bulk fashion. It also allows to set a timeout of how long the operator should wait for a batch to be filled. Supported configuration options are: count - Elements per batch timeout - Maximum delay between the first element of a batch and the last element of a batch. Outputs : out Example : define generic :: batch operator batch with count = 300 end ; generic::counter \u00b6 Keeps track of the number of events as they come and emits the current count out alongside the event. The output is a record of the form {\"count\": n, \"event\": event} , where n is the current count and event is the original event. The counter starts when the first event comes through and begins from 1. Outputs : out Example : define generic :: counter operator counter ; debug::history \u00b6 Note This operator is for debugging purposes only, and should not be used in production deployments. This operator generates a history entry in the event metadata underneath the field provided in the name config value. Data is pushed to the array as a Striong in the form: \"event: <op>(<event_id>)\" . This can be used as a tracepoint of events in a complex pipeline setup. Configuration options : op - The operation name of this operator name - The field to store the history on Outputs : out Example : define debug :: history operator history with op = \"my-checkpoint\" , name = \"event_history\" end ;","title":"Operators"},{"location":"tremor-query/operators/#operators","text":"Operators are part of the pipeline configuration. Operators process events and signals in the context of a pipeline. An operator, upon receiving an event from an upstream operator or stream, MAY produce one or many events to one or many downstream directly connected operators. An operator MAY drop events which halts any further processing. Operators allow the data processing capabilities of tremor to be extended or specialized without changes to runtime behavior, concurrency, event ordering or other aspects of a running tremor system. Operators are created in the context of a pipeline and configured as part of tremor-query statements . An operator MUST have an identifier that is unique for its owning pipeline. Configuration is of the general form: define module :: operator_name operator my_custom_operator with param1 = \"foo\" , param2 = [ 1 , 2 , 3 ] end ; # create - full form create operator my_custom_operator_instance from my_custom_operator with param1 = \"bar\" , param2 = [ true , false , {} ] end ; # create - short form create operator my_custom_operator ;","title":"Operators"},{"location":"tremor-query/operators/#script","text":"An embedded tremor script is created with a special syntax that deviates from the operator creation. For a full reference see the section on tremor-query embedded-scripts . The tremor script runtime allows to modify events or their metadata. To learn more about Tremor Script please see the related section . Outputs : out (default output used with emit ) error - channel for runtime errors <anything else> used when emit event => \"<anything else>\" Examples : # definition define script rt script emit end ; define script add script emit + 1 end ; create script add ; select event from in into add ; select event from add into out ;","title":"script"},{"location":"tremor-query/operators/#grouperbucket","text":"Bucket will perform a sliding window rate limiting based on event metadata. Limits are applied for every $class . In a $class each $dimensions is allowed to pass $rate messages per second. This operator does not support configuration. Metadata Variables : $class - The class of an event. (String) $rate - Allowed events per second per class/dimension (Number) (Optional) $dimensions - The dimensions of the event. (Any) (Optional) $cardinality - the maximum number of dimensions kept track of at the same time (Number, default: 1000 ) Outputs : out error - Unprocessable events for example if $class or $rate are not set. overflow - Events that exceed the rate defined for them Example : define grouper :: bucket operator group ; Metrics : The bucket operator generates additional metrics. For each class the following two statistics are generated (as an example): { \"measurement\" : \"bucketing\" , \"tags\" :{ \"action\" : \"pass\" , \"class\" : \"test\" , \"direction\" : \"output\" , \"node\" : \"bucketing\" , \"pipeline\" : \"main\" , \"port\" : \"out\" }, \"fields\" :{ \"count\" : 93 }, \"timestamp\" : 1553012903452340000 } { \"measurement\" : \"bucketing\" , \"tags\" :{ \"action\" : \"overflow\" , \"class\" : \"test\" , \"direction\" : \"output\" , \"node\" : \"bucketing\" , \"pipeline\" : \"main\" , \"port\" : \"out\" }, \"fields\" :{ \"count\" : 127 }, \"timestamp\" : 1553012903452340000 } This tells us the following, up until this measurement was published in the class test : ( pass ) Passed 93 events ( overflow ) Marked 127 events as overflow due to not fitting in the limit","title":"grouper::bucket"},{"location":"tremor-query/operators/#genericbackpressure","text":"This operator is deprecated please use qos::backpressure instead.","title":"generic::backpressure"},{"location":"tremor-query/operators/#qosbackpressure","text":"The backpressure operator is used to introduce delays based on downstream systems load. Longer backpressure steps are introduced every time the latency of a downstream system reached timeout , or an error occurs. On a successful transmission within the timeout limit, the delay is reset. Configuration options : timeout - Maximum allowed 'write' time in milliseconds. steps - Array of values to delay when a we detect backpressure. (default: [50, 100, 250, 500, 1000, 5000, 10000] ) Outputs : out overflow - Events that are not let past due to active backpressure Example : define qos :: backpressure operator bp with timeout = 100 end ;","title":"qos::backpressure"},{"location":"tremor-query/operators/#qospercentile","text":"An alternative traffic shaping option to backpressure. Instead of all dropping events for a given time we drop a statistical subset with an increasing percentage of events dropped the longer we see errors / timeouts. In general step_up should always be significantly smaller then step_down to ensure we gradually reapproach the ideal state. Configuration options : timeout - Maximum allowed 'write' time in milliseconds. step_down - What additional percentile should be dropped in the case of a timeout (default 5%: 0.05 ) step_up - What percentile should be recovered in case of a good event. (default: 0.1%: 0.001 ) Outputs : out overflow - Events that are not let past due to active backpressure Example : define qos :: percentile operator as perc with timeout = 100 , step_down = 0.1 # 10% end ;","title":"qos::percentile"},{"location":"tremor-query/operators/#qosroundrobin","text":"Evenly distributes events over it's outputs. If a CB trigger event is received from an output this output is skipped until the circuit breaker is restored. If all outputs are triggered the operator itself triggers a CB event. Outputs : * (any named output is possible) Example : define qos :: roundrobin operator roundrobin with outputs = [ \"round\" , \"robin\" , \"outputs\" ] end ;","title":"qos::roundrobin"},{"location":"tremor-query/operators/#qoswal","text":"A Write Ahead Log that will persist data to disk and feed the following operators from this disk cache. It allows to run onramps that do not provide any support for delivery guarantees with offramps that do. The wal operator will intercept and generate it's own circuit breaker events. You can think about it as a firewall that will protect all operators before itself from issues beyond it. On the other hand it will indiscriminately consume data from sources and operators before itself until it's own circuit breaking conditions are met. At the same time will it interact with tremors guaranteed delivery system, events are only removed from disk once they're acknowledged. In case of delivery failure the WAL operator will replay the failed events. On the same way the WAL operator will acknowledge events that it persists to disk. The WAL operator should be used with caution, since every event that passes through it will be written to the hard drive it has a significant performance impact. Configuration options : read_count - Maximum number of events that are read form the WAL at one time. dir - Directory to store the WAL-file in (optional, if omitted the WAL will remain in memory and not be persisted to disk) max_elements - Maximum number of elements the WAL will cache before triggering a CB event max_bytes - Maximum space on disk the WAL should take (this is a soft limit!) Only one of max_elements or max_bytes is required, setting both is possible. Outputs : out Example : define qos :: wal operator wal with dir = \"./wal\" , read_count = 20 , max_elements = 1000 , max_bytes = 10485760 end ;","title":"qos::wal"},{"location":"tremor-query/operators/#genericbatch","text":"The batch operator is used to batch multiple events and send them in a bulk fashion. It also allows to set a timeout of how long the operator should wait for a batch to be filled. Supported configuration options are: count - Elements per batch timeout - Maximum delay between the first element of a batch and the last element of a batch. Outputs : out Example : define generic :: batch operator batch with count = 300 end ;","title":"generic::batch"},{"location":"tremor-query/operators/#genericcounter","text":"Keeps track of the number of events as they come and emits the current count out alongside the event. The output is a record of the form {\"count\": n, \"event\": event} , where n is the current count and event is the original event. The counter starts when the first event comes through and begins from 1. Outputs : out Example : define generic :: counter operator counter ;","title":"generic::counter"},{"location":"tremor-query/operators/#debughistory","text":"Note This operator is for debugging purposes only, and should not be used in production deployments. This operator generates a history entry in the event metadata underneath the field provided in the name config value. Data is pushed to the array as a Striong in the form: \"event: <op>(<event_id>)\" . This can be used as a tracepoint of events in a complex pipeline setup. Configuration options : op - The operation name of this operator name - The field to store the history on Outputs : out Example : define debug :: history operator history with op = \"my-checkpoint\" , name = \"event_history\" end ;","title":"debug::history"},{"location":"tremor-query/pp/","text":"Lexical Preprocessor \u00b6 In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users. Directives \u00b6 The preprocessor has two directives The #!line directive The #!config directive Line directive \u00b6 This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line directive is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future. Config directive \u00b6 This directive allows compile-time configuration parameters to be passed into tremor Example preprocessed tremor-script \u00b6 #!line 0 0 0 1 ./foo/bar/snot.trickle mod snot with #!line 0 0 0 1 ./foo/bar/snot.trickle define tumbling window second with interval = 1000 end ; end ; #!line 19 1 0 0 main.trickle #!line 0 0 0 2 ./foo/baz/badger.trickle mod badger with #!line 0 0 0 2 ./foo/baz/badger.trickle define tumbling window minute with interval = 60000 end ; end ; #!line 41 1 0 0 main.trickle select event from in [ snot :: second , badger :: minute ] into out ;","title":"Lexical Preprocessor"},{"location":"tremor-query/pp/#lexical-preprocessor","text":"In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users.","title":"Lexical Preprocessor"},{"location":"tremor-query/pp/#directives","text":"The preprocessor has two directives The #!line directive The #!config directive","title":"Directives"},{"location":"tremor-query/pp/#line-directive","text":"This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line directive is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future.","title":"Line directive"},{"location":"tremor-query/pp/#config-directive","text":"This directive allows compile-time configuration parameters to be passed into tremor","title":"Config directive"},{"location":"tremor-query/pp/#example-preprocessed-tremor-script","text":"#!line 0 0 0 1 ./foo/bar/snot.trickle mod snot with #!line 0 0 0 1 ./foo/bar/snot.trickle define tumbling window second with interval = 1000 end ; end ; #!line 19 1 0 0 main.trickle #!line 0 0 0 2 ./foo/baz/badger.trickle mod badger with #!line 0 0 0 2 ./foo/baz/badger.trickle define tumbling window minute with interval = 60000 end ; end ; #!line 41 1 0 0 main.trickle select event from in [ snot :: second , badger :: minute ] into out ;","title":"Example preprocessed tremor-script"},{"location":"tremor-query/recipes/","text":"Periodic Synthetic Metrics Events \u00b6 Periodically, produce basic statistics and percentiles / quartiles from a stream of ingested events, for a particular value in the inbound event stream. # Every 10 seconds create tumbling window ` 10 secs ` with interval = datetime :: with_seconds ( 10 ), end ; # Aggregate events producing statistics into a temporary stream select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => \"10s\" end , \"stats\" : aggr :: stats :: hdr ( event . fields [ group [ 2 ]], [ \"0.42\" , \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"class\" : group [ 2 ] } from in [` 10 secs `] group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into normalize having event . stats . count > 100 ; # discard if not enough sample data for group # create a temporary stream to normalize results create stream normalize ; # normalize output record to match requirements downstream ( influx ) select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . class} \" : event . stats . count , \"min_ #{ event . class} \" : event . stats . min , \"max_ #{ event . class} \" : event . stats . max , \"mean_ #{ event . class} \" : event . stats . mean , \"stdev_ #{ event . class} \" : event . stats . stdev , \"var_ #{ event . class} \" : event . stats . var , \"p42_ #{ event . class} \" : event . stats . percentiles [ \"0.42\" ], \"p50_ #{ event . class} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . class} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . class} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . class} \" : event . stats . percentiles [ \"0.999\" ] } } from normalize into out ;","title":"Recipes"},{"location":"tremor-query/recipes/#periodic-synthetic-metrics-events","text":"Periodically, produce basic statistics and percentiles / quartiles from a stream of ingested events, for a particular value in the inbound event stream. # Every 10 seconds create tumbling window ` 10 secs ` with interval = datetime :: with_seconds ( 10 ), end ; # Aggregate events producing statistics into a temporary stream select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => \"10s\" end , \"stats\" : aggr :: stats :: hdr ( event . fields [ group [ 2 ]], [ \"0.42\" , \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"class\" : group [ 2 ] } from in [` 10 secs `] group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into normalize having event . stats . count > 100 ; # discard if not enough sample data for group # create a temporary stream to normalize results create stream normalize ; # normalize output record to match requirements downstream ( influx ) select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . class} \" : event . stats . count , \"min_ #{ event . class} \" : event . stats . min , \"max_ #{ event . class} \" : event . stats . max , \"mean_ #{ event . class} \" : event . stats . mean , \"stdev_ #{ event . class} \" : event . stats . stdev , \"var_ #{ event . class} \" : event . stats . var , \"p42_ #{ event . class} \" : event . stats . percentiles [ \"0.42\" ], \"p50_ #{ event . class} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . class} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . class} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . class} \" : event . stats . percentiles [ \"0.999\" ] } } from normalize into out ;","title":"Periodic Synthetic Metrics Events"},{"location":"tremor-query/walkthrough/","text":"Trickle Walkthough \u00b6 In this section we walk through increasingly complex tremor query language ( trickle ) programming examples, introducing key concepts as we progress. Overview \u00b6 Trickle is a near real-time stream-based structured query language that builds on the tremor scripting language. Trickle queries are compiled in pipeline DAGs and replace the yaml pipeline format used in previous versions to describe event processing graphs in the tremor runtime. The most basic query possible in trickle is select event from in into out ; # A basic passthrough query pipeline The event keyword selects the event from the standard input stream in and passes it through unchanged to the standard output stream out . In tremor's trickle query language queries are compiled and optimised and data is streamed though the query. Data can be passed through, transformed, filtered, aggregated, branched or combined to form continuous stream processing algorithms. Like other event processing languages, trickle inverts the relationship between query and data when compared to normal RDBMS SQL languages. Instead of running a dynamic query against static in memory or disk persisted data, we compile and optimise queries and stream near real-time data through each query. Data can be ingested from the outside world via the 'in' standard stream. Data can be produced to the outside world via the 'out' standard stream. Errors can be processed through the 'err' standard stream. These three primitives are analogous to the stdin , stdout and stderr streams in UNIX-like systems. These can be chained or interconnected via connecting multiple statements together to form a directed acyclic graph. We can branch inputs using where and having clauses as filters to logically partition streams into independent processing streams. In the below example we partition events by their seq_num field. If the number is even, we branch the corresponding events into a stream named evens . If the number is odd, we branch to a stream named odds . The logical inverse of branching is to unify or union streams together, this operation is also supported via the select operation and is demonstrated below. We combine the evens and odds streams into the standard output stream # create private intermediate internal streams create stream evens ; create stream odds ; # branch input into even/odd sequenced events using where clause select { \"even\" : event } from in where ( event . seq_num % 2 == 1 ) into evens ; select { \"odd\" : event } from in where ( event . seq_num % 2 == 0 ) into odds ; # combine / union evens and odds into standard out stream select event from evens into out ; select event from odds into out ; We can test this with a json event using the tremor command line tool { \"seq_num\" : 4 , \"value\" : 10 , \"group\" : \"horse\" } Assuming the trickle query is stored in a file called evenodd.trickle with the sample event in a file called data.json $ tremor run evenodd.trickle -i data.json The command line tool will inject all events from the file provided by the -i argument and we would expect to see output from the tool as follows: { \"odd\" : { \"seq_num\" : 4 , \"value\" : 10 , \"group\" : \"horse\" }} Scripts and Operators \u00b6 Here's the logic for an entire backpressure algorithm that could be introduced as a proxy between two systems, implemented by using a builtin operator called qos::backpressure : define qos :: backpressure operator bp with timeout = 10000 , steps = [ 1 , 50 , 100 , 250 , 750 ], end ; create operator bp from bp ; select event from in into bp ; select event from bp into out ; A slightly more complex example that uses both operators and the tremor scripting language with the query language all together: define grouper :: bucket operator kfc ; define script categorize script let $ rate = 1 ; let $ class = event .` group `; { \"event\" : event , \"rate\" : $ rate , \"class\" : $ class } ; end ; create operator kfc from kfc ; # where script definition and instance name are the same, we can # omit the from clause in operator and script 'create' statements create script categorize ; # Stream ingested data into categorize script select event from in into categorize ; # Stream scripted events into kfc bucket operator select event from categorize into kfc ; # Stream bucketed events into out stream select event from kfc into out ; Operators are defined as <module>::<name> in the context of an operator definition clause. Operators, like script definitions can take arguments. Definitions in tremor are non-executing. They should be considered as templates or specifications. In the query language, any define clause creates specifications, possibly with arguments for specialization. They are typically incarnated via the create clause. Anything that is create ed will form a stream or node in the query graph - these do consume memory and participate in a pipeline query algorithm. So in the above example, the categorize script and the categorize node have both a definition or specification and an instance node that participates in the graph at runtime. It is often convenient to use the same name where there is only one instance of an operator of a given type. Building Query Graph Algorithms \u00b6 Data streams can be branched and combined in the trickle query language via the select statement. The resulting graphs must be acyclic with no direct or indirect looping cycles. Branching \u00b6 Branching data streams to multiple streams is performed via select operations Branch data into 3 different output stream ports select event from in into out / a ; select event from in into out / b ; select event from in into out / c ; Branch data into 3 different intermediate streams create stream a ; create stream b ; create stream c ; select event from in into a ; select event from in into b ; select event from in into c ; Combining \u00b6 Multiple data streams can also be combined via select operations. Combine 3 data streams into a single output stream ... select event from a into out ; select event from b into out ; select event from c into out ; Combine 3 data stream ports from 1 or many streams into a single output stream ... select event from a / 1 into out ; select event from a / 2 into out ; select event from b into out ; Aggregations \u00b6 A key feature of the tremor query language are aggregations. These are supported with: Windows - A window is a range of events, clock or data time. There can be many different types of windows. Aggregate functions - An aggregate function is a function that runs in the context of a window of events, emitting results intermittently Tilt Frames - A tilt frame is a chain of compatible windows with decreasing resolution used to reduce memory pressure and preserve relative accuracy of windowed aggregate functions An example clock-driven tumbling window: define tumbling window `15secs` with interval = core::datetime::with_seconds(15), end; select { \"count\": aggr::stats::count(), # Aggregate 'count' function \"min\": aggr::stats::min(event.value), \"max\": aggr::stats::max(event.value), \"mean\": aggr::stats::mean(event.value), \"stdev\": aggr::stats::stdev(event.value), \"var\": aggr::stats::var(event.value), } from in[`15secs`] # We apply the window nominally to streams into out; To use a window we need to define the window specifications, such as a 15 second clock-based tumbling window called 15secs as above. We can then create instances of these windows at runtime by applying those windows to streams. This is done in the from clause of a select statement. Wherever windows are applied, aggregate functions can be used. In the above example, we are calculating the minimum, maximum, average, standard deviation and variance of a value numeric field in data streaming into the query via the standard input stream. The query language is not constrained to clock-driven window definitions. Windows can also be data-driven or fully programmatic. A more complete example: select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . fields [ group [ 2 ]], [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"class\" : group [ 2 ], \"timestamp\" : aggr :: win :: first ( event . timestamp ), } from in [` 10 secs `, ` 1 min `, ` 10 min `, ` 1 h `] where event . measurement == \"udp_lb_test\" or event . measurement == \"kafka-proxy.endpoints\" or event . measurement == \"burrow_group\" or event . measurement == \"burrow_partition\" or event . measurement == \"burrow_topic\" group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into normalize ; In the above example we use a single aggregate function called aggr::stats::hdr which uses a high dynamic range or HDR Histogram to compute quantile estimates and basic statistics against a number of dynamic grouping fields set by the group clause. A group clause effectively partitions our operation by the group expressions provided by the trickle query programmer. In the example, we're using the field names of the nested 'fields' record on inbound events to compose a component of a group that is also qualified by tags and a measurement name. The field component is used as a numeric input to the histogram aggregate function. In the from clause, we are using a tilt frame, or a succession of window resolutions over which this aggregate function is producing results. So a 10secs window is emitting on a 10-second repeating basis into a 1min frame. So 6 times per second the state of the 10 second window is merged into the 1min frame. This merge process is performed for each frame in the tilt frame. The advantage of tilt-frames is that as the target expression is the same for each frame, we can merge across each frame without amplifying error - in short, we get the effect of summarisation without loss of accuracy. Aggregation Mechanics \u00b6 The mechanics of aggregation in the query language are non-trivial. A high level non-normative summary follows. Windowing \u00b6 Assuming a periodic event delivered every 2 seconds into tremor. A size based window of size 2 would emit a synthetic output event every 2 events. So the lifespan of a size based window is 2 events, repeated and non-overlapping for tumbling style windows. In the illustration above events 1 and 2 in the first window w0 produce a single synthetic or derivate event a Events 3 and 4 in the second window w1 produce a single synthetic or derivate event b As there is no 6th event in the example illustration, we will never get another synthetic output event Contrast this with the 10 second or clock-based tumbling window. In the first window w0 s lifetime we capture all events in the illustration. Tilt Frames \u00b6 Assuming a continuous flow of events into tremor... All the synthetic outputs of successive 5 minute windows that fit into a 15 minute interval are merged into the 15 minute window. All the outputs of successive 15 minute intervals that fit into a 1 hour interval are merged into the 1 hour window. By chaining and merging, tremor can optimise ( reduce ) the amount of memory required across the chain when compared to multiple independent windows select expressions. In the case of aggregate functions like aggr::stats::hdr`` or aggr::stats::dds``` the savings are significant. If we imagine 1M events per second, that is 300M events every 5 minutes. 900M every 15, 3.6B every hour. By using tilt frames we can maximally minimize internal memory consumption, whilst reducing the volume of incremental computation ( per event, per frame ), and further whilst preserving relative accuracy for merge-capable aggregate functions. The converged statistics under merge exhibit the same relative accuracy at a fraction of the computational and memory overhead without the using the tilt-frame mechanism. Group Mechanics \u00b6 The group by clause in the query language partitions streams before windows and tilt frames are applied. Groups can be set-based, each-based or composites thereof. Set based grouping \u00b6 Grouping by set partitions streams by a concatenation of expressions. select event from in group by set ( event . country , event . region ) into out ; In the example expression we are partitioning into a composite group that is composed of the country and region of each inbound event. So we expect data of the following form { \"country\" : \"US\" , \"region\" : \"east\" , \"az\" : \"1\" , ... } { \"country\" : \"US\" , \"region\" : \"east\" , \"az\" : \"2\" , ... } Each based grouping \u00b6 Given that our data can be nested, however, our data could be structured differently: { \"country\" : \"US\" , regio ns : { \"east\" : [ \"1\" , \"2\" ], } ... } select event from in group by each ( record :: keys ( event . regions )) into out ; Each field in the nested locations field becomes a component of our set and qualified by country ... Limitations \u00b6 There are cases however that are currently complex to partition with a single select statement due to limitations with the grouping clause. For example what if we wanted to make availability zones a component of our group partitions? How would we structure such a query? { \"country\" : \"US\" , regio ns : { \"east\" : [ \"1\" , \"2\" ], # AZs by regio n } ... } create stream by_country_region ; select { \"country\" : event . country , \"region\" : group [ 0 ], \"azs\" : event . regions [ group [ 0 ]] } from in group by each ( record :: keys ( event . regions )) into by_country_region ; We can preprocess our inbound stream and collapse our locations sub-record a single level by hoisting the region field to the top level of a synthetic intermediate outbound stream by_country_region . We can postprocess the intermediate stream by_country_region into a single outbound stream that further extracts and hoists the 'az' dimension select { \"country\" : event . country , \"region\" : event . region , \"az\" : group [ 0 ], } from by_country_region group by each ( event . azs ) into out ; So, we need 2 select statements to compose a solution where there are multiple nesting levels via successive extraction of group components. The same principle works with more complex grouping requirements. Once the grouping mechanics are resolved, windowing, aggregation and tilt-frames can be applied to further refine queries.","title":"Walkthrough"},{"location":"tremor-query/walkthrough/#trickle-walkthough","text":"In this section we walk through increasingly complex tremor query language ( trickle ) programming examples, introducing key concepts as we progress.","title":"Trickle Walkthough"},{"location":"tremor-query/walkthrough/#overview","text":"Trickle is a near real-time stream-based structured query language that builds on the tremor scripting language. Trickle queries are compiled in pipeline DAGs and replace the yaml pipeline format used in previous versions to describe event processing graphs in the tremor runtime. The most basic query possible in trickle is select event from in into out ; # A basic passthrough query pipeline The event keyword selects the event from the standard input stream in and passes it through unchanged to the standard output stream out . In tremor's trickle query language queries are compiled and optimised and data is streamed though the query. Data can be passed through, transformed, filtered, aggregated, branched or combined to form continuous stream processing algorithms. Like other event processing languages, trickle inverts the relationship between query and data when compared to normal RDBMS SQL languages. Instead of running a dynamic query against static in memory or disk persisted data, we compile and optimise queries and stream near real-time data through each query. Data can be ingested from the outside world via the 'in' standard stream. Data can be produced to the outside world via the 'out' standard stream. Errors can be processed through the 'err' standard stream. These three primitives are analogous to the stdin , stdout and stderr streams in UNIX-like systems. These can be chained or interconnected via connecting multiple statements together to form a directed acyclic graph. We can branch inputs using where and having clauses as filters to logically partition streams into independent processing streams. In the below example we partition events by their seq_num field. If the number is even, we branch the corresponding events into a stream named evens . If the number is odd, we branch to a stream named odds . The logical inverse of branching is to unify or union streams together, this operation is also supported via the select operation and is demonstrated below. We combine the evens and odds streams into the standard output stream # create private intermediate internal streams create stream evens ; create stream odds ; # branch input into even/odd sequenced events using where clause select { \"even\" : event } from in where ( event . seq_num % 2 == 1 ) into evens ; select { \"odd\" : event } from in where ( event . seq_num % 2 == 0 ) into odds ; # combine / union evens and odds into standard out stream select event from evens into out ; select event from odds into out ; We can test this with a json event using the tremor command line tool { \"seq_num\" : 4 , \"value\" : 10 , \"group\" : \"horse\" } Assuming the trickle query is stored in a file called evenodd.trickle with the sample event in a file called data.json $ tremor run evenodd.trickle -i data.json The command line tool will inject all events from the file provided by the -i argument and we would expect to see output from the tool as follows: { \"odd\" : { \"seq_num\" : 4 , \"value\" : 10 , \"group\" : \"horse\" }}","title":"Overview"},{"location":"tremor-query/walkthrough/#scripts-and-operators","text":"Here's the logic for an entire backpressure algorithm that could be introduced as a proxy between two systems, implemented by using a builtin operator called qos::backpressure : define qos :: backpressure operator bp with timeout = 10000 , steps = [ 1 , 50 , 100 , 250 , 750 ], end ; create operator bp from bp ; select event from in into bp ; select event from bp into out ; A slightly more complex example that uses both operators and the tremor scripting language with the query language all together: define grouper :: bucket operator kfc ; define script categorize script let $ rate = 1 ; let $ class = event .` group `; { \"event\" : event , \"rate\" : $ rate , \"class\" : $ class } ; end ; create operator kfc from kfc ; # where script definition and instance name are the same, we can # omit the from clause in operator and script 'create' statements create script categorize ; # Stream ingested data into categorize script select event from in into categorize ; # Stream scripted events into kfc bucket operator select event from categorize into kfc ; # Stream bucketed events into out stream select event from kfc into out ; Operators are defined as <module>::<name> in the context of an operator definition clause. Operators, like script definitions can take arguments. Definitions in tremor are non-executing. They should be considered as templates or specifications. In the query language, any define clause creates specifications, possibly with arguments for specialization. They are typically incarnated via the create clause. Anything that is create ed will form a stream or node in the query graph - these do consume memory and participate in a pipeline query algorithm. So in the above example, the categorize script and the categorize node have both a definition or specification and an instance node that participates in the graph at runtime. It is often convenient to use the same name where there is only one instance of an operator of a given type.","title":"Scripts and Operators"},{"location":"tremor-query/walkthrough/#building-query-graph-algorithms","text":"Data streams can be branched and combined in the trickle query language via the select statement. The resulting graphs must be acyclic with no direct or indirect looping cycles.","title":"Building Query Graph Algorithms"},{"location":"tremor-query/walkthrough/#branching","text":"Branching data streams to multiple streams is performed via select operations Branch data into 3 different output stream ports select event from in into out / a ; select event from in into out / b ; select event from in into out / c ; Branch data into 3 different intermediate streams create stream a ; create stream b ; create stream c ; select event from in into a ; select event from in into b ; select event from in into c ;","title":"Branching"},{"location":"tremor-query/walkthrough/#combining","text":"Multiple data streams can also be combined via select operations. Combine 3 data streams into a single output stream ... select event from a into out ; select event from b into out ; select event from c into out ; Combine 3 data stream ports from 1 or many streams into a single output stream ... select event from a / 1 into out ; select event from a / 2 into out ; select event from b into out ;","title":"Combining"},{"location":"tremor-query/walkthrough/#aggregations","text":"A key feature of the tremor query language are aggregations. These are supported with: Windows - A window is a range of events, clock or data time. There can be many different types of windows. Aggregate functions - An aggregate function is a function that runs in the context of a window of events, emitting results intermittently Tilt Frames - A tilt frame is a chain of compatible windows with decreasing resolution used to reduce memory pressure and preserve relative accuracy of windowed aggregate functions An example clock-driven tumbling window: define tumbling window `15secs` with interval = core::datetime::with_seconds(15), end; select { \"count\": aggr::stats::count(), # Aggregate 'count' function \"min\": aggr::stats::min(event.value), \"max\": aggr::stats::max(event.value), \"mean\": aggr::stats::mean(event.value), \"stdev\": aggr::stats::stdev(event.value), \"var\": aggr::stats::var(event.value), } from in[`15secs`] # We apply the window nominally to streams into out; To use a window we need to define the window specifications, such as a 15 second clock-based tumbling window called 15secs as above. We can then create instances of these windows at runtime by applying those windows to streams. This is done in the from clause of a select statement. Wherever windows are applied, aggregate functions can be used. In the above example, we are calculating the minimum, maximum, average, standard deviation and variance of a value numeric field in data streaming into the query via the standard input stream. The query language is not constrained to clock-driven window definitions. Windows can also be data-driven or fully programmatic. A more complete example: select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . fields [ group [ 2 ]], [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"class\" : group [ 2 ], \"timestamp\" : aggr :: win :: first ( event . timestamp ), } from in [` 10 secs `, ` 1 min `, ` 10 min `, ` 1 h `] where event . measurement == \"udp_lb_test\" or event . measurement == \"kafka-proxy.endpoints\" or event . measurement == \"burrow_group\" or event . measurement == \"burrow_partition\" or event . measurement == \"burrow_topic\" group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into normalize ; In the above example we use a single aggregate function called aggr::stats::hdr which uses a high dynamic range or HDR Histogram to compute quantile estimates and basic statistics against a number of dynamic grouping fields set by the group clause. A group clause effectively partitions our operation by the group expressions provided by the trickle query programmer. In the example, we're using the field names of the nested 'fields' record on inbound events to compose a component of a group that is also qualified by tags and a measurement name. The field component is used as a numeric input to the histogram aggregate function. In the from clause, we are using a tilt frame, or a succession of window resolutions over which this aggregate function is producing results. So a 10secs window is emitting on a 10-second repeating basis into a 1min frame. So 6 times per second the state of the 10 second window is merged into the 1min frame. This merge process is performed for each frame in the tilt frame. The advantage of tilt-frames is that as the target expression is the same for each frame, we can merge across each frame without amplifying error - in short, we get the effect of summarisation without loss of accuracy.","title":"Aggregations"},{"location":"tremor-query/walkthrough/#aggregation-mechanics","text":"The mechanics of aggregation in the query language are non-trivial. A high level non-normative summary follows.","title":"Aggregation Mechanics"},{"location":"tremor-query/walkthrough/#windowing","text":"Assuming a periodic event delivered every 2 seconds into tremor. A size based window of size 2 would emit a synthetic output event every 2 events. So the lifespan of a size based window is 2 events, repeated and non-overlapping for tumbling style windows. In the illustration above events 1 and 2 in the first window w0 produce a single synthetic or derivate event a Events 3 and 4 in the second window w1 produce a single synthetic or derivate event b As there is no 6th event in the example illustration, we will never get another synthetic output event Contrast this with the 10 second or clock-based tumbling window. In the first window w0 s lifetime we capture all events in the illustration.","title":"Windowing"},{"location":"tremor-query/walkthrough/#tilt-frames","text":"Assuming a continuous flow of events into tremor... All the synthetic outputs of successive 5 minute windows that fit into a 15 minute interval are merged into the 15 minute window. All the outputs of successive 15 minute intervals that fit into a 1 hour interval are merged into the 1 hour window. By chaining and merging, tremor can optimise ( reduce ) the amount of memory required across the chain when compared to multiple independent windows select expressions. In the case of aggregate functions like aggr::stats::hdr`` or aggr::stats::dds``` the savings are significant. If we imagine 1M events per second, that is 300M events every 5 minutes. 900M every 15, 3.6B every hour. By using tilt frames we can maximally minimize internal memory consumption, whilst reducing the volume of incremental computation ( per event, per frame ), and further whilst preserving relative accuracy for merge-capable aggregate functions. The converged statistics under merge exhibit the same relative accuracy at a fraction of the computational and memory overhead without the using the tilt-frame mechanism.","title":"Tilt Frames"},{"location":"tremor-query/walkthrough/#group-mechanics","text":"The group by clause in the query language partitions streams before windows and tilt frames are applied. Groups can be set-based, each-based or composites thereof.","title":"Group Mechanics"},{"location":"tremor-query/walkthrough/#set-based-grouping","text":"Grouping by set partitions streams by a concatenation of expressions. select event from in group by set ( event . country , event . region ) into out ; In the example expression we are partitioning into a composite group that is composed of the country and region of each inbound event. So we expect data of the following form { \"country\" : \"US\" , \"region\" : \"east\" , \"az\" : \"1\" , ... } { \"country\" : \"US\" , \"region\" : \"east\" , \"az\" : \"2\" , ... }","title":"Set based grouping"},{"location":"tremor-query/walkthrough/#each-based-grouping","text":"Given that our data can be nested, however, our data could be structured differently: { \"country\" : \"US\" , regio ns : { \"east\" : [ \"1\" , \"2\" ], } ... } select event from in group by each ( record :: keys ( event . regions )) into out ; Each field in the nested locations field becomes a component of our set and qualified by country ...","title":"Each based grouping"},{"location":"tremor-query/walkthrough/#limitations","text":"There are cases however that are currently complex to partition with a single select statement due to limitations with the grouping clause. For example what if we wanted to make availability zones a component of our group partitions? How would we structure such a query? { \"country\" : \"US\" , regio ns : { \"east\" : [ \"1\" , \"2\" ], # AZs by regio n } ... } create stream by_country_region ; select { \"country\" : event . country , \"region\" : group [ 0 ], \"azs\" : event . regions [ group [ 0 ]] } from in group by each ( record :: keys ( event . regions )) into by_country_region ; We can preprocess our inbound stream and collapse our locations sub-record a single level by hoisting the region field to the top level of a synthetic intermediate outbound stream by_country_region . We can postprocess the intermediate stream by_country_region into a single outbound stream that further extracts and hoists the 'az' dimension select { \"country\" : event . country , \"region\" : event . region , \"az\" : group [ 0 ], } from by_country_region group by each ( event . azs ) into out ; So, we need 2 select statements to compose a solution where there are multiple nesting levels via successive extraction of group components. The same principle works with more complex grouping requirements. Once the grouping mechanics are resolved, windowing, aggregation and tilt-frames can be applied to further refine queries.","title":"Limitations"},{"location":"tremor-query/functions/stats/","text":"The stats namespace \u00b6 The stats module contains functions for aggregating statistical measures of various events. Size \u00b6 When using stats aggregate functions size in memory becomes an important factor from a capacity planning perspective. The exact size of a window using aggregates depends on three main factors: The size of the dimension identifier. I.e. if the window is identified by the string \"window\" it will require that amount of memory related to this. If it is identified by an array of 10.000 elements all reading \"window\" it will use (about) 10.000 times that size. The unit size of each aggregate used in the window. We will try to give an estimate of size for each aggregate but please be aware that those are not always exact as they can depend on the data they hold. The number of groups, if grouping is configured. Each group will maintain a separate window of data For aggregates we'll provide an \"order of magnitude\" and a growth rate if applicable. For example Fixed, 10 bytes indicate that the size doesn't grow and is in the order of two digit bytes. We try to give pessimistic estimates where possible. Functions \u00b6 aggr::stats::count() -> int \u00b6 size : Fixed, 10 bytes Counts the number of events aggregated in the current windowed operation. aggr :: stats :: count () # number of items in the window aggr::stats::min(int|float) -> int|float \u00b6 size : Fixed, 10 bytes Determines the smallest event value in the current windowed operation. aggr :: stats :: min ( event . value ) aggr::stats::max(int|float) -> int|float \u00b6 size : Fixed, 10 bytes Determines the largest event value in the current windowed operation. aggr :: stats :: max ( event . value ) aggr::stats::sum(int|float) -> int|float \u00b6 size : Fixed, 10 bytes Determines the arithmetic sum of event values in the current windowed operation. aggr :: stats :: sum ( event . value ) aggr::stats::var(int|float) -> float \u00b6 size : Fixed, 100 bytes Calculates the sample variance of event values in the current windowed operation. aggr :: stats :: var ( event . value ) aggr::stats::stdev(int|float) -> float \u00b6 size : Fixed, 100 bytes Calculates the sample standard deviation of event values in the current windowed operation. aggr :: stats :: stdev ( event . value ) aggr::stats::mean(int|float) -> float \u00b6 size : Fixed, 100 bytes Calculates the stastical mean of the event values in the current windowed operation. aggr :: stats :: mean ( event . value ) aggr::stats::hdr(int|float) -> record \u00b6 size : Fixed, 100 Kilo Bytes (note: this strongly depends on configuration, and can be estimated more correctly with this formula ) Uses a High Dynamic Range ( HDR ) Histogram to calculate all basic statistics against the event values sin the current windowed operation. The function additionally interpolates percentiles or quartiles based on a configuration specification passed in as an argument to the aggregater function. The HDR Histogram trades off memory utilisation for accuracy and is configured internally to limit accuracy to 2 significant decimal places. aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.75\" , \"0.9\" , \"0.99\" , \"0.999\" ]) aggr::stats::dds(int|float) -> record \u00b6 size : Fixed, 10 Kilo Bytes (estimate based on this paper ) Uses a Distributed data-stream Sketch ( DDS (paper) Histogram to calculate count, min, max, mean and quartiles with quartile relative-error accurate over the range of points in the histogram. The DDS histogram trades off accuracy ( to a very low error and guaranteed low relative error ) and unlike HDR histograms does not need bounds specified. aggr :: stats :: dds ( event . value , [ \"0.5\" , \"0.75\" , \"0.9\" , \"0.99\" , \"0.999\" ])","title":"stats"},{"location":"tremor-query/functions/stats/#the-stats-namespace","text":"The stats module contains functions for aggregating statistical measures of various events.","title":"The stats namespace"},{"location":"tremor-query/functions/stats/#size","text":"When using stats aggregate functions size in memory becomes an important factor from a capacity planning perspective. The exact size of a window using aggregates depends on three main factors: The size of the dimension identifier. I.e. if the window is identified by the string \"window\" it will require that amount of memory related to this. If it is identified by an array of 10.000 elements all reading \"window\" it will use (about) 10.000 times that size. The unit size of each aggregate used in the window. We will try to give an estimate of size for each aggregate but please be aware that those are not always exact as they can depend on the data they hold. The number of groups, if grouping is configured. Each group will maintain a separate window of data For aggregates we'll provide an \"order of magnitude\" and a growth rate if applicable. For example Fixed, 10 bytes indicate that the size doesn't grow and is in the order of two digit bytes. We try to give pessimistic estimates where possible.","title":"Size"},{"location":"tremor-query/functions/stats/#functions","text":"","title":"Functions"},{"location":"tremor-query/functions/stats/#aggrstatscount-int","text":"size : Fixed, 10 bytes Counts the number of events aggregated in the current windowed operation. aggr :: stats :: count () # number of items in the window","title":"aggr::stats::count() -&gt; int"},{"location":"tremor-query/functions/stats/#aggrstatsminintfloat-intfloat","text":"size : Fixed, 10 bytes Determines the smallest event value in the current windowed operation. aggr :: stats :: min ( event . value )","title":"aggr::stats::min(int|float) -&gt; int|float"},{"location":"tremor-query/functions/stats/#aggrstatsmaxintfloat-intfloat","text":"size : Fixed, 10 bytes Determines the largest event value in the current windowed operation. aggr :: stats :: max ( event . value )","title":"aggr::stats::max(int|float) -&gt; int|float"},{"location":"tremor-query/functions/stats/#aggrstatssumintfloat-intfloat","text":"size : Fixed, 10 bytes Determines the arithmetic sum of event values in the current windowed operation. aggr :: stats :: sum ( event . value )","title":"aggr::stats::sum(int|float) -&gt; int|float"},{"location":"tremor-query/functions/stats/#aggrstatsvarintfloat-float","text":"size : Fixed, 100 bytes Calculates the sample variance of event values in the current windowed operation. aggr :: stats :: var ( event . value )","title":"aggr::stats::var(int|float) -&gt; float"},{"location":"tremor-query/functions/stats/#aggrstatsstdevintfloat-float","text":"size : Fixed, 100 bytes Calculates the sample standard deviation of event values in the current windowed operation. aggr :: stats :: stdev ( event . value )","title":"aggr::stats::stdev(int|float) -&gt; float"},{"location":"tremor-query/functions/stats/#aggrstatsmeanintfloat-float","text":"size : Fixed, 100 bytes Calculates the stastical mean of the event values in the current windowed operation. aggr :: stats :: mean ( event . value )","title":"aggr::stats::mean(int|float) -&gt; float"},{"location":"tremor-query/functions/stats/#aggrstatshdrintfloat-record","text":"size : Fixed, 100 Kilo Bytes (note: this strongly depends on configuration, and can be estimated more correctly with this formula ) Uses a High Dynamic Range ( HDR ) Histogram to calculate all basic statistics against the event values sin the current windowed operation. The function additionally interpolates percentiles or quartiles based on a configuration specification passed in as an argument to the aggregater function. The HDR Histogram trades off memory utilisation for accuracy and is configured internally to limit accuracy to 2 significant decimal places. aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.75\" , \"0.9\" , \"0.99\" , \"0.999\" ])","title":"aggr::stats::hdr(int|float) -&gt; record"},{"location":"tremor-query/functions/stats/#aggrstatsddsintfloat-record","text":"size : Fixed, 10 Kilo Bytes (estimate based on this paper ) Uses a Distributed data-stream Sketch ( DDS (paper) Histogram to calculate count, min, max, mean and quartiles with quartile relative-error accurate over the range of points in the histogram. The DDS histogram trades off accuracy ( to a very low error and guaranteed low relative error ) and unlike HDR histograms does not need bounds specified. aggr :: stats :: dds ( event . value , [ \"0.5\" , \"0.75\" , \"0.9\" , \"0.99\" , \"0.999\" ])","title":"aggr::stats::dds(int|float) -&gt; record"},{"location":"tremor-query/functions/win/","text":"The win namespace \u00b6 The win module contains functions for aggregating over the current active window in a window of events. The functions can also be used in tilt frames where events emitting from a window are chained across multiple window frames in sequence. Functions \u00b6 aggr::win::first() -> event \u00b6 Capture and return the first event that hits a window upon/after opening. aggr :: win :: first () # first event in a window aggr::win::last() -> event \u00b6 Capture and return the last event that hits a window upon/after opening. aggr :: win :: last () aggr::win::collect_flattened() -> [event] \u00b6 Captures all events in a window into an array of events. In the case of tilt frames, flattens out any tilt frame sub-arrays aggr :: win :: collect_flattened () aggr::win::collect_nested() -> [[event]]|[event] \u00b6 Captures all events in a window into an array of events. In the case of tilt frames, each frame is preserved as a nested array of arrays. For a tilt frame of 3 windows, the inner-most leaf array contains events, and higher levels are arrays of arrays. aggr :: win :: collect_nested ()","title":"win"},{"location":"tremor-query/functions/win/#the-win-namespace","text":"The win module contains functions for aggregating over the current active window in a window of events. The functions can also be used in tilt frames where events emitting from a window are chained across multiple window frames in sequence.","title":"The win namespace"},{"location":"tremor-query/functions/win/#functions","text":"","title":"Functions"},{"location":"tremor-query/functions/win/#aggrwinfirst-event","text":"Capture and return the first event that hits a window upon/after opening. aggr :: win :: first () # first event in a window","title":"aggr::win::first() -&gt; event"},{"location":"tremor-query/functions/win/#aggrwinlast-event","text":"Capture and return the last event that hits a window upon/after opening. aggr :: win :: last ()","title":"aggr::win::last() -&gt; event"},{"location":"tremor-query/functions/win/#aggrwincollect_flattened-event","text":"Captures all events in a window into an array of events. In the case of tilt frames, flattens out any tilt frame sub-arrays aggr :: win :: collect_flattened ()","title":"aggr::win::collect_flattened() -&gt; [event]"},{"location":"tremor-query/functions/win/#aggrwincollect_nested-eventevent","text":"Captures all events in a window into an array of events. In the case of tilt frames, each frame is preserved as a nested array of arrays. For a tilt frame of 3 windows, the inner-most leaf array contains events, and higher levels are arrays of arrays. aggr :: win :: collect_nested ()","title":"aggr::win::collect_nested() -&gt; [[event]]|[event]"},{"location":"tremor-script/","text":"Tremor-Script \u00b6 The tremor-script scripting language is an interpreted expression-oriented language designed for the filtering, extraction, transformation and streaming of structured data in a stream or event-based processing system. At its core, tremor-script supports a structured type system equivalent to JSON. It supports integer, floating point, boolean and UTF-8 encoded string literals, literal arrays and associative dictionaries or record types in addition to a null marker. A well-formed JSON document is a legal tremor-script expression. Principles \u00b6 Safety \u00b6 The language is explicitly not Turing-complete: there are no unstructured goto grammar forms there are no unbounded for , while or do..while looping constructs the language is built on top of rust, inheriting its robustness and safety features, without the development overheads Developer friendly \u00b6 The language adopts a Fortran-like syntax for key expression forms and has a path-like syntax for indexing into records and arrays Stream-oriented / event-based \u00b6 Tremor-script is designed to process unstructured ( but well-formed ) data events. Event data can be JSON, MsgPack or any other form supported by the tremor event processing system. Self-documenting \u00b6 The fortran-like syntax allows rich operations to be computed against arbitrary JSON-like data. For example JSON documents can be patch ed and merge ed with operation and document based templates. Records and Arrays can be iterated over to transform them, merge them with other documents or to extract subsets for further processing Extensibility \u00b6 The expression-based nature of tremor-script means that computational forms and any processing is transient. The language describes a set of rules ( expressions ) that process an inbound event document into an outbound documented emitted after evaluation of the script. The core expression language is designed for reuse in other script-based DSLs and can currently be extended through its modular function subsystem. The language also supports a pluggable data extraction model allowing base64 encoded, regular expressions and other micro-formats encoded within record fields to be referenced based on their internal structure and for subsets to be mapped accordingly. In the future, tremor-script may be retargeted as a JIT-compiled language. Performant \u00b6 Data ingested into tremor-script is vectorized via SIMD-parallel instructions on x86-64 or other Intel processor architectures supporting ssev3/avx extensions. Processing streams of such event-data incurs some allocation overhead at this time, but these event-bound allocations are being written out of the interpreter. The current meaning of performant as documented here means that tremor-script is more efficient at processing log-like data than the system it replaces ( logstash - which mixes extraction plugins such as grok and dissect with JRuby scripts and a terse configuration format ) Productive \u00b6 The tremor-script parsing tool-chain has been designed with ease-of-debugging and ease-of-development in mind. It has buitin support for syntax-highlighting on the console with errors annotating highlighted sections of badly written scripts to simplify fixing such scripts. Language \u00b6 This section details the major components of the tremor-script language Comments \u00b6 Comments in tremor-script are single-line comments that begin with a '#' symbol and continue until end of line. # I am a comment Literals \u00b6 Literal in tremor-script are equivalent to their sibling types supported by the JSON format. Null \u00b6 The null literal which represents the absence of a defined value null Boolean \u00b6 Boolean literal. true false Integer Numerics \u00b6 Integers in tremor-script are signed and are limited to 64-bit internal representation 404 The stdlib provides useful function for integers in std::integer . use std :: integer ; integer :: parse ( \"42\" ) == 42 Floating-Point Numerics \u00b6 Floating point numerics in tremor-script are signed and are limited to 64-bit IEEE representation 1.67 - e10 The stdlib provides useful function for floats in std::float . Character and Unicode Code-points \u00b6 The language does not support literal character or Unicode code-points at this time. UTF-8 encoded Strings \u00b6 \"I am a string\" The standard library provides useful function for string manipulation in std::string : use std :: string ; string :: uppercase ( string :: substr ( \"snotty\" , 0 , 4 )) == \"SNOT\" String Interpolation \u00b6 For strings tremor allows string interpolation, this means embedding code directly into strings to create strings out of them. \"I am a #{ \"string with #{ 1 } interpolation.\" } \" A hash sign followed by a curly bracket needs to be escaped \\#{ hash signs themselves do not need to be escaped. HereDocs \u00b6 To deal with pre formatted strings in tremor script we allow for heredocs they are started by using triple quotes \"\"\" that terminate the line (aka \"\"\"bla isn't legal). Heredocs do not truncate leading indentation, only the first leading linebreak after the leading triple-quote \"\"\" stripped. \"\"\" I am a long multi-line string with #{ \" #{ 1 } interpolation\" } \"\"\" Since Tremor 0.9 Heredocs also support String Interpolation . A hash sign followed by a curly bracket needs to be escaped \\#{ hash signs themselves do not need to be escaped. Arrays \u00b6 Array grammar: Array literals in tremor-script are a comma-delimited set of expressions bracketed by the square brakcets '[' and ']'. [ 1 , 2 , \"foobar\" , 3.456e10 , { \"some\" : \"json-like-document\" } , null ] The standard library provides several useful functions to work with arrays in std::array : use std :: array ; array :: push ([ \"snot\" ], \"badger\" ) == [ \"snot\" , \"badger\" ] Records \u00b6 Record grammar: Field grammar: Record literals in tremor-script are syntactically equivalent to JSON document objects { \"field1\" : \"value1\" , \"field2\" : [ \"value\" , \"value\" , \"value\" ], \"field3\" : { \"field4\" : \"field5\" } } Check out the stdlib std::record module for some helpful function for working with records. Binary \u00b6 Binaries are based on the Erlang bit syntax . Binary grammar: BinaryFields grammar: BinaryField grammar: Parts of each field are: <value>:<size>/<type> where both size and type are optional. The binary types consists of up to three parts. That is 2 prefixes and 1 main type identifier. Examples: unsigned-big-integer , signed-integer , binary . The types currently supported are: binary - this can handle both binaries and strings, size here refers to the number of bytes integer - this can represent integers, size here means size in bits. In addition the type can be prefixed with big and little for indianness and signed and unsigned for signedness. Some examples would be: <<1:1, 42:7>> <<(1 + 1)/unsigned-big-integer>> <<1:4, \"badger\"/binary, -2:4/signed-little-integer>> We could construct a TCP package this way: # constructing a TCP package # using made up, non-correct values let event = { \"src\" : { \"port\" : 1234 } , \"dst\" : { \"port\" : 2345 } , \"seq\" : event , \"ack\" : 4567 , \"offset\" : 1 , \"res\" : 2 , \"flags\" : 3 , \"win\" : 4 , \"checksum\" : 5 , \"urgent\" : 6 , \"data\" : \"snot badger!\" } ; << event . src . port : 16 , event . dst . port : 16 , event . seq : 32 , event . ack : 32 , event . offset : 4 , event . res : 4 , event . flags : 8 , event . win : 16 , event . checksum : 16 , event . urgent : 16 , event . data / binary >> See also: std::binary for useful function for working with binary data. std::string::into_binary and std::string::from_utf8_lossy std::base64 for encoding and decoding binary data to string using base64. Operators \u00b6 List of binary and unary operators in tremor-script , ordered by precedence (from low to high): Symbol Name Example Types or Logical OR true or false bool and Logical AND true and false bool | Bitwise OR Bitwise OR has not been implemented yet - ^ Bitwise XOR 42 ^ 42, true ^ true integer, bool & Bitwise AND 42 & 0, true & false integer, bool ==, != Equality, Inequality \"snot\" != \"badger\" all <, <=, >, >= Comparison Operators 42 > 0 integer, float, string, binary <<, >>, >>> Bitwise shift -- Left, Right(signed), Right(unsigned) 42 >> 2 integer +, - Addition, Subtraction 42 + 0 integer, float, string *, /, % Multiplication, Division, Modulus 42 * 1 integer, float (no modulo) +, - Unary Plus, Unary Minus +42 integer, float, string not , ! Unary Logical NOT, Unary Bitwise NOT not false , Bitwise NOT has not been implemented yet bool Paths \u00b6 Path grammar: Qualified Segments grammar: PathSegment grammar: ArraySegment grammar: Path-like structures in tremor-script allow a subset of an ingested event, meta-data passed to the tremor-script function and script-local data to be indexed. Example event for illustration purposes: { \"store\" : { \"book\" : [ { \"category\" : \"reference\" , \"author\" : \"Nigel Rees\" , \"title\" : \"Sayings of the Century\" , \"price\" : 8.95 }, { \"category\" : \"fiction\" , \"author\" : \"Herman Melville\" , \"title\" : \"Moby Dick\" , \"isbn\" : \"0-553-21311-3\" , \"price\" : 8.99 }, { \"category\" : \"fiction\" , \"author\" : \"J.R.R. Tolkien\" , \"title\" : \"The Lord of the Rings\" , \"isbn\" : \"0-395-19395-8\" , \"price\" : 22.99 } ], \"bicycle\" : { \"color\" : \"red\" , \"price\" : 19.95 } }, \"expensive\" : 10 } Grab the entire event document: let capture = event ; Grab the books from the store (the same using key, index and escaped key notation for field lookup): let capture = event . store . book ; # index and escaped notation can acomodate keys that include 'odd' characters such as whitespaces or dots. let capture = event . store [ \"book\" ]; let capture = event . store .` book `; Grab the first book: let capture = event . store . book [ 0 ]; Grab the title of the 3rd book: let capture = event . store . book [ 2 ]. title Grab the range of books from 0 ( the first ) to 2 ( the last ), exclusive of the last book: let capture = event . store . book [ 0 : 2 ]; The type of a path is equivalent to the type of the data returned by a path expression. So in the above examples, a reference to a book title would return the value at that path, which in the reference event document is a string . Path's in tremor-script are themselves expressions in their own right. Const \u00b6 Const grammer: Const can be used to define immutable, constant values that get evaluated at compile time. This is more performant then let as all logic can happen at compile time and is helpful for setting up lookup tables or other never changing data structures. Let \u00b6 Let grammar: The let expression allows data pointed to by a path to be destructively mutated, and the pointed-to value reassigned. If the path does not yet exist, it will be created in-situ: Set a local variable a to the literal integer value 10: let a = 10 ; Set a local variable a to be the ingested event record let a = event ; Set the metadata variable a to be the value of the local variable a : let $ a = a ; Drop \u00b6 Drop expressions enable short-circuiting the evaluation of a tremor-script when badly formed data is discovered. If no argument is supplied, drop will return the event record. If an argument is supplied, the result of evaluating the expression will be returned. Tremor or other processing tools can process dropped events or data using purpose-built error-handling. As the content of the dropped event is user-defined, operators can standardise the format of the error emitted on drop from tremor-script drop ; drop ; # As the first drop always wins, this expression never runs Emit \u00b6 Emit grammar: Emit expressions enable short-circuiting the evaluation of a tremor-script when processing is known to be complete and further processing can be avoided. If no argument is supplied, emit` will return the event record. If an argument is supplied, the result of evaluating the expression will be returned. Tremor or other processing tools can process emitted events or data using their default flow-based or stream-based data processing pipelines. As the content of the emitted event is user-defined, oeprators can standardise the format of the event emitted on emit from tremor-script Note By default, if no emit or drop expressions are defined, all expressions in a correctly written tremor-script will be evaluated until completion and the value of the last expression evaluated will be returned as an emit message. Implicit emission: \"badgers\" # implicit emit Explicit emission of \"snot\" : \"badgers\" # literals do not short-circuit processing, so we continue to the next expression in this case emit \"snot\" emit \"oh noes!\" emit \"never happens\" ; # As the first emit always wins, this expression never runs There are times when it is necessary to emit synthetic events from tremor-script within a tremor pipeline to an alternate operator port than the default success route. For example, when data is well-formed but not valid and the data needs to be diverted into an alternate flow. The emit clause can be deployed for this purpose by specifying an optional named port. emit { \"event\" : event , \"status\" : \"malformed\" , \"description\" : \"required field `loglevel` is absent\" } => \"invalid\" ; Match \u00b6 Match grammar: Match case grammar: Match expressions enable data to be filtered or queried using case-based reasoning. Match expressions take the form: match < target > of case < case - expr > [ < guard > ] => < block > ... default => < block > end Where: target: An expression that is the target of case-based queries case-expr: A predicate test, literal value or pattern to match against guard: An optional predicate expression to gate whether or not an otherwise matching case-clause will in fact match block: The expressions to be evaluated if the case matches, and any supplied guard evaluates to true Examples: Discover if the store.book path is an array, record or scalar structure: match store . book of case %[] => let msg = \"store.book is an array-like data-structure\" , msg case % {} => \"store.book is a record-like data-structure\" default => \"store.book is a scalar data-type\" end Find all fiction books in the store: let found = match store . book of case fiction = %[ % { category ~= \"fiction\" } ] => fiction default => [] end ; emit found ; Matching literal expressions \u00b6 The simplest form of case expression in match expressions is matching a literal value. Values can be any legal tremor-script type and they can be provided as literals, computed values or path references to local variables, metadata or values arriving via events. let example = match 12 of case 12 => \"matched\" default => drop \"not possible\" end ; let a = \"this is a\" ; let b = \" string\" ; let example = match a + b of case \"this is a string\" => \"matched\" default => drop \"not possible\" end ; let a = [ 1 , \"this is a string\" , { \"record-field\" : \"field-value\" } ]; match a of case a => a default => drop \"not possible\" end ; Matching on test predicate expressions \u00b6 It is also possible to perform predicate based matching match \"this is not base64 encoded\" of case ~ base64 || => \"surprisingly, this is legal base64 data\" default => drop \"as suspected, this is not base64 encoded\" end ; These are often referred to informally as tilde expressions and tremor supports a variety of micro-formats that can be used for predicate or test-based matching such as logstash dissect, json, influx, perl-compatible regular expressions. Tilde expressions can under certain conditions elementize ( extract ) micro-format data. The elementization or extraction is covered in the Extractors section of this document and in the Extractor reference. Match and extract expressions \u00b6 It is also possible to elementize or ingest supported micro-formats into tremor-script for further processing. For example, we can use the ~= and ~ operator to perform a predicate test, such as the base64 test in the previous example, which upon success, extracts ( in the base64 case, decoding ) a value for further processing. For example if we had an embedded JSON document in a string, we could test for the value being well-formed json, and extract the contents to a local variable as follows: let sneaky_json = \" { \\\" snot \\\" : \\\" badger \\\" } \" ; match sneaky_json of case json = ~ json || => json default => drop \"this is not the json we were looking for\" end ; Matching tuple patterns \u00b6 Tip A tuple pattern matches a target value if the target is an array and each test matches the positionally correspondent value in the target . The target needs to be at least as long as the pattern but can be longer if the pattern ends with ... . If you are looking for a more set like operation look at the array pattern . Tuple Pattern grammar: Tuple Pattern filter grammar: In addition to literal array matching, where the case expression tuple literal must exactly match the target of the match expression one for one, tuple patterns enable testing for matching elements within an array and filtering on the basis of matched elements. let a = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ]; match a of case %( 0 ) => \"is a zero\" case %( 0 , .. ) => \"starts with a zero\" case %( _ , 1 , .. ) => \"has 1 one at index 1\" default => \"does not contain zero's\" end ; Matching array patterns \u00b6 Tip An array pattern matches a target value if the target is an array and each test in the pattern matches at least for one element in the target indiscriminate of their positions. If you are looking for a more array like / positional operation look at the tuple pattern . Array Pattern grammar: Array Pattern filter grammar: In addition to a subset match, where the elements of the pattern must be included in the target of the match expression, array patterns enable testing for matching elements within an array and filtering on the basis of matched elements. let a = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ]; match a of case %[ 0 ] => \"contains zero's\" default => \"does not contain zero's\" end ; Predicate matching against supported micro-formats is also supported in array pattern matching. let a = [ \"snot\" , \"snot badger\" , \"snot snot\" , \"badger badger\" , \"badger\" ]; match a of case got = %[ ~ re |^( P < hit > snot .*)$| ] => got default => \"not snotty at all\" end ; Matching record patterns \u00b6 Tip A record pattern matches a target if the target is a record that contains at least all declared keys and the tests for each of the declared key match. Record Pattern grammar Record Pattern Fields grammar Similarly to record literal matching where the case expression record must exactly match the target of the match expression, record patterns enable testing for matching fields or sub-structures within a record and extracting and elementizing data on the basis of matched predicate tests ( via ~= ). We can check for the presence of fields: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { present superhero , present human } => \"ok\" default => \"not possible\" end We can check for the absence of fields: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { absent superhero , absent human } => \"not possible\" default => \"ok\" end We can test the values of fields that are present: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { superhero == \"superman\" } => \"we are saved! \\o/\" case % { superhero != \"superman\" } => \"we may be saved! \\o/\" default => \"call 911\" end ; We can test for records within records: match { \"superhero\" : { \"name\" : \"superman\" } } of case % { superhero ~= % { present name } } => \"superman is super\" case % { superhero ~= % { absent name } } => \"anonymous superhero is anonymous\" default => \"something bad happened\" end ; We can also test for records within arrays within records tersely through nested pattern matching: match { \"superhero\" : [ { \"name\" : \"batman\" } , { \"name\" : \"robin\" } ] } of case id = % { superhero ~= %[ % { name ~= re |^(? P < kind > bat .*)$| } ] } => id default => \"something bad happened\" end ; Guard clauses \u00b6 Guard expressions in Match case clauses enable matching data structures to be further filtered based on predicate expressions. For example they can be used to restrict the match to a subset of matching cases where appropriate. match event of case record = % {} when record . log_level == \"ERROR\" => \"error\" default => \"non-error\" end Effectors \u00b6 Effectors grammar: Block: Effectors are the expressions evaluated when a case pattern and guard succeeded. Things are simple when using only a single expression as the match case effector. When we have to use multiple expressions to do some more complex processing, we need to separate those expressions with commas , : use std :: string ; match event of case record = % { present foo } => let foo_content = record [ \"foo\" ], let replaced = string :: replace ( foo_content , \"foo\" , \"bar\" ), let record [ \"foo\" ] = replaced default => null end Merge \u00b6 Merge expressions defines a difference against a targetted record and applies that difference to produce a result record. Merge operations in tremor-script follow merge-semantics defined in RFC 7386 . let event = merge event of { \"some\" : \"record\" } end Given Merge Result Explanation {\"a\":\"b\"} {\"a\":\"c\"} {\"a\":\"c\"} Insert/Update field 'a' {\"a\":\"b\"} {\"b\":\"c\"} {\"a\":\"b\", \"b\":\"c\"} Insert field 'b' {\"a\":\"b\"} {\"a\":null} {} Erase field 'a' {\"a\":\"b\",\"b\":\"c\"} {\"a\":null} {\"b\":\"c\"} Erase field 'a' {\"a\": [{\"b\":\"c\"}]} {\"a\": [1]} {\"a\": [1]} Replace field 'a' with literal array Patch \u00b6 Patch operation grammar Patch expressions define a set of record level field operations to be applied to a target record in order to transform a targetted record. Patch allows fields to be: inserted where there was no field before; removed where there was a field before; updated where there was a field before; or inserted or updated regardless of whether or not there was a field before. Patch also allows field level merge operations on records or for the targetted document itself to be merged. Merge operations in patch are syntax sugar in that they are both based on the merge operation. Patch follows the semantics of RFC 6902 with the explicit exclusion of the copy and move operations and with the addition of an upsert operation the variant supported by tremor-script Example Expression Result Explanation let foo = {\"foo\":\"bar\"} patch foo of insert \"baz\" => \"qux\" end {\"foo\":\"bar\",\"baz\":\"qux\"} Add baz field let foo = {\"foo\":\"bar\",\"baz\":\"qux\"} patch foo of erase \"foo\" end {\"baz\":\"qux\"} Erase foo and add baz field let foo = {\"foo\":\"bar\"} patch foo of upsert \"foo\" => null end {\"foo\":null} Set foo to null, or reset to null if field already exists For comprehensions \u00b6 For Case Clause grammar For expressions are case-based record or array comprehensions that can iterate over index/element or key/value pairs in record or array literals respectively. Given our book store example from above: let wishlist = for store . book of case ( i , e ) => for e of of case ( k , v ) when k == \"price\" and v > 20.00 => { \"title\" : e . title , \"isbn\" : e . isbn } default => {} end end State \u00b6 As part of the tremor pipeline processing, there are times when it's necessary to track state across events over time (eg: in order to exploit stateful algorithms for session tracking, or building and maintaining application state). For this purpose, a tremor pipeline is equipped with operator node-level state management and storage capabilities that persists for the running lifetime of a pipeline deployed into the tremor runtime. From tremor-script, this shared storage is accessbile via the state keyword, which allows for accessing the storage contents via path expressions, akin to how the event keyword works (with the key difference being that the state storage is shared across events). On pipeline initialization, the state is initialized as null and users are free to set it to arbitrary value over the course of processing. Here's a tremor-script example demonstrating the usage of the state keyword -- it maintains a counter for the events coming in and emits the count alongside the event: match type :: is_null ( state ) of case true => let state = { \"count\" : 1 } default => let state . count = state . count + 1 end ; { \"count\" : state . count , \"event\" : event } This will work as part of the runtime::tremor operator confguration in the legacy pipeline yaml setup, and also as an embedded script in the trickle definition of the pipeline. A key thing to note is that by design, state is not shared across operator nodes in the pipeline. Therefore, if we have scripts across multiple nodes in the pipeline, the state keyword in each script allows access only to the local node-specific state storage, and not the state from any other operator nodes or something global to all the nodes. Since the state storage lives for the lifetime of a pipeline, state will not be persisted when the pipeline is undeployed or the host process is shut down. Extractors \u00b6 TEST_LITERAL Grammar: TEST_ESCAPE Grammar: The language has pluggable support for a number of microformats with two basic modes of operation that enable predicate tests ( does a particular value match the expected micro-format ) and elementization ( if a value does match a specific micro-format, then extract and elementize accordingly ). The general form of a supported micro-format is as follows: <name>|<format>| Where: name - The key for the micro-format being used for testing or extraction format - An optional multi-line micro-format specific format encoding used for testing and extraction Formats can be spread out over multiple lines by adding a \\ as a last character of the line. Spaces at the start of the line will be truncated by the lowest number of leading spaces. So if 3 lines respectively have 2, 4, and 7 spaces then 2 spaces are going to be removed from each line leaving 0, 2, and 5 spaces at the start. The set of supported micro-formats at the time of writing are as follows: Name Format Test mode Return type Extraction mode base64 Not required Tests if underlying value is a base64 encoded string string Performs a base64 decode, returning a UTF-8 encoded string glob Glob expression Tests if underlying value conforms to the supplied glob pattern string Returns the value that matches the glob ( identity extraction ) re PCRE regular expression with match groups Tests if underlying value conforms to supplied PCRE format record Extracts matched named values into a record cidr Plain IP or netmask Tests if underlying value conforms to cidr specification record Extracted numeric ip range, netmask and relevant information as a record kv Logstash KV specification Tests if the underlying value conforms to Logstash KV specification record Returns a key/value record dissect Logstash Dissect specification Tests if the underlying value conforms to Logstash Dissect specification record Returns a record of matching extractions based on supplied specification grok Logstash Grok specification Tests if the underlying value conforms to Logstash Grok specification record Returns a record of matching extractions based on supplied specification influx Not required Tests if the underlying value conforms to Influx line protocol specification record Returns an influx line protocol record matching extractions based on supplied specification json Not required Tests if the underlying value is json encoded depends on value Returns a hydrated tremor-script value upon extraction There is no concept of injector in the tremor-script language that is analogous to extractors. Where relevant the language supports functions that support the underlying operation ( such as base64 encoding ) and let expressions can be used for assignments. Additional Grammar Rules \u00b6 These rules are referenced in the main tremor-query grammar rules above and are listed here as extended reference. DocComment Grammar: DocCommentLine Grammar:","title":"Overview"},{"location":"tremor-script/#tremor-script","text":"The tremor-script scripting language is an interpreted expression-oriented language designed for the filtering, extraction, transformation and streaming of structured data in a stream or event-based processing system. At its core, tremor-script supports a structured type system equivalent to JSON. It supports integer, floating point, boolean and UTF-8 encoded string literals, literal arrays and associative dictionaries or record types in addition to a null marker. A well-formed JSON document is a legal tremor-script expression.","title":"Tremor-Script"},{"location":"tremor-script/#principles","text":"","title":"Principles"},{"location":"tremor-script/#safety","text":"The language is explicitly not Turing-complete: there are no unstructured goto grammar forms there are no unbounded for , while or do..while looping constructs the language is built on top of rust, inheriting its robustness and safety features, without the development overheads","title":"Safety"},{"location":"tremor-script/#developer-friendly","text":"The language adopts a Fortran-like syntax for key expression forms and has a path-like syntax for indexing into records and arrays","title":"Developer friendly"},{"location":"tremor-script/#stream-oriented-event-based","text":"Tremor-script is designed to process unstructured ( but well-formed ) data events. Event data can be JSON, MsgPack or any other form supported by the tremor event processing system.","title":"Stream-oriented / event-based"},{"location":"tremor-script/#self-documenting","text":"The fortran-like syntax allows rich operations to be computed against arbitrary JSON-like data. For example JSON documents can be patch ed and merge ed with operation and document based templates. Records and Arrays can be iterated over to transform them, merge them with other documents or to extract subsets for further processing","title":"Self-documenting"},{"location":"tremor-script/#extensibility","text":"The expression-based nature of tremor-script means that computational forms and any processing is transient. The language describes a set of rules ( expressions ) that process an inbound event document into an outbound documented emitted after evaluation of the script. The core expression language is designed for reuse in other script-based DSLs and can currently be extended through its modular function subsystem. The language also supports a pluggable data extraction model allowing base64 encoded, regular expressions and other micro-formats encoded within record fields to be referenced based on their internal structure and for subsets to be mapped accordingly. In the future, tremor-script may be retargeted as a JIT-compiled language.","title":"Extensibility"},{"location":"tremor-script/#performant","text":"Data ingested into tremor-script is vectorized via SIMD-parallel instructions on x86-64 or other Intel processor architectures supporting ssev3/avx extensions. Processing streams of such event-data incurs some allocation overhead at this time, but these event-bound allocations are being written out of the interpreter. The current meaning of performant as documented here means that tremor-script is more efficient at processing log-like data than the system it replaces ( logstash - which mixes extraction plugins such as grok and dissect with JRuby scripts and a terse configuration format )","title":"Performant"},{"location":"tremor-script/#productive","text":"The tremor-script parsing tool-chain has been designed with ease-of-debugging and ease-of-development in mind. It has buitin support for syntax-highlighting on the console with errors annotating highlighted sections of badly written scripts to simplify fixing such scripts.","title":"Productive"},{"location":"tremor-script/#language","text":"This section details the major components of the tremor-script language","title":"Language"},{"location":"tremor-script/#comments","text":"Comments in tremor-script are single-line comments that begin with a '#' symbol and continue until end of line. # I am a comment","title":"Comments"},{"location":"tremor-script/#literals","text":"Literal in tremor-script are equivalent to their sibling types supported by the JSON format.","title":"Literals"},{"location":"tremor-script/#null","text":"The null literal which represents the absence of a defined value null","title":"Null"},{"location":"tremor-script/#boolean","text":"Boolean literal. true false","title":"Boolean"},{"location":"tremor-script/#integer-numerics","text":"Integers in tremor-script are signed and are limited to 64-bit internal representation 404 The stdlib provides useful function for integers in std::integer . use std :: integer ; integer :: parse ( \"42\" ) == 42","title":"Integer Numerics"},{"location":"tremor-script/#floating-point-numerics","text":"Floating point numerics in tremor-script are signed and are limited to 64-bit IEEE representation 1.67 - e10 The stdlib provides useful function for floats in std::float .","title":"Floating-Point Numerics"},{"location":"tremor-script/#character-and-unicode-code-points","text":"The language does not support literal character or Unicode code-points at this time.","title":"Character and Unicode Code-points"},{"location":"tremor-script/#utf-8-encoded-strings","text":"\"I am a string\" The standard library provides useful function for string manipulation in std::string : use std :: string ; string :: uppercase ( string :: substr ( \"snotty\" , 0 , 4 )) == \"SNOT\"","title":"UTF-8 encoded Strings"},{"location":"tremor-script/#string-interpolation","text":"For strings tremor allows string interpolation, this means embedding code directly into strings to create strings out of them. \"I am a #{ \"string with #{ 1 } interpolation.\" } \" A hash sign followed by a curly bracket needs to be escaped \\#{ hash signs themselves do not need to be escaped.","title":"String Interpolation"},{"location":"tremor-script/#heredocs","text":"To deal with pre formatted strings in tremor script we allow for heredocs they are started by using triple quotes \"\"\" that terminate the line (aka \"\"\"bla isn't legal). Heredocs do not truncate leading indentation, only the first leading linebreak after the leading triple-quote \"\"\" stripped. \"\"\" I am a long multi-line string with #{ \" #{ 1 } interpolation\" } \"\"\" Since Tremor 0.9 Heredocs also support String Interpolation . A hash sign followed by a curly bracket needs to be escaped \\#{ hash signs themselves do not need to be escaped.","title":"HereDocs"},{"location":"tremor-script/#arrays","text":"Array grammar: Array literals in tremor-script are a comma-delimited set of expressions bracketed by the square brakcets '[' and ']'. [ 1 , 2 , \"foobar\" , 3.456e10 , { \"some\" : \"json-like-document\" } , null ] The standard library provides several useful functions to work with arrays in std::array : use std :: array ; array :: push ([ \"snot\" ], \"badger\" ) == [ \"snot\" , \"badger\" ]","title":"Arrays"},{"location":"tremor-script/#records","text":"Record grammar: Field grammar: Record literals in tremor-script are syntactically equivalent to JSON document objects { \"field1\" : \"value1\" , \"field2\" : [ \"value\" , \"value\" , \"value\" ], \"field3\" : { \"field4\" : \"field5\" } } Check out the stdlib std::record module for some helpful function for working with records.","title":"Records"},{"location":"tremor-script/#binary","text":"Binaries are based on the Erlang bit syntax . Binary grammar: BinaryFields grammar: BinaryField grammar: Parts of each field are: <value>:<size>/<type> where both size and type are optional. The binary types consists of up to three parts. That is 2 prefixes and 1 main type identifier. Examples: unsigned-big-integer , signed-integer , binary . The types currently supported are: binary - this can handle both binaries and strings, size here refers to the number of bytes integer - this can represent integers, size here means size in bits. In addition the type can be prefixed with big and little for indianness and signed and unsigned for signedness. Some examples would be: <<1:1, 42:7>> <<(1 + 1)/unsigned-big-integer>> <<1:4, \"badger\"/binary, -2:4/signed-little-integer>> We could construct a TCP package this way: # constructing a TCP package # using made up, non-correct values let event = { \"src\" : { \"port\" : 1234 } , \"dst\" : { \"port\" : 2345 } , \"seq\" : event , \"ack\" : 4567 , \"offset\" : 1 , \"res\" : 2 , \"flags\" : 3 , \"win\" : 4 , \"checksum\" : 5 , \"urgent\" : 6 , \"data\" : \"snot badger!\" } ; << event . src . port : 16 , event . dst . port : 16 , event . seq : 32 , event . ack : 32 , event . offset : 4 , event . res : 4 , event . flags : 8 , event . win : 16 , event . checksum : 16 , event . urgent : 16 , event . data / binary >> See also: std::binary for useful function for working with binary data. std::string::into_binary and std::string::from_utf8_lossy std::base64 for encoding and decoding binary data to string using base64.","title":"Binary"},{"location":"tremor-script/#operators","text":"List of binary and unary operators in tremor-script , ordered by precedence (from low to high): Symbol Name Example Types or Logical OR true or false bool and Logical AND true and false bool | Bitwise OR Bitwise OR has not been implemented yet - ^ Bitwise XOR 42 ^ 42, true ^ true integer, bool & Bitwise AND 42 & 0, true & false integer, bool ==, != Equality, Inequality \"snot\" != \"badger\" all <, <=, >, >= Comparison Operators 42 > 0 integer, float, string, binary <<, >>, >>> Bitwise shift -- Left, Right(signed), Right(unsigned) 42 >> 2 integer +, - Addition, Subtraction 42 + 0 integer, float, string *, /, % Multiplication, Division, Modulus 42 * 1 integer, float (no modulo) +, - Unary Plus, Unary Minus +42 integer, float, string not , ! Unary Logical NOT, Unary Bitwise NOT not false , Bitwise NOT has not been implemented yet bool","title":"Operators"},{"location":"tremor-script/#paths","text":"Path grammar: Qualified Segments grammar: PathSegment grammar: ArraySegment grammar: Path-like structures in tremor-script allow a subset of an ingested event, meta-data passed to the tremor-script function and script-local data to be indexed. Example event for illustration purposes: { \"store\" : { \"book\" : [ { \"category\" : \"reference\" , \"author\" : \"Nigel Rees\" , \"title\" : \"Sayings of the Century\" , \"price\" : 8.95 }, { \"category\" : \"fiction\" , \"author\" : \"Herman Melville\" , \"title\" : \"Moby Dick\" , \"isbn\" : \"0-553-21311-3\" , \"price\" : 8.99 }, { \"category\" : \"fiction\" , \"author\" : \"J.R.R. Tolkien\" , \"title\" : \"The Lord of the Rings\" , \"isbn\" : \"0-395-19395-8\" , \"price\" : 22.99 } ], \"bicycle\" : { \"color\" : \"red\" , \"price\" : 19.95 } }, \"expensive\" : 10 } Grab the entire event document: let capture = event ; Grab the books from the store (the same using key, index and escaped key notation for field lookup): let capture = event . store . book ; # index and escaped notation can acomodate keys that include 'odd' characters such as whitespaces or dots. let capture = event . store [ \"book\" ]; let capture = event . store .` book `; Grab the first book: let capture = event . store . book [ 0 ]; Grab the title of the 3rd book: let capture = event . store . book [ 2 ]. title Grab the range of books from 0 ( the first ) to 2 ( the last ), exclusive of the last book: let capture = event . store . book [ 0 : 2 ]; The type of a path is equivalent to the type of the data returned by a path expression. So in the above examples, a reference to a book title would return the value at that path, which in the reference event document is a string . Path's in tremor-script are themselves expressions in their own right.","title":"Paths"},{"location":"tremor-script/#const","text":"Const grammer: Const can be used to define immutable, constant values that get evaluated at compile time. This is more performant then let as all logic can happen at compile time and is helpful for setting up lookup tables or other never changing data structures.","title":"Const"},{"location":"tremor-script/#let","text":"Let grammar: The let expression allows data pointed to by a path to be destructively mutated, and the pointed-to value reassigned. If the path does not yet exist, it will be created in-situ: Set a local variable a to the literal integer value 10: let a = 10 ; Set a local variable a to be the ingested event record let a = event ; Set the metadata variable a to be the value of the local variable a : let $ a = a ;","title":"Let"},{"location":"tremor-script/#drop","text":"Drop expressions enable short-circuiting the evaluation of a tremor-script when badly formed data is discovered. If no argument is supplied, drop will return the event record. If an argument is supplied, the result of evaluating the expression will be returned. Tremor or other processing tools can process dropped events or data using purpose-built error-handling. As the content of the dropped event is user-defined, operators can standardise the format of the error emitted on drop from tremor-script drop ; drop ; # As the first drop always wins, this expression never runs","title":"Drop"},{"location":"tremor-script/#emit","text":"Emit grammar: Emit expressions enable short-circuiting the evaluation of a tremor-script when processing is known to be complete and further processing can be avoided. If no argument is supplied, emit` will return the event record. If an argument is supplied, the result of evaluating the expression will be returned. Tremor or other processing tools can process emitted events or data using their default flow-based or stream-based data processing pipelines. As the content of the emitted event is user-defined, oeprators can standardise the format of the event emitted on emit from tremor-script Note By default, if no emit or drop expressions are defined, all expressions in a correctly written tremor-script will be evaluated until completion and the value of the last expression evaluated will be returned as an emit message. Implicit emission: \"badgers\" # implicit emit Explicit emission of \"snot\" : \"badgers\" # literals do not short-circuit processing, so we continue to the next expression in this case emit \"snot\" emit \"oh noes!\" emit \"never happens\" ; # As the first emit always wins, this expression never runs There are times when it is necessary to emit synthetic events from tremor-script within a tremor pipeline to an alternate operator port than the default success route. For example, when data is well-formed but not valid and the data needs to be diverted into an alternate flow. The emit clause can be deployed for this purpose by specifying an optional named port. emit { \"event\" : event , \"status\" : \"malformed\" , \"description\" : \"required field `loglevel` is absent\" } => \"invalid\" ;","title":"Emit"},{"location":"tremor-script/#match","text":"Match grammar: Match case grammar: Match expressions enable data to be filtered or queried using case-based reasoning. Match expressions take the form: match < target > of case < case - expr > [ < guard > ] => < block > ... default => < block > end Where: target: An expression that is the target of case-based queries case-expr: A predicate test, literal value or pattern to match against guard: An optional predicate expression to gate whether or not an otherwise matching case-clause will in fact match block: The expressions to be evaluated if the case matches, and any supplied guard evaluates to true Examples: Discover if the store.book path is an array, record or scalar structure: match store . book of case %[] => let msg = \"store.book is an array-like data-structure\" , msg case % {} => \"store.book is a record-like data-structure\" default => \"store.book is a scalar data-type\" end Find all fiction books in the store: let found = match store . book of case fiction = %[ % { category ~= \"fiction\" } ] => fiction default => [] end ; emit found ;","title":"Match"},{"location":"tremor-script/#matching-literal-expressions","text":"The simplest form of case expression in match expressions is matching a literal value. Values can be any legal tremor-script type and they can be provided as literals, computed values or path references to local variables, metadata or values arriving via events. let example = match 12 of case 12 => \"matched\" default => drop \"not possible\" end ; let a = \"this is a\" ; let b = \" string\" ; let example = match a + b of case \"this is a string\" => \"matched\" default => drop \"not possible\" end ; let a = [ 1 , \"this is a string\" , { \"record-field\" : \"field-value\" } ]; match a of case a => a default => drop \"not possible\" end ;","title":"Matching literal expressions"},{"location":"tremor-script/#matching-on-test-predicate-expressions","text":"It is also possible to perform predicate based matching match \"this is not base64 encoded\" of case ~ base64 || => \"surprisingly, this is legal base64 data\" default => drop \"as suspected, this is not base64 encoded\" end ; These are often referred to informally as tilde expressions and tremor supports a variety of micro-formats that can be used for predicate or test-based matching such as logstash dissect, json, influx, perl-compatible regular expressions. Tilde expressions can under certain conditions elementize ( extract ) micro-format data. The elementization or extraction is covered in the Extractors section of this document and in the Extractor reference.","title":"Matching on test predicate expressions"},{"location":"tremor-script/#match-and-extract-expressions","text":"It is also possible to elementize or ingest supported micro-formats into tremor-script for further processing. For example, we can use the ~= and ~ operator to perform a predicate test, such as the base64 test in the previous example, which upon success, extracts ( in the base64 case, decoding ) a value for further processing. For example if we had an embedded JSON document in a string, we could test for the value being well-formed json, and extract the contents to a local variable as follows: let sneaky_json = \" { \\\" snot \\\" : \\\" badger \\\" } \" ; match sneaky_json of case json = ~ json || => json default => drop \"this is not the json we were looking for\" end ;","title":"Match and extract expressions"},{"location":"tremor-script/#matching-tuple-patterns","text":"Tip A tuple pattern matches a target value if the target is an array and each test matches the positionally correspondent value in the target . The target needs to be at least as long as the pattern but can be longer if the pattern ends with ... . If you are looking for a more set like operation look at the array pattern . Tuple Pattern grammar: Tuple Pattern filter grammar: In addition to literal array matching, where the case expression tuple literal must exactly match the target of the match expression one for one, tuple patterns enable testing for matching elements within an array and filtering on the basis of matched elements. let a = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ]; match a of case %( 0 ) => \"is a zero\" case %( 0 , .. ) => \"starts with a zero\" case %( _ , 1 , .. ) => \"has 1 one at index 1\" default => \"does not contain zero's\" end ;","title":"Matching tuple patterns"},{"location":"tremor-script/#matching-array-patterns","text":"Tip An array pattern matches a target value if the target is an array and each test in the pattern matches at least for one element in the target indiscriminate of their positions. If you are looking for a more array like / positional operation look at the tuple pattern . Array Pattern grammar: Array Pattern filter grammar: In addition to a subset match, where the elements of the pattern must be included in the target of the match expression, array patterns enable testing for matching elements within an array and filtering on the basis of matched elements. let a = [ 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 0 ]; match a of case %[ 0 ] => \"contains zero's\" default => \"does not contain zero's\" end ; Predicate matching against supported micro-formats is also supported in array pattern matching. let a = [ \"snot\" , \"snot badger\" , \"snot snot\" , \"badger badger\" , \"badger\" ]; match a of case got = %[ ~ re |^( P < hit > snot .*)$| ] => got default => \"not snotty at all\" end ;","title":"Matching array patterns"},{"location":"tremor-script/#matching-record-patterns","text":"Tip A record pattern matches a target if the target is a record that contains at least all declared keys and the tests for each of the declared key match. Record Pattern grammar Record Pattern Fields grammar Similarly to record literal matching where the case expression record must exactly match the target of the match expression, record patterns enable testing for matching fields or sub-structures within a record and extracting and elementizing data on the basis of matched predicate tests ( via ~= ). We can check for the presence of fields: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { present superhero , present human } => \"ok\" default => \"not possible\" end We can check for the absence of fields: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { absent superhero , absent human } => \"not possible\" default => \"ok\" end We can test the values of fields that are present: match { \"superhero\" : \"superman\" , \"human\" : \"clark kent\" } of case % { superhero == \"superman\" } => \"we are saved! \\o/\" case % { superhero != \"superman\" } => \"we may be saved! \\o/\" default => \"call 911\" end ; We can test for records within records: match { \"superhero\" : { \"name\" : \"superman\" } } of case % { superhero ~= % { present name } } => \"superman is super\" case % { superhero ~= % { absent name } } => \"anonymous superhero is anonymous\" default => \"something bad happened\" end ; We can also test for records within arrays within records tersely through nested pattern matching: match { \"superhero\" : [ { \"name\" : \"batman\" } , { \"name\" : \"robin\" } ] } of case id = % { superhero ~= %[ % { name ~= re |^(? P < kind > bat .*)$| } ] } => id default => \"something bad happened\" end ;","title":"Matching record patterns"},{"location":"tremor-script/#guard-clauses","text":"Guard expressions in Match case clauses enable matching data structures to be further filtered based on predicate expressions. For example they can be used to restrict the match to a subset of matching cases where appropriate. match event of case record = % {} when record . log_level == \"ERROR\" => \"error\" default => \"non-error\" end","title":"Guard clauses"},{"location":"tremor-script/#effectors","text":"Effectors grammar: Block: Effectors are the expressions evaluated when a case pattern and guard succeeded. Things are simple when using only a single expression as the match case effector. When we have to use multiple expressions to do some more complex processing, we need to separate those expressions with commas , : use std :: string ; match event of case record = % { present foo } => let foo_content = record [ \"foo\" ], let replaced = string :: replace ( foo_content , \"foo\" , \"bar\" ), let record [ \"foo\" ] = replaced default => null end","title":"Effectors"},{"location":"tremor-script/#merge","text":"Merge expressions defines a difference against a targetted record and applies that difference to produce a result record. Merge operations in tremor-script follow merge-semantics defined in RFC 7386 . let event = merge event of { \"some\" : \"record\" } end Given Merge Result Explanation {\"a\":\"b\"} {\"a\":\"c\"} {\"a\":\"c\"} Insert/Update field 'a' {\"a\":\"b\"} {\"b\":\"c\"} {\"a\":\"b\", \"b\":\"c\"} Insert field 'b' {\"a\":\"b\"} {\"a\":null} {} Erase field 'a' {\"a\":\"b\",\"b\":\"c\"} {\"a\":null} {\"b\":\"c\"} Erase field 'a' {\"a\": [{\"b\":\"c\"}]} {\"a\": [1]} {\"a\": [1]} Replace field 'a' with literal array","title":"Merge"},{"location":"tremor-script/#patch","text":"Patch operation grammar Patch expressions define a set of record level field operations to be applied to a target record in order to transform a targetted record. Patch allows fields to be: inserted where there was no field before; removed where there was a field before; updated where there was a field before; or inserted or updated regardless of whether or not there was a field before. Patch also allows field level merge operations on records or for the targetted document itself to be merged. Merge operations in patch are syntax sugar in that they are both based on the merge operation. Patch follows the semantics of RFC 6902 with the explicit exclusion of the copy and move operations and with the addition of an upsert operation the variant supported by tremor-script Example Expression Result Explanation let foo = {\"foo\":\"bar\"} patch foo of insert \"baz\" => \"qux\" end {\"foo\":\"bar\",\"baz\":\"qux\"} Add baz field let foo = {\"foo\":\"bar\",\"baz\":\"qux\"} patch foo of erase \"foo\" end {\"baz\":\"qux\"} Erase foo and add baz field let foo = {\"foo\":\"bar\"} patch foo of upsert \"foo\" => null end {\"foo\":null} Set foo to null, or reset to null if field already exists","title":"Patch"},{"location":"tremor-script/#for-comprehensions","text":"For Case Clause grammar For expressions are case-based record or array comprehensions that can iterate over index/element or key/value pairs in record or array literals respectively. Given our book store example from above: let wishlist = for store . book of case ( i , e ) => for e of of case ( k , v ) when k == \"price\" and v > 20.00 => { \"title\" : e . title , \"isbn\" : e . isbn } default => {} end end","title":"For comprehensions"},{"location":"tremor-script/#state","text":"As part of the tremor pipeline processing, there are times when it's necessary to track state across events over time (eg: in order to exploit stateful algorithms for session tracking, or building and maintaining application state). For this purpose, a tremor pipeline is equipped with operator node-level state management and storage capabilities that persists for the running lifetime of a pipeline deployed into the tremor runtime. From tremor-script, this shared storage is accessbile via the state keyword, which allows for accessing the storage contents via path expressions, akin to how the event keyword works (with the key difference being that the state storage is shared across events). On pipeline initialization, the state is initialized as null and users are free to set it to arbitrary value over the course of processing. Here's a tremor-script example demonstrating the usage of the state keyword -- it maintains a counter for the events coming in and emits the count alongside the event: match type :: is_null ( state ) of case true => let state = { \"count\" : 1 } default => let state . count = state . count + 1 end ; { \"count\" : state . count , \"event\" : event } This will work as part of the runtime::tremor operator confguration in the legacy pipeline yaml setup, and also as an embedded script in the trickle definition of the pipeline. A key thing to note is that by design, state is not shared across operator nodes in the pipeline. Therefore, if we have scripts across multiple nodes in the pipeline, the state keyword in each script allows access only to the local node-specific state storage, and not the state from any other operator nodes or something global to all the nodes. Since the state storage lives for the lifetime of a pipeline, state will not be persisted when the pipeline is undeployed or the host process is shut down.","title":"State"},{"location":"tremor-script/#extractors","text":"TEST_LITERAL Grammar: TEST_ESCAPE Grammar: The language has pluggable support for a number of microformats with two basic modes of operation that enable predicate tests ( does a particular value match the expected micro-format ) and elementization ( if a value does match a specific micro-format, then extract and elementize accordingly ). The general form of a supported micro-format is as follows: <name>|<format>| Where: name - The key for the micro-format being used for testing or extraction format - An optional multi-line micro-format specific format encoding used for testing and extraction Formats can be spread out over multiple lines by adding a \\ as a last character of the line. Spaces at the start of the line will be truncated by the lowest number of leading spaces. So if 3 lines respectively have 2, 4, and 7 spaces then 2 spaces are going to be removed from each line leaving 0, 2, and 5 spaces at the start. The set of supported micro-formats at the time of writing are as follows: Name Format Test mode Return type Extraction mode base64 Not required Tests if underlying value is a base64 encoded string string Performs a base64 decode, returning a UTF-8 encoded string glob Glob expression Tests if underlying value conforms to the supplied glob pattern string Returns the value that matches the glob ( identity extraction ) re PCRE regular expression with match groups Tests if underlying value conforms to supplied PCRE format record Extracts matched named values into a record cidr Plain IP or netmask Tests if underlying value conforms to cidr specification record Extracted numeric ip range, netmask and relevant information as a record kv Logstash KV specification Tests if the underlying value conforms to Logstash KV specification record Returns a key/value record dissect Logstash Dissect specification Tests if the underlying value conforms to Logstash Dissect specification record Returns a record of matching extractions based on supplied specification grok Logstash Grok specification Tests if the underlying value conforms to Logstash Grok specification record Returns a record of matching extractions based on supplied specification influx Not required Tests if the underlying value conforms to Influx line protocol specification record Returns an influx line protocol record matching extractions based on supplied specification json Not required Tests if the underlying value is json encoded depends on value Returns a hydrated tremor-script value upon extraction There is no concept of injector in the tremor-script language that is analogous to extractors. Where relevant the language supports functions that support the underlying operation ( such as base64 encoding ) and let expressions can be used for assignments.","title":"Extractors"},{"location":"tremor-script/#additional-grammar-rules","text":"These rules are referenced in the main tremor-query grammar rules above and are listed here as extended reference. DocComment Grammar: DocCommentLine Grammar:","title":"Additional Grammar Rules"},{"location":"tremor-script/extractors/","text":"Extractors \u00b6 The tremor-script language can recognize micro-formats and extract or elementize data from those micro-formats. There are two basic variants of this in the language: Predicate Form \u00b6 In the predicate form the ~ ( tilde ) operator performs a test to see if the associated micro-format pattern matches a supplied value. match event of case % { level ~ re |^ ERR .*$| } => \"is an error\" default => \"is not an error\" end In the above example a regular expression re extractor is tested ( by use of the ~ test operator ) against the pattern form delimited by | symbols. The field level ( that must exist on the event supplied, and requires that event is of record type ) is then tested against the regular expression. Extraction Form \u00b6 The extraction for is similar. In this case the predicate conditions must pass, but the regular expression can be written with match groups and named matches extracted into a key/value record for further processing. Usage \u00b6 Both the predicate and extraction form of extraction use operators with a ~ ( tilde ) operator. When ~= the micro-format in use ( a regular expression in our example ) acts as both a predicate test ( is it valid given the test specification ) and an extractor that elementizes and returns a subset of information from the micro-format. When only validity against the pattern is desired, then the extraction overhead can be eliminated or reduced depending on the implementation of the extractor configured. Extractors implicitly check if a field is present. If a field isn't present in the record, the predicate will fail and it will check the next predicate. Thus, no further explicit check such as present <field> is required. Note \u00b6 Extractors should not be followed by a function that uses the same field as the one in the extractor. This could result in unintended behaviour as the latter function might return the original value instead of the extracted value e.g. match { \"superman\" = \"message_key: ruler: batman\" } of case r = % {superman ~= grok |(?< message_key >(.\\|\\\\ n ) { 0 , 200 } )|, present superman} => let event . new_ruler = r . superman . name default => \"switch to marvel\" end ; This will result in unintended behaviour because present will return the original string instead of the record extracted by grok. Available extractors \u00b6 The different extractors available are: Base64 CIDR Datetime Dissect Glob Grok Influx JSON KV Regex (Re)","title":"Overview"},{"location":"tremor-script/extractors/#extractors","text":"The tremor-script language can recognize micro-formats and extract or elementize data from those micro-formats. There are two basic variants of this in the language:","title":"Extractors"},{"location":"tremor-script/extractors/#predicate-form","text":"In the predicate form the ~ ( tilde ) operator performs a test to see if the associated micro-format pattern matches a supplied value. match event of case % { level ~ re |^ ERR .*$| } => \"is an error\" default => \"is not an error\" end In the above example a regular expression re extractor is tested ( by use of the ~ test operator ) against the pattern form delimited by | symbols. The field level ( that must exist on the event supplied, and requires that event is of record type ) is then tested against the regular expression.","title":"Predicate Form"},{"location":"tremor-script/extractors/#extraction-form","text":"The extraction for is similar. In this case the predicate conditions must pass, but the regular expression can be written with match groups and named matches extracted into a key/value record for further processing.","title":"Extraction Form"},{"location":"tremor-script/extractors/#usage","text":"Both the predicate and extraction form of extraction use operators with a ~ ( tilde ) operator. When ~= the micro-format in use ( a regular expression in our example ) acts as both a predicate test ( is it valid given the test specification ) and an extractor that elementizes and returns a subset of information from the micro-format. When only validity against the pattern is desired, then the extraction overhead can be eliminated or reduced depending on the implementation of the extractor configured. Extractors implicitly check if a field is present. If a field isn't present in the record, the predicate will fail and it will check the next predicate. Thus, no further explicit check such as present <field> is required.","title":"Usage"},{"location":"tremor-script/extractors/#note","text":"Extractors should not be followed by a function that uses the same field as the one in the extractor. This could result in unintended behaviour as the latter function might return the original value instead of the extracted value e.g. match { \"superman\" = \"message_key: ruler: batman\" } of case r = % {superman ~= grok |(?< message_key >(.\\|\\\\ n ) { 0 , 200 } )|, present superman} => let event . new_ruler = r . superman . name default => \"switch to marvel\" end ; This will result in unintended behaviour because present will return the original string instead of the record extracted by grok.","title":"Note"},{"location":"tremor-script/extractors/#available-extractors","text":"The different extractors available are: Base64 CIDR Datetime Dissect Glob Grok Influx JSON KV Regex (Re)","title":"Available extractors"},{"location":"tremor-script/functions/","text":"Functions \u00b6 Tremor-script provides access to a growing number of functions that allow advanced data manipulation or access to additional information. Functions are namespaced to make identification easier. Tremor also supports user defined functions. There are a few noteworthy restrictions: Functions are pure / side effect free - you can not mutate event , state , or $ inside of a function. Functions have to return a value, as tremor-script is expression oriented. Functions can only be defined once, even if they take different forms or arguments. Function overloading is not supported. In matching functions, a default case is required. Functions can call other functions but they have to be a priori defined. The order of definitions is significant. Tail recursion is supported, and constrained to a maximum recursion depth. A recursion depth is imposed as tremor-script is designed to operate on infinite streams of data so indefinite blocking/recursion is not supportable by design. Function Declaration Grammar: Lets look at the types of functions we have. Intrinsic Functions \u00b6 Intrinsic functions represent builtin or pre-defined functions implemented in the rust programming language that are a builtin component of the tremor project and are provided out of the box. The function reference in this documentation set, for example, is generated from the documentation provided in the standard library. The standard library is primarily composed of intrinsic or bulitin functions. Standard functions \u00b6 Standard functions are functions that take a given number of arguments, each with a name. This function can be tail-recursive. An example would be: ## This function adds two values together fn add ( a , b ) with a + b end Variable arguments \u00b6 It is possible to use variable arguments by appending a final ... ellipsis in a standard function definition. The anonymous arguments will be provided via the args keyword. Recursion \u00b6 Tail-recursion is provided for fixed arity ( number of arguments ) tandard functions. Tremor imposes a restriction in recursion depth. As tremor is an event processing system it is not desirable to have long running functions that block events from being processed through the system. use std :: array ; fn sum_ ( e , es ) with let l = array :: len ( es ); match l of case l when l > 0 => let a = es [ 0 ], recur ( e + es [ 0 ], es [ 1 : l ]) default => e end end ; fn sum (...) with sum_ ( 0 , args ) end Match functions \u00b6 Since matching and extracting are a core functionality for tremor matching on function arguments is directly supported. The same patterns that are used in match can be used in function cases including extractors. If any extracting pattern is used and matches the function argument will be replaced by the result of the extraction. ## calculates the fibonaci sequence fn fib_ ( a , b , n ) of case ( a , b , n ) when n > 0 => recur ( b , a + b , n - 1 ) default => a end ; ## calculates the fibonaci sequence fn fib ( n ) with fib_ ( 0 , 1 , n ) end ;","title":"Overview"},{"location":"tremor-script/functions/#functions","text":"Tremor-script provides access to a growing number of functions that allow advanced data manipulation or access to additional information. Functions are namespaced to make identification easier. Tremor also supports user defined functions. There are a few noteworthy restrictions: Functions are pure / side effect free - you can not mutate event , state , or $ inside of a function. Functions have to return a value, as tremor-script is expression oriented. Functions can only be defined once, even if they take different forms or arguments. Function overloading is not supported. In matching functions, a default case is required. Functions can call other functions but they have to be a priori defined. The order of definitions is significant. Tail recursion is supported, and constrained to a maximum recursion depth. A recursion depth is imposed as tremor-script is designed to operate on infinite streams of data so indefinite blocking/recursion is not supportable by design. Function Declaration Grammar: Lets look at the types of functions we have.","title":"Functions"},{"location":"tremor-script/functions/#intrinsic-functions","text":"Intrinsic functions represent builtin or pre-defined functions implemented in the rust programming language that are a builtin component of the tremor project and are provided out of the box. The function reference in this documentation set, for example, is generated from the documentation provided in the standard library. The standard library is primarily composed of intrinsic or bulitin functions.","title":"Intrinsic Functions"},{"location":"tremor-script/functions/#standard-functions","text":"Standard functions are functions that take a given number of arguments, each with a name. This function can be tail-recursive. An example would be: ## This function adds two values together fn add ( a , b ) with a + b end","title":"Standard functions"},{"location":"tremor-script/functions/#variable-arguments","text":"It is possible to use variable arguments by appending a final ... ellipsis in a standard function definition. The anonymous arguments will be provided via the args keyword.","title":"Variable arguments"},{"location":"tremor-script/functions/#recursion","text":"Tail-recursion is provided for fixed arity ( number of arguments ) tandard functions. Tremor imposes a restriction in recursion depth. As tremor is an event processing system it is not desirable to have long running functions that block events from being processed through the system. use std :: array ; fn sum_ ( e , es ) with let l = array :: len ( es ); match l of case l when l > 0 => let a = es [ 0 ], recur ( e + es [ 0 ], es [ 1 : l ]) default => e end end ; fn sum (...) with sum_ ( 0 , args ) end","title":"Recursion"},{"location":"tremor-script/functions/#match-functions","text":"Since matching and extracting are a core functionality for tremor matching on function arguments is directly supported. The same patterns that are used in match can be used in function cases including extractors. If any extracting pattern is used and matches the function argument will be replaced by the result of the extraction. ## calculates the fibonaci sequence fn fib_ ( a , b , n ) of case ( a , b , n ) when n > 0 => recur ( b , a + b , n - 1 ) default => a end ; ## calculates the fibonaci sequence fn fib ( n ) with fib_ ( 0 , 1 , n ) end ;","title":"Match functions"},{"location":"tremor-script/modules/","text":"Modules \u00b6 Tremor-script supports nested namespaces or modules. Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file, nesting is via the mod clause. Modules can define const constants, fn functions, or nested mod sub-modules. Module Path \u00b6 Modules can be defined physically on the file system. For example given the following modular hierarchy on the file system, relative to a root module path: Nested modules can be defined as follows: +-- foo +-- bar +-- snot.tremor +-- baz +-- badger.tremor Assuming this module hierarchy is rooted at /opt/my-project/lib they can be registered with tremor by setting the TREMOR_PATH environment variable export TREMOR_PATH = /opt/my-project/lib The TREMOR_PATH uses ':' on linux/unix to separate multiple module paths. The modules can be used using the use clause as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.tremor' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.tremor' \" #{snot :: snot}#{badger :: badger} \" ; # emits an interpolated string The same modular hierarchy can be defined as nested module declarations as follows: mod foo with mod bar with const snot = \"beep\" ; end ; mod baz with const badger = \"boop\" ; end ; end ; let snot = foo :: bar :: snot ; let badger = foo :: baz :: badger ; \" #{snot} - #{badger} \" ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; \"Hello #{fleek :: snot} \" It is to be noted that inclusion via use will prevent circular inclusion as in file a.tremor can use b.tremor but beyond that point b.tremor can no longer use a.tremor as this would create a dependency cycle. This is a restriction of the current implementation and may or may not be relaxed in the future.","title":"Modules"},{"location":"tremor-script/modules/#modules","text":"Tremor-script supports nested namespaces or modules. Modules in tremor are the lowest unit of compilation available to developers to modularise tremor logic across multiple logical namespaces. On the filesystem, modules are rooted at a base path and are nested with folders. Within a file, nesting is via the mod clause. Modules can define const constants, fn functions, or nested mod sub-modules.","title":"Modules"},{"location":"tremor-script/modules/#module-path","text":"Modules can be defined physically on the file system. For example given the following modular hierarchy on the file system, relative to a root module path: Nested modules can be defined as follows: +-- foo +-- bar +-- snot.tremor +-- baz +-- badger.tremor Assuming this module hierarchy is rooted at /opt/my-project/lib they can be registered with tremor by setting the TREMOR_PATH environment variable export TREMOR_PATH = /opt/my-project/lib The TREMOR_PATH uses ':' on linux/unix to separate multiple module paths. The modules can be used using the use clause as follows: use foo :: bar :: snot ; # snot is a ref to 'foo/bar/snot.tremor' use foo :: baz :: badger ; # badger is a ref to 'foo/bar/badger.tremor' \" #{snot :: snot}#{badger :: badger} \" ; # emits an interpolated string The same modular hierarchy can be defined as nested module declarations as follows: mod foo with mod bar with const snot = \"beep\" ; end ; mod baz with const badger = \"boop\" ; end ; end ; let snot = foo :: bar :: snot ; let badger = foo :: baz :: badger ; \" #{snot} - #{badger} \" ; Modules can be loaded via the use clause which in turn loads a module from the physical file system via the module path. Inline and externalized modules can be used separately or together as appropriate. Where there are existing references a module can be aliased to avoid clashes in the local scope: use foo :: bar as fleek ; \"Hello #{fleek :: snot} \" It is to be noted that inclusion via use will prevent circular inclusion as in file a.tremor can use b.tremor but beyond that point b.tremor can no longer use a.tremor as this would create a dependency cycle. This is a restriction of the current implementation and may or may not be relaxed in the future.","title":"Module Path"},{"location":"tremor-script/pp/","text":"Lexical Preprocessor \u00b6 In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users. Directives \u00b6 The preprocessor has two directives The #!line directive The #!config directive Line directive \u00b6 This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line directive is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future. Config directive \u00b6 This directive allows compile-time configuration parameters to be passed into tremor Example preprocessed tremor-script \u00b6 #!line 0 0 0 1 ./foo/bar/snot.tremor mod snot with #!line 0 0 0 1 ./foo/bar/snot.tremor const snot = \"beep\" ; end ; #!line 19 1 0 0 script.tremor #!line 0 0 0 2 ./foo/baz/badger.tremor mod badger with #!line 0 0 0 2 ./foo/baz/badger.tremor const badger = \"boop\" ; end ; #!line 41 1 0 0 script.tremor let c = \" #{snot :: snot}#{badger :: badger} \" ; emit c","title":"Lexical Preprocessor"},{"location":"tremor-script/pp/#lexical-preprocessor","text":"In order to support the module mechanism with minimal changes to the API and runtime, a preprocessor loads all externally referenced modules used in tremor logic defined in tremor-script or tremor-query and loads them inline into a preprocessed file. It is an error to attempt to deploy a tremor-script or tremor-query file that uses the module mechanism as source. The API only accepts non-modular files for backward compatibility or preprocessed files. The latter constraint is to ensure that logic deployed into the runtime is always traceable to source loaded by a user. Tremor explicitly avoids possibilities of modular logic changing at runtime. The preprocessor defends this guarantee on behalf of our users.","title":"Lexical Preprocessor"},{"location":"tremor-script/pp/#directives","text":"The preprocessor has two directives The #!line directive The #!config directive","title":"Directives"},{"location":"tremor-script/pp/#line-directive","text":"This directive tells the preprocessor that it is now in a logically different position of the file. For each folder/directory that an included source traverses a module statement is injected into the consolidated source. The #!line directive is a implementation detail mentioned here for the same of completeness and not meant to be used or relied on by end users. It may, without prior warning, be removed in the future.","title":"Line directive"},{"location":"tremor-script/pp/#config-directive","text":"This directive allows compile-time configuration parameters to be passed into tremor","title":"Config directive"},{"location":"tremor-script/pp/#example-preprocessed-tremor-script","text":"#!line 0 0 0 1 ./foo/bar/snot.tremor mod snot with #!line 0 0 0 1 ./foo/bar/snot.tremor const snot = \"beep\" ; end ; #!line 19 1 0 0 script.tremor #!line 0 0 0 2 ./foo/baz/badger.tremor mod badger with #!line 0 0 0 2 ./foo/baz/badger.tremor const badger = \"boop\" ; end ; #!line 41 1 0 0 script.tremor let c = \" #{snot :: snot}#{badger :: badger} \" ; emit c","title":"Example preprocessed tremor-script"},{"location":"tremor-script/recipes/","text":"Recipes \u00b6 This document provides a few recipes for common patterns in tremor script. Please note however that it neither is exhaustive nor should those patterns considered the 'only way' to perform certain tasks. Extracting a raw message \u00b6 If the event is a unstructured / raw message parsing it can be tricky since we can not match over a string. The following code offers a solution to it: # event = \"John Doe\" let event = match { \"message\" : event } of case r = % { message ~= dissect |% {first} % {last} | } => r . message default => drop end ; # event = {\"first\" : \"John\", \"last\": \"Doe\"} Appending to an array \u00b6 When appending to an array we can use the array::push function use std :: array ; # event = {\"key\": \"value\", \"tags\": [\"tag1\"]} let event . tags = array :: push ( event . tags , \"tag2\" ); # event = {\"key\": \"value\", \"tags\": [\"tag1\", \"tag2\"]} Validating over extracted data \u00b6 Sometimes we want to validate over extracted data without forcing the extraction to be a regular expression. For validations like the one below this pattern can be used. use std :: array ; match event of # ... case r = % {message ~= dissect |% {log_level} % {log_timestamp} : % {logger} : % {message} | } when array :: contains ([ \"ERROR\" , \"WARN\" , \"INFO\" , \"DEBUG\" ], result . message . log_level ) => let event = merge event of r . message end # ... end Here we extract the log_level and validate of that the it is one of ERROR , WARN , INFO or DEBUG by moving the check into the when guard we don't need to use a regular expression for this validation instead can use array membership. Replacing a field with an extraction \u00b6 When extracting a field to merge with with the event and wanting to remove the extracted field we can take advantage of the merge expressions behaviour that it will treat null in merged records as a command to delete the data by setting the field to replace to null before merging. # event = %{\"message\": \"John Doe\"} let event = merge event of match event of case r = % {message ~= dissect |% {first} % {last} | } => let r = r . message , let r . message = null , r default => {} end ; # event = %{\"first\": \"John\", \"last\": \"Doe\"} No effect on non matching case \u00b6 If we use merge with match we can make the default case to have no effect by using {} . This is possible since merge on {} is a identity function. # event = %{\"message\": \"John Doe\"} let event = merge event of match event of case r = % {message ~= dissect |% {first} % {midle} % {last} | } => let r = r . message , let r . message = null , r default => {} end ; # event = %{\"message\": \"John Doe\"} Boolean decisions \u00b6 To make boolean decisions we can match on true or false . use std :: type ; match type :: is_record ( event ) of case true => let event_ type = \"record\" case false => let event_ type = \"other\" end Diverting an event to a different channel \u00b6 By default the tremor-script operator forwards all events that are not dropped to the out channel for further processing. However it is possible to route events to different channels using the emit keyword. This allows, for example, diverting certain events to reserve bandwidth for a more important subset. match event . importance of case \"high\" => emit # this is the same as emit event => \"out\" case \"medium\" => emit event => \"divert\" default => drop # deletes the event end The 'null default' \u00b6 When the result of a match statement isn't needed - as in we use it purely for it's side effects - and we want the default to have no effect we can use null here. match event of case % { tags ~= [ \"high-priority\" ] } => let event . importance = \"high\" default => null end Testing against the type of a field \u00b6 Sometimes we want to know if a field has a certain type. The type module provides help here but common types such as record or array can be checked using their patterns. match event of case % {field ~= % {}} => emit \"event.field is a record\" case % {field ~= %[] } => emit \"event.field is a array\" case % {} when type :: is_record ( event . field ) => emit \"event.field is a record\" case % {} when type :: is_array ( event . field ) => emit \"event.field is a array\" case % {} when type :: is_number ( event . field ) => emit \"event.field is a number (float or integer)\" case % {} when type :: is_integer ( event . field ) => emit \"event.field is an integer\" case % {} when type :: is_float ( event . field ) => emit \"event.field is a float\" case % {} when type :: is_null ( event . field ) => emit \"event.field is null (but set)\" case % { absent field} => emit \"event.field is not set\" # ... end Routing messages \u00b6 Tremor script can be used to route messages by combining the emit feature and the fact that the tremor runtime operator allows different outputs. To route to doing a blue / green split based on a field in a record we could use the following code: match event of case % {key == \"blue\" } => emit event => \"blue\" case % {key == \"green\" } => emit event => \"green\" default => drop end define script split_script script # see above end; create script split_script; select event from split_script/blue into out/blue; select event from split_script/green into out/green; Percentage drops of events \u00b6 To drop a percentage of all events, functions in the random module can be used. We generate a random number in a range and based on the outcome, we decide whether we want to drop an event or not. Example: # drop 50% of the events match random :: integer ( 0 , 100 ) < 50 of case true => drop default => null end Most of the time, we want to do this only for certain matching events (as opposed to all events). let random_number = random :: integer ( 0 , 100 ); match event of case % {key == \"blue\" } when random_number < 25 => drop # drop 25% of blue events case % {key == \"yellow\" } when random_number < 75 => drop # drop 75% of yellow events case % {key == \"red\" } => drop # drop 100% of red events default => null # drop 0% of other events end Check if a variable is present/absent \u00b6 To check if a variable is present, we can rely on the present keyword (and inversely, absent ). # matches default case match present non_existent_var of case true => \"is present\" default => \"not present\" end ; Note that this is different from the case where a variable is set to null , for which we can do function-based checks as well as pattern-match with match . Using non-existent variables in contexts other than present or absent will throw an error terminating the script, so this is useful for guarding against that when needed. This is especially useful when working with meta variables as part of tremor runtime, where -- as part of a pipeline node -- we may need to check if a certain meta variable is set or not (eg: from a previous pipeline node) and act accordingly. For such needs, the approach above can be used. Alternatively, we can also rely on record patterns there: # tests for presence of $key match $ of case % { present key} => \"present\" default => \"not present\" end ; Since $ gives a record with all the meta variable name-value mapping, this works nicely.","title":"Recipes"},{"location":"tremor-script/recipes/#recipes","text":"This document provides a few recipes for common patterns in tremor script. Please note however that it neither is exhaustive nor should those patterns considered the 'only way' to perform certain tasks.","title":"Recipes"},{"location":"tremor-script/recipes/#extracting-a-raw-message","text":"If the event is a unstructured / raw message parsing it can be tricky since we can not match over a string. The following code offers a solution to it: # event = \"John Doe\" let event = match { \"message\" : event } of case r = % { message ~= dissect |% {first} % {last} | } => r . message default => drop end ; # event = {\"first\" : \"John\", \"last\": \"Doe\"}","title":"Extracting a raw message"},{"location":"tremor-script/recipes/#appending-to-an-array","text":"When appending to an array we can use the array::push function use std :: array ; # event = {\"key\": \"value\", \"tags\": [\"tag1\"]} let event . tags = array :: push ( event . tags , \"tag2\" ); # event = {\"key\": \"value\", \"tags\": [\"tag1\", \"tag2\"]}","title":"Appending to an array"},{"location":"tremor-script/recipes/#validating-over-extracted-data","text":"Sometimes we want to validate over extracted data without forcing the extraction to be a regular expression. For validations like the one below this pattern can be used. use std :: array ; match event of # ... case r = % {message ~= dissect |% {log_level} % {log_timestamp} : % {logger} : % {message} | } when array :: contains ([ \"ERROR\" , \"WARN\" , \"INFO\" , \"DEBUG\" ], result . message . log_level ) => let event = merge event of r . message end # ... end Here we extract the log_level and validate of that the it is one of ERROR , WARN , INFO or DEBUG by moving the check into the when guard we don't need to use a regular expression for this validation instead can use array membership.","title":"Validating over extracted data"},{"location":"tremor-script/recipes/#replacing-a-field-with-an-extraction","text":"When extracting a field to merge with with the event and wanting to remove the extracted field we can take advantage of the merge expressions behaviour that it will treat null in merged records as a command to delete the data by setting the field to replace to null before merging. # event = %{\"message\": \"John Doe\"} let event = merge event of match event of case r = % {message ~= dissect |% {first} % {last} | } => let r = r . message , let r . message = null , r default => {} end ; # event = %{\"first\": \"John\", \"last\": \"Doe\"}","title":"Replacing a field with an extraction"},{"location":"tremor-script/recipes/#no-effect-on-non-matching-case","text":"If we use merge with match we can make the default case to have no effect by using {} . This is possible since merge on {} is a identity function. # event = %{\"message\": \"John Doe\"} let event = merge event of match event of case r = % {message ~= dissect |% {first} % {midle} % {last} | } => let r = r . message , let r . message = null , r default => {} end ; # event = %{\"message\": \"John Doe\"}","title":"No effect on non matching case"},{"location":"tremor-script/recipes/#boolean-decisions","text":"To make boolean decisions we can match on true or false . use std :: type ; match type :: is_record ( event ) of case true => let event_ type = \"record\" case false => let event_ type = \"other\" end","title":"Boolean decisions"},{"location":"tremor-script/recipes/#diverting-an-event-to-a-different-channel","text":"By default the tremor-script operator forwards all events that are not dropped to the out channel for further processing. However it is possible to route events to different channels using the emit keyword. This allows, for example, diverting certain events to reserve bandwidth for a more important subset. match event . importance of case \"high\" => emit # this is the same as emit event => \"out\" case \"medium\" => emit event => \"divert\" default => drop # deletes the event end","title":"Diverting an event to a different channel"},{"location":"tremor-script/recipes/#the-null-default","text":"When the result of a match statement isn't needed - as in we use it purely for it's side effects - and we want the default to have no effect we can use null here. match event of case % { tags ~= [ \"high-priority\" ] } => let event . importance = \"high\" default => null end","title":"The 'null default'"},{"location":"tremor-script/recipes/#testing-against-the-type-of-a-field","text":"Sometimes we want to know if a field has a certain type. The type module provides help here but common types such as record or array can be checked using their patterns. match event of case % {field ~= % {}} => emit \"event.field is a record\" case % {field ~= %[] } => emit \"event.field is a array\" case % {} when type :: is_record ( event . field ) => emit \"event.field is a record\" case % {} when type :: is_array ( event . field ) => emit \"event.field is a array\" case % {} when type :: is_number ( event . field ) => emit \"event.field is a number (float or integer)\" case % {} when type :: is_integer ( event . field ) => emit \"event.field is an integer\" case % {} when type :: is_float ( event . field ) => emit \"event.field is a float\" case % {} when type :: is_null ( event . field ) => emit \"event.field is null (but set)\" case % { absent field} => emit \"event.field is not set\" # ... end","title":"Testing against the type of a field"},{"location":"tremor-script/recipes/#routing-messages","text":"Tremor script can be used to route messages by combining the emit feature and the fact that the tremor runtime operator allows different outputs. To route to doing a blue / green split based on a field in a record we could use the following code: match event of case % {key == \"blue\" } => emit event => \"blue\" case % {key == \"green\" } => emit event => \"green\" default => drop end define script split_script script # see above end; create script split_script; select event from split_script/blue into out/blue; select event from split_script/green into out/green;","title":"Routing messages"},{"location":"tremor-script/recipes/#percentage-drops-of-events","text":"To drop a percentage of all events, functions in the random module can be used. We generate a random number in a range and based on the outcome, we decide whether we want to drop an event or not. Example: # drop 50% of the events match random :: integer ( 0 , 100 ) < 50 of case true => drop default => null end Most of the time, we want to do this only for certain matching events (as opposed to all events). let random_number = random :: integer ( 0 , 100 ); match event of case % {key == \"blue\" } when random_number < 25 => drop # drop 25% of blue events case % {key == \"yellow\" } when random_number < 75 => drop # drop 75% of yellow events case % {key == \"red\" } => drop # drop 100% of red events default => null # drop 0% of other events end","title":"Percentage drops of events"},{"location":"tremor-script/recipes/#check-if-a-variable-is-presentabsent","text":"To check if a variable is present, we can rely on the present keyword (and inversely, absent ). # matches default case match present non_existent_var of case true => \"is present\" default => \"not present\" end ; Note that this is different from the case where a variable is set to null , for which we can do function-based checks as well as pattern-match with match . Using non-existent variables in contexts other than present or absent will throw an error terminating the script, so this is useful for guarding against that when needed. This is especially useful when working with meta variables as part of tremor runtime, where -- as part of a pipeline node -- we may need to check if a certain meta variable is set or not (eg: from a previous pipeline node) and act accordingly. For such needs, the approach above can be used. Alternatively, we can also rely on record patterns there: # tests for presence of $key match $ of case % { present key} => \"present\" default => \"not present\" end ; Since $ gives a record with all the meta variable name-value mapping, this works nicely.","title":"Check if a variable is present/absent"},{"location":"tremor-script/extractors/base64/","text":"Base64 \u00b6 The base64 extractor decodes content encoded with the Base64 binary-to-text encoding scheme into the corresponding string value. Predicate \u00b6 When used as a predicate test with ~ , and the referent target is a valid string and is base64 encoded, then the test succeeds. Extraction \u00b6 If predicate test succeeds, then the decoded base64 content it extracted and returned as a string literal. Example \u00b6 match { \"test\" : \"8J+MiiBzbm90IGJhZGdlcg==\" , \"footle\" :\u00b7 \"bar\" } of case foo = % {test ~= base64 || } => foo end ; ## Output: \ud83c\udf0a snot badger","title":"Base64"},{"location":"tremor-script/extractors/base64/#base64","text":"The base64 extractor decodes content encoded with the Base64 binary-to-text encoding scheme into the corresponding string value.","title":"Base64"},{"location":"tremor-script/extractors/base64/#predicate","text":"When used as a predicate test with ~ , and the referent target is a valid string and is base64 encoded, then the test succeeds.","title":"Predicate"},{"location":"tremor-script/extractors/base64/#extraction","text":"If predicate test succeeds, then the decoded base64 content it extracted and returned as a string literal.","title":"Extraction"},{"location":"tremor-script/extractors/base64/#example","text":"match { \"test\" : \"8J+MiiBzbm90IGJhZGdlcg==\" , \"footle\" :\u00b7 \"bar\" } of case foo = % {test ~= base64 || } => foo end ; ## Output: \ud83c\udf0a snot badger","title":"Example"},{"location":"tremor-script/extractors/cidr/","text":"CIDR \u00b6 Classless Inter-Domain Routing ( CIDR ) is a method of allocating IP addresses and IP routing paths. CIDR notation is a compact representation of an IP address and its associated routing prefix. The notation is constructed from a possibly incomplete IP address, followed by a slash ( / ) character, and then a decimal number. The number is the count of leading 1 bits in the subnet mask demarcating routing boundaries for the range of addresses being routed over. Larger values here indicate smaller networks. The maximum size of the network is given by the number of addresses that are possible with the remaining, least-significant bits below the prefix. Predicate \u00b6 When used as a predicate test with ~ , and no arguments are provided, then any valid IP address will pass the predicate test. When used as a predicate test with ~ , with one or many command delimited CIDR forms, then any valid IP address must be within the specified set of CIDR patterns for the predicate to pass. Pattern forms may be based on IPv4 or IPv6. Extraction \u00b6 When used as an extraction operation with ~= the predicate test must pass for successful extraction. If the predicate test succeeds, then the the prefix and network mask for each CIDR is provided as a key/value record with the original pattern form as key. Examples \u00b6 match { \"meta\" : \"192.168.1.1\" } of case rp = % { meta ~= cidr || } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 192 , 168 , 1 , 1 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } Cidr also supports filtering the IP if it is within the CIDR specified. This errors out if the IP Address specified isn't in the range of the CIDR. Otherwise, it will return the prefix & mask as above. match { \"meta\" : \"10.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 | } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 10 , 22 , 0 , 254 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } The extractor also supports multiple comma-separated ranges. This will return the prefix and mask if it belongs to any one of the CIDRs specified: match { \"meta\" : \"10.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 , 10.21.1.0 / 24 | } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 10 , 22 , 0 , 254 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } In case it doesn't belong to the CIDR: match { \"meta\" : \"19.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 , 10.21.1.0 / 24 | } => rp default => \"no match\" end ; ## Output: \"no match\"","title":"CIDR"},{"location":"tremor-script/extractors/cidr/#cidr","text":"Classless Inter-Domain Routing ( CIDR ) is a method of allocating IP addresses and IP routing paths. CIDR notation is a compact representation of an IP address and its associated routing prefix. The notation is constructed from a possibly incomplete IP address, followed by a slash ( / ) character, and then a decimal number. The number is the count of leading 1 bits in the subnet mask demarcating routing boundaries for the range of addresses being routed over. Larger values here indicate smaller networks. The maximum size of the network is given by the number of addresses that are possible with the remaining, least-significant bits below the prefix.","title":"CIDR"},{"location":"tremor-script/extractors/cidr/#predicate","text":"When used as a predicate test with ~ , and no arguments are provided, then any valid IP address will pass the predicate test. When used as a predicate test with ~ , with one or many command delimited CIDR forms, then any valid IP address must be within the specified set of CIDR patterns for the predicate to pass. Pattern forms may be based on IPv4 or IPv6.","title":"Predicate"},{"location":"tremor-script/extractors/cidr/#extraction","text":"When used as an extraction operation with ~= the predicate test must pass for successful extraction. If the predicate test succeeds, then the the prefix and network mask for each CIDR is provided as a key/value record with the original pattern form as key.","title":"Extraction"},{"location":"tremor-script/extractors/cidr/#examples","text":"match { \"meta\" : \"192.168.1.1\" } of case rp = % { meta ~= cidr || } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 192 , 168 , 1 , 1 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } Cidr also supports filtering the IP if it is within the CIDR specified. This errors out if the IP Address specified isn't in the range of the CIDR. Otherwise, it will return the prefix & mask as above. match { \"meta\" : \"10.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 | } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 10 , 22 , 0 , 254 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } The extractor also supports multiple comma-separated ranges. This will return the prefix and mask if it belongs to any one of the CIDRs specified: match { \"meta\" : \"10.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 , 10.21.1.0 / 24 | } => rp default => \"no match\" end ; This will output: \"meta\" : { \"prefix\" : [ 10 , 22 , 0 , 254 ] , \"mask\" : [ 255 , 255 , 255 , 255 ] } In case it doesn't belong to the CIDR: match { \"meta\" : \"19.22.0.254\" } of case rp = % { meta ~= cidr | 10.22.0.0 / 24 , 10.21.1.0 / 24 | } => rp default => \"no match\" end ; ## Output: \"no match\"","title":"Examples"},{"location":"tremor-script/extractors/datetime/","text":"Datetime \u00b6 The datetime extractor parses the input into a timestamp. The format of the target needs to be specified as a parameter to the extractor. The extractor is equivalent to the Datetime::parse(\u2026) . Predicate \u00b6 When used with ~ , the predicate parses if the target can be parsed to a nanosecond-precision timestamp. The predicate will fail if it encounters any of the errors described in the Error section below. Extraction \u00b6 If the predicate parses, the extractor returns the 64-bit nanosecond-precise UTC UNIX timestamp. Example \u00b6 match { \"test\" : \"2019-01-01 09:42\" } of case foo = % { test ~= datetime |% Y -% m -% d % H :% M | } => foo . test default => \"ko\" end ; ## output: 1546335720000000000 Errors \u00b6 The extractor will fail due to one of the following: Incorrect input is passed Input doesn't match the format passed Input doesn't contain the Year, Month, Day, Hour & Minute section irrespective of the format passed. Input contains more components than the format passed","title":"Datetime"},{"location":"tremor-script/extractors/datetime/#datetime","text":"The datetime extractor parses the input into a timestamp. The format of the target needs to be specified as a parameter to the extractor. The extractor is equivalent to the Datetime::parse(\u2026) .","title":"Datetime"},{"location":"tremor-script/extractors/datetime/#predicate","text":"When used with ~ , the predicate parses if the target can be parsed to a nanosecond-precision timestamp. The predicate will fail if it encounters any of the errors described in the Error section below.","title":"Predicate"},{"location":"tremor-script/extractors/datetime/#extraction","text":"If the predicate parses, the extractor returns the 64-bit nanosecond-precise UTC UNIX timestamp.","title":"Extraction"},{"location":"tremor-script/extractors/datetime/#example","text":"match { \"test\" : \"2019-01-01 09:42\" } of case foo = % { test ~= datetime |% Y -% m -% d % H :% M | } => foo . test default => \"ko\" end ; ## output: 1546335720000000000","title":"Example"},{"location":"tremor-script/extractors/datetime/#errors","text":"The extractor will fail due to one of the following: Incorrect input is passed Input doesn't match the format passed Input doesn't contain the Year, Month, Day, Hour & Minute section irrespective of the format passed. Input contains more components than the format passed","title":"Errors"},{"location":"tremor-script/extractors/dissect/","text":"Dissect \u00b6 The Dissect extractor is loosely based on logstash's dissect plugin. It extracts data from strings in form of key-value pairs based on the pattern specified. It supports patterns which makes it lightweight compared to other extractors such as grok or regex. Tokens are enclosed within %{ } and any characters placed between tokens is considered as a delimiter. There should be at least one delimiter in between 2 tokens except if one of them is a padding token. Predicate \u00b6 When used as a predicate test with ~ , it passes if and only if the referent target matches the pattern exhaustively. Extraction \u00b6 If the predicate passes, then a record of matched entries are returned where the keys are the fields specified in the pattern, and the values are the values extracted from the input string. Patterns \u00b6 The pattern can contain any of the following types: Name Syntax Description Sample Pattern Input Output Notes Simple %{ field } Given field is used as the key for the pattern extracted %{name} John \"name\" : \"John\" Append % { + field } Appends the value to another field specified with the same field name. % {+name} %{+name} John Doe \"name\" : \"John Doe\" + symbol on the first token is optional Does not support types Named keys %{& field} Returns key value pair of the field. Takes the key from the previously matched field. %{ code } % {country} DE Germany \"DE\" : \"Germany\" Needs a field present earlier with the same name Empty field % { field } Will return an empty value if no data is present %{ code } %{country} Germany \"code\" : \"\",<br />\"country\" : \"Germany\" Skipped fields %{? field} Skips the extracted value %{ ? first_name} {last_name} John Doe \"name\" : \"Doe\" Typed Values % { field : type} Extracts the value from the data and converts it to another type specified %{age : int} 22 \"age\" : 22 Supported Types: int, float Padding %{_} or %{_(chars)} Removes padding from the field in the output %{name} %{_} %{age} John 22 \"name\" : \"John\", \"age\":\"22\" The field being extracted may not contain the padding. A custom padding can be specified by using the %{_(custom)} notation The operation can error in the following cases: There is no delimiter between 2 tokens. Skipped, Append or Named field with no name (e.g. %{?} ). A delimiter specified in the pattern is missing. A named field is not found in the pattern. the type specified is not valid. Types used with the append token. Pattern isn't completely parsed (at end of input). Input isn't completely parsed. Examples: match { \"test\" : \"http://example.com/\" } of case foo = % {test ~= dissect |% {protocol} ://% {host} .% { . tld} | } => foo default => \"ko\" end ; Will output: \"test\" : { \"protocol\" : \"http\" , \"host\" : \"example\" , \".tld\" : \"com/\" } match { \"test\" : \"2019-04-20------------------- high 3 foo bar\" } of case foo = % {test ~= dissect |% {date} % { _ (-) } % { ? priority} % { & priority} % { + snot} % { + snot} | } => foo default => \"ko\" end ; Will output: { \"test\" : { \"date\" : \"2019-04-20\" , \"high\" : \"3\" , \"snot\" : \"foo bar\" }","title":"Dissect"},{"location":"tremor-script/extractors/dissect/#dissect","text":"The Dissect extractor is loosely based on logstash's dissect plugin. It extracts data from strings in form of key-value pairs based on the pattern specified. It supports patterns which makes it lightweight compared to other extractors such as grok or regex. Tokens are enclosed within %{ } and any characters placed between tokens is considered as a delimiter. There should be at least one delimiter in between 2 tokens except if one of them is a padding token.","title":"Dissect"},{"location":"tremor-script/extractors/dissect/#predicate","text":"When used as a predicate test with ~ , it passes if and only if the referent target matches the pattern exhaustively.","title":"Predicate"},{"location":"tremor-script/extractors/dissect/#extraction","text":"If the predicate passes, then a record of matched entries are returned where the keys are the fields specified in the pattern, and the values are the values extracted from the input string.","title":"Extraction"},{"location":"tremor-script/extractors/dissect/#patterns","text":"The pattern can contain any of the following types: Name Syntax Description Sample Pattern Input Output Notes Simple %{ field } Given field is used as the key for the pattern extracted %{name} John \"name\" : \"John\" Append % { + field } Appends the value to another field specified with the same field name. % {+name} %{+name} John Doe \"name\" : \"John Doe\" + symbol on the first token is optional Does not support types Named keys %{& field} Returns key value pair of the field. Takes the key from the previously matched field. %{ code } % {country} DE Germany \"DE\" : \"Germany\" Needs a field present earlier with the same name Empty field % { field } Will return an empty value if no data is present %{ code } %{country} Germany \"code\" : \"\",<br />\"country\" : \"Germany\" Skipped fields %{? field} Skips the extracted value %{ ? first_name} {last_name} John Doe \"name\" : \"Doe\" Typed Values % { field : type} Extracts the value from the data and converts it to another type specified %{age : int} 22 \"age\" : 22 Supported Types: int, float Padding %{_} or %{_(chars)} Removes padding from the field in the output %{name} %{_} %{age} John 22 \"name\" : \"John\", \"age\":\"22\" The field being extracted may not contain the padding. A custom padding can be specified by using the %{_(custom)} notation The operation can error in the following cases: There is no delimiter between 2 tokens. Skipped, Append or Named field with no name (e.g. %{?} ). A delimiter specified in the pattern is missing. A named field is not found in the pattern. the type specified is not valid. Types used with the append token. Pattern isn't completely parsed (at end of input). Input isn't completely parsed. Examples: match { \"test\" : \"http://example.com/\" } of case foo = % {test ~= dissect |% {protocol} ://% {host} .% { . tld} | } => foo default => \"ko\" end ; Will output: \"test\" : { \"protocol\" : \"http\" , \"host\" : \"example\" , \".tld\" : \"com/\" } match { \"test\" : \"2019-04-20------------------- high 3 foo bar\" } of case foo = % {test ~= dissect |% {date} % { _ (-) } % { ? priority} % { & priority} % { + snot} % { + snot} | } => foo default => \"ko\" end ; Will output: { \"test\" : { \"date\" : \"2019-04-20\" , \"high\" : \"3\" , \"snot\" : \"foo bar\" }","title":"Patterns"},{"location":"tremor-script/extractors/glob/","text":"Glob \u00b6 Glob is an extractor that checks if the input string matches the specified Unix shell-style pattern . The extractor fails if an pattern is specified that is not valid or the string doesn't match the pattern. Predicate \u00b6 When used as a predicate with ~ , the predicate will pass if the input matches the glob pattern passed as the parameter to the extractor. Extraction \u00b6 The extractor returns true if the predicate passes else returns an error Patterns \u00b6 Patterns can be of the following types: Pattern Matches ? Single character * any (0 or more) sequence or characters [\u2026] any character inside the bracket. Supports ranges (e,g. [0-9] will match any digit) [!\u2026] negation of [\u2026] Meta characters (e..g * , ? ) can be matched by using [ ] . (e.g. [ * ] will match a string that contains * ). match { \"test\" : \"INFO\" } of case foo = % { test ~= glob | INFO *| } => foo default => \"ko\" end ; ## will output true","title":"Glob"},{"location":"tremor-script/extractors/glob/#glob","text":"Glob is an extractor that checks if the input string matches the specified Unix shell-style pattern . The extractor fails if an pattern is specified that is not valid or the string doesn't match the pattern.","title":"Glob"},{"location":"tremor-script/extractors/glob/#predicate","text":"When used as a predicate with ~ , the predicate will pass if the input matches the glob pattern passed as the parameter to the extractor.","title":"Predicate"},{"location":"tremor-script/extractors/glob/#extraction","text":"The extractor returns true if the predicate passes else returns an error","title":"Extraction"},{"location":"tremor-script/extractors/glob/#patterns","text":"Patterns can be of the following types: Pattern Matches ? Single character * any (0 or more) sequence or characters [\u2026] any character inside the bracket. Supports ranges (e,g. [0-9] will match any digit) [!\u2026] negation of [\u2026] Meta characters (e..g * , ? ) can be matched by using [ ] . (e.g. [ * ] will match a string that contains * ). match { \"test\" : \"INFO\" } of case foo = % { test ~= glob | INFO *| } => foo default => \"ko\" end ; ## will output true","title":"Patterns"},{"location":"tremor-script/extractors/grok/","text":"Grok \u00b6 The grok extractor is useful for parsing unstructured data into a structured form. It is based on logstash's grok plugin. Grok uses regular expressions, so any regular expression can be used as a grok pattern. Grok pattern is of the form %{SYNTAX : SEMANTIC} where SYNTAX is the name of the pattern that matches the text and SEMANTIC is the identifier Predicate \u00b6 When used with ~ , the predicate passes if the target matches the pattern passed by the input (fetched from the grok pattern's file). Extraction \u00b6 If the predicate passes, the extractor returns the matches found when the target was matched to the pattern. Example \u00b6 match { \"meta\" : \"55.3.244.1 GET /index.html 15824 0.043\" } of case rp = % { meta ~= grok |% {IP : client} % {WORD : method} % {URIPATHPARAM : request} % {NUMBER : bytes} % {NUMBER : duration} | } => rp default => \"no match\" end ;","title":"Grok"},{"location":"tremor-script/extractors/grok/#grok","text":"The grok extractor is useful for parsing unstructured data into a structured form. It is based on logstash's grok plugin. Grok uses regular expressions, so any regular expression can be used as a grok pattern. Grok pattern is of the form %{SYNTAX : SEMANTIC} where SYNTAX is the name of the pattern that matches the text and SEMANTIC is the identifier","title":"Grok"},{"location":"tremor-script/extractors/grok/#predicate","text":"When used with ~ , the predicate passes if the target matches the pattern passed by the input (fetched from the grok pattern's file).","title":"Predicate"},{"location":"tremor-script/extractors/grok/#extraction","text":"If the predicate passes, the extractor returns the matches found when the target was matched to the pattern.","title":"Extraction"},{"location":"tremor-script/extractors/grok/#example","text":"match { \"meta\" : \"55.3.244.1 GET /index.html 15824 0.043\" } of case rp = % { meta ~= grok |% {IP : client} % {WORD : method} % {URIPATHPARAM : request} % {NUMBER : bytes} % {NUMBER : duration} | } => rp default => \"no match\" end ;","title":"Example"},{"location":"tremor-script/extractors/influx/","text":"Influx \u00b6 Influx extrector matches data from the string that uses the Influx Line Protocol . It will fail if the input isn't a valid string. Predicate \u00b6 When used as a predicate with ~ , the predicate will pass if the target conforms to the influx line protocol. Extraction \u00b6 The extractor will return a record with the measurement, fields, tags and the timestamp extracted from the input. Example: match { \"meta\" : \"wea \\\\ ther,location=us-midwest temperature=82 1465839830100400200\" } of case rp = % { meta ~= influx || } => rp default => \"no match\" end ; This will return: \"meta\" : { \"measurement\" : \"wea ther\" , \"tags\" : { \"location\" : \"us-midwest\" } , \"fields\" : { \"temperature\" : 82 .0 } , \"timestamp\" : 1465839830100400200 }","title":"Influx"},{"location":"tremor-script/extractors/influx/#influx","text":"Influx extrector matches data from the string that uses the Influx Line Protocol . It will fail if the input isn't a valid string.","title":"Influx"},{"location":"tremor-script/extractors/influx/#predicate","text":"When used as a predicate with ~ , the predicate will pass if the target conforms to the influx line protocol.","title":"Predicate"},{"location":"tremor-script/extractors/influx/#extraction","text":"The extractor will return a record with the measurement, fields, tags and the timestamp extracted from the input. Example: match { \"meta\" : \"wea \\\\ ther,location=us-midwest temperature=82 1465839830100400200\" } of case rp = % { meta ~= influx || } => rp default => \"no match\" end ; This will return: \"meta\" : { \"measurement\" : \"wea ther\" , \"tags\" : { \"location\" : \"us-midwest\" } , \"fields\" : { \"temperature\" : 82 .0 } , \"timestamp\" : 1465839830100400200 }","title":"Extraction"},{"location":"tremor-script/extractors/json/","text":"JSON \u00b6 The JSON extractor converts the input string into its respective JSON representation conforming to The Javascript Object Notation Data Interchange Format (RFC 8259) Predicate \u00b6 When used with ~ , the predicate will pass if the input is a valid JSON Extraction \u00b6 If the predicate passes, the extractor will return the JSON representation of the target. match { \"test\" : \"{ \\\" foo \\\" : \\\" bar \\\" , \" \\ snot \"\\:\" \\ badger \"\\ }\" } of case foo = % { test ~= json || } => foo default => \"ko\" end ;","title":"JSON"},{"location":"tremor-script/extractors/json/#json","text":"The JSON extractor converts the input string into its respective JSON representation conforming to The Javascript Object Notation Data Interchange Format (RFC 8259)","title":"JSON"},{"location":"tremor-script/extractors/json/#predicate","text":"When used with ~ , the predicate will pass if the input is a valid JSON","title":"Predicate"},{"location":"tremor-script/extractors/json/#extraction","text":"If the predicate passes, the extractor will return the JSON representation of the target. match { \"test\" : \"{ \\\" foo \\\" : \\\" bar \\\" , \" \\ snot \"\\:\" \\ badger \"\\ }\" } of case foo = % { test ~= json || } => foo default => \"ko\" end ;","title":"Extraction"},{"location":"tremor-script/extractors/kv/","text":"KV \u00b6 Parses a string into a map. It is possible to split based on different characters that represent either field or key value boundaries. A good part of the logstash functionality will be handled ouYeside of this function and in a generic way in tremor script. Predicate \u00b6 When used with ~ , the predicate passes if a string is valid that can be converted into a map. Extractor \u00b6 If the predicate passes, it converts the target into its equivalent map. Features (in relation to Logstash): Setting Supported allow_duplicate_values No default_keys (via tremor script) exclude_keys (via tremor script) field_split Yes (default \" \" (space)) field_split_pattern No include_bracke (via tremor script) include_keys (via tremor script) prefix (via tremor script) recursive No remove_char_key (via tremor script) remove_char_value (via tremor script) source (via tremor script) target (via tremor script) tag_on_failure (via tremor script) tag_on_timeout No timeout_millis No transform_key (via tremor script) transform_value (via tremor script) trim_key (via tremor script) trim_value (via tremor script) value_split Yes (default \":\" (colon)) value_split_pattern No whitespace (via tremor script) To specify a value separator (the separator used between key and value) use the pattern form kv|%{key}=%{val}| . Both %{key} and %{val} are fixed keywords and can not be substituted for other names. The pattern kv|%{key}=%{val}| would lead to = being the separator. Multiple of those pairs can be given to use multiple separators. To specify field separators they need to be either before or after a value separator or on their own. kv|&| would separate the fields by & . Both field and value separators can be related without harm. Field and value separators can not overlap, even partially. Complex patterns can be given by using multiple key value pairs with different separators, their order does not matter and they will not be required to be present. Example \u00b6 All of the following are equivalent: match { \"test\" : \"foo:bar snot:badger\" } of case cake = % { test ~= kv || } => cake case cake = % { test ~= kv | | } => cake case cake = % { test ~= kv || } => cake case cake = % { test ~= kv |% {key} :% {val} | } => cake case cake = % { test ~= kv |% {key} :% {val} | } => cake case cake = % { test ~= kv | % {key} :% {val} | } => cake case cake = % { test ~= kv |% {key} :% {val} % {key} :% {val} % {key} :% {val} | } => cake default => \"ko\" end ; This will output: \"test\" : { \"foo\" : \"bar\" , \"snot\" : \"badger\" } Example 2 \u00b6 Match query parameters match event of case cake = % {test ~= kv |% {key} =% {val} &| } => cake default => \"ko\" end ;","title":"KV"},{"location":"tremor-script/extractors/kv/#kv","text":"Parses a string into a map. It is possible to split based on different characters that represent either field or key value boundaries. A good part of the logstash functionality will be handled ouYeside of this function and in a generic way in tremor script.","title":"KV"},{"location":"tremor-script/extractors/kv/#predicate","text":"When used with ~ , the predicate passes if a string is valid that can be converted into a map.","title":"Predicate"},{"location":"tremor-script/extractors/kv/#extractor","text":"If the predicate passes, it converts the target into its equivalent map. Features (in relation to Logstash): Setting Supported allow_duplicate_values No default_keys (via tremor script) exclude_keys (via tremor script) field_split Yes (default \" \" (space)) field_split_pattern No include_bracke (via tremor script) include_keys (via tremor script) prefix (via tremor script) recursive No remove_char_key (via tremor script) remove_char_value (via tremor script) source (via tremor script) target (via tremor script) tag_on_failure (via tremor script) tag_on_timeout No timeout_millis No transform_key (via tremor script) transform_value (via tremor script) trim_key (via tremor script) trim_value (via tremor script) value_split Yes (default \":\" (colon)) value_split_pattern No whitespace (via tremor script) To specify a value separator (the separator used between key and value) use the pattern form kv|%{key}=%{val}| . Both %{key} and %{val} are fixed keywords and can not be substituted for other names. The pattern kv|%{key}=%{val}| would lead to = being the separator. Multiple of those pairs can be given to use multiple separators. To specify field separators they need to be either before or after a value separator or on their own. kv|&| would separate the fields by & . Both field and value separators can be related without harm. Field and value separators can not overlap, even partially. Complex patterns can be given by using multiple key value pairs with different separators, their order does not matter and they will not be required to be present.","title":"Extractor"},{"location":"tremor-script/extractors/kv/#example","text":"All of the following are equivalent: match { \"test\" : \"foo:bar snot:badger\" } of case cake = % { test ~= kv || } => cake case cake = % { test ~= kv | | } => cake case cake = % { test ~= kv || } => cake case cake = % { test ~= kv |% {key} :% {val} | } => cake case cake = % { test ~= kv |% {key} :% {val} | } => cake case cake = % { test ~= kv | % {key} :% {val} | } => cake case cake = % { test ~= kv |% {key} :% {val} % {key} :% {val} % {key} :% {val} | } => cake default => \"ko\" end ; This will output: \"test\" : { \"foo\" : \"bar\" , \"snot\" : \"badger\" }","title":"Example"},{"location":"tremor-script/extractors/kv/#example-2","text":"Match query parameters match event of case cake = % {test ~= kv |% {key} =% {val} &| } => cake default => \"ko\" end ;","title":"Example 2"},{"location":"tremor-script/extractors/regex/","text":"Regex (re) \u00b6 The regex extractor extracts fields from data by parsing a regular expression provided by the user. It accepts a \"perl-style regular expression\" Predicate \u00b6 When used with ~ , the predicate passes if a valid regular expression is passed. Extraction \u00b6 If the predicate passes, the extractor returns the matched values from the target. Returns an error if the regex fails to match. Example \u00b6 drop match { \"test\" : \"http://example.com/\" , \"footle\" : \"bar\" } of case foo = % { test ~= re |^ http ://.*/$|, footle == \"bar\" } => foo default => \"ko\" end The extractor is called by using the ~= operator and specifying re as the extractor followed by regular expression after the pipe operator. The following syntax is supported: Matching one character \u00b6 . any character except new line (includes new line with s flag) \\d digit (\\p{Nd}) \\D not digit \\pN One-letter name Unicode character class \\p{Greek} Unicode character class (general category or script) \\PN Negated one-letter name Unicode character class \\P{Greek} negated Unicode character class (general category or script) Character classes \u00b6 [xyz] A character class matching either x, y or z (union). [^xyz] A character class matching any character except x, y and z. [a-z] A character class matching any character in range a-z. [[:alpha:]] ASCII character class ([A-Za-z]) [[:^alpha:]] Negated ASCII character class ([^A-Za-z]) [x[^xyz]] Nested/grouping character class (matching any character except y and z) [a-y&&xyz] Intersection (matching x or y) [0-9&&[^4]] Subtraction using intersection and negation (matching 0-9 except 4) [0-9--4] Direct subtraction (matching 0-9 except 4) [a-g~~b-h] Symmetric difference (matching `a` and `h` only) [\\[\\]] Escaping in character classes (matching [ or ]) Any named character class may appear inside a bracketed [...] character class. For example, [\\p{Greek}[:digit:]] matches any Greek or ASCII digit. [\\p{Greek}&&\\pL] matches Greek letters. Precedence in character classes, from most binding to least: Ranges: a-cd == [a-c]d Union: ab&&bc == [ab]&&[bc] Intersection: ^a-z&&b == ^[a-z&&b] Negation Composites \u00b6 xy concatenation (x followed by y) x|y alternation (x or y, prefer x) Repetitions \u00b6 x* zero or more of x (greedy) x+ one or more of x (greedy) x? zero or one of x (greedy) x*? zero or more of x (ungreedy/lazy) x+? one or more of x (ungreedy/lazy) x?? zero or one of x (ungreedy/lazy) x{n,m} at least n x and at most m x (greedy) x{n,} at least n x (greedy) x{n} exactly n x x{n,m}? at least n x and at most m x (ungreedy/lazy) x{n,}? at least n x (ungreedy/lazy) x{n}? exactly n x Empty matches \u00b6 ^ the beginning of text (or start-of-line with multi-line mode) $ the end of text (or end-of-line with multi-line mode) \\A only the beginning of text (even with multi-line mode enabled) \\z only the end of text (even with multi-line mode enabled) \\b a Unicode word boundary (\\w on one side and \\W, \\A, or \\z on other) \\B not a Unicode word boundary Grouping and flags \u00b6 (exp) numbered capture group (indexed by opening parenthesis) (?P<name>exp) named (also numbered) capture group (allowed chars: [_0-9a-zA-Z]) (?:exp) non-capturing group (?flags) set flags within current group (?flags:exp) set flags for exp (non-capturing) Flags are each a single character. For example, (?x) sets the flag x and (?-x) clears the flag x . Multiple flags can be set or cleared at the same time: (?xy) sets both the x and y flags and (?x-y) sets the x flag and clears the y flag. All flags are by default set to off unless stated otherwise. They are: i case-insensitive: letters match both upper and lower case m multi-line mode: ^ and $ match begin/end of line s allow . to match \\n U swap the meaning of x* and x*? u Unicode support (enabled by default) x ignore whitespace and allow line comments (starting with `#`)","title":"Regex"},{"location":"tremor-script/extractors/regex/#regex-re","text":"The regex extractor extracts fields from data by parsing a regular expression provided by the user. It accepts a \"perl-style regular expression\"","title":"Regex (re)"},{"location":"tremor-script/extractors/regex/#predicate","text":"When used with ~ , the predicate passes if a valid regular expression is passed.","title":"Predicate"},{"location":"tremor-script/extractors/regex/#extraction","text":"If the predicate passes, the extractor returns the matched values from the target. Returns an error if the regex fails to match.","title":"Extraction"},{"location":"tremor-script/extractors/regex/#example","text":"drop match { \"test\" : \"http://example.com/\" , \"footle\" : \"bar\" } of case foo = % { test ~= re |^ http ://.*/$|, footle == \"bar\" } => foo default => \"ko\" end The extractor is called by using the ~= operator and specifying re as the extractor followed by regular expression after the pipe operator. The following syntax is supported:","title":"Example"},{"location":"tremor-script/extractors/regex/#matching-one-character","text":". any character except new line (includes new line with s flag) \\d digit (\\p{Nd}) \\D not digit \\pN One-letter name Unicode character class \\p{Greek} Unicode character class (general category or script) \\PN Negated one-letter name Unicode character class \\P{Greek} negated Unicode character class (general category or script)","title":"Matching one character"},{"location":"tremor-script/extractors/regex/#character-classes","text":"[xyz] A character class matching either x, y or z (union). [^xyz] A character class matching any character except x, y and z. [a-z] A character class matching any character in range a-z. [[:alpha:]] ASCII character class ([A-Za-z]) [[:^alpha:]] Negated ASCII character class ([^A-Za-z]) [x[^xyz]] Nested/grouping character class (matching any character except y and z) [a-y&&xyz] Intersection (matching x or y) [0-9&&[^4]] Subtraction using intersection and negation (matching 0-9 except 4) [0-9--4] Direct subtraction (matching 0-9 except 4) [a-g~~b-h] Symmetric difference (matching `a` and `h` only) [\\[\\]] Escaping in character classes (matching [ or ]) Any named character class may appear inside a bracketed [...] character class. For example, [\\p{Greek}[:digit:]] matches any Greek or ASCII digit. [\\p{Greek}&&\\pL] matches Greek letters. Precedence in character classes, from most binding to least: Ranges: a-cd == [a-c]d Union: ab&&bc == [ab]&&[bc] Intersection: ^a-z&&b == ^[a-z&&b] Negation","title":"Character classes"},{"location":"tremor-script/extractors/regex/#composites","text":"xy concatenation (x followed by y) x|y alternation (x or y, prefer x)","title":"Composites"},{"location":"tremor-script/extractors/regex/#repetitions","text":"x* zero or more of x (greedy) x+ one or more of x (greedy) x? zero or one of x (greedy) x*? zero or more of x (ungreedy/lazy) x+? one or more of x (ungreedy/lazy) x?? zero or one of x (ungreedy/lazy) x{n,m} at least n x and at most m x (greedy) x{n,} at least n x (greedy) x{n} exactly n x x{n,m}? at least n x and at most m x (ungreedy/lazy) x{n,}? at least n x (ungreedy/lazy) x{n}? exactly n x","title":"Repetitions"},{"location":"tremor-script/extractors/regex/#empty-matches","text":"^ the beginning of text (or start-of-line with multi-line mode) $ the end of text (or end-of-line with multi-line mode) \\A only the beginning of text (even with multi-line mode enabled) \\z only the end of text (even with multi-line mode enabled) \\b a Unicode word boundary (\\w on one side and \\W, \\A, or \\z on other) \\B not a Unicode word boundary","title":"Empty matches"},{"location":"tremor-script/extractors/regex/#grouping-and-flags","text":"(exp) numbered capture group (indexed by opening parenthesis) (?P<name>exp) named (also numbered) capture group (allowed chars: [_0-9a-zA-Z]) (?:exp) non-capturing group (?flags) set flags within current group (?flags:exp) set flags for exp (non-capturing) Flags are each a single character. For example, (?x) sets the flag x and (?-x) clears the flag x . Multiple flags can be set or cleared at the same time: (?xy) sets both the x and y flags and (?x-y) sets the x flag and clears the y flag. All flags are by default set to off unless stated otherwise. They are: i case-insensitive: letters match both upper and lower case m multi-line mode: ^ and $ match begin/end of line s allow . to match \\n U swap the meaning of x* and x*? u Unicode support (enabled by default) x ignore whitespace and allow line comments (starting with `#`)","title":"Grouping and flags"},{"location":"tremor-script/stdlib/cncf/","text":"cncf \u00b6 The Tremor language CNCF library. \u00b6 It provides the following modules for cloud native computing: otel - functionality related to CNCF OpenTelemetry","title":"cncf"},{"location":"tremor-script/stdlib/cncf/#cncf","text":"","title":"cncf"},{"location":"tremor-script/stdlib/cncf/#the-tremor-language-cncf-library","text":"It provides the following modules for cloud native computing: otel - functionality related to CNCF OpenTelemetry","title":"The Tremor language CNCF library."},{"location":"tremor-script/stdlib/std/","text":"std \u00b6 The tremor language standard library it provides the following modules: array - functions to deal with arrays ( [] ) base64 - functions for base64 en and decoding binary - functions to deal with binary data ( << 1, 2, 3 >> ) float - functions to deal with floating point numbers integer - functions to deal with integer numbers json - functions to deal with JSON math - mathematical functions random - random related functions range - range related functions re - functions handeling regular expressions record - functions dealing with records ( {} ) string - functions dealing with strings test - test related functions type - functions dealing with strings url - url decoding/encoding functions","title":"std"},{"location":"tremor-script/stdlib/std/#std","text":"The tremor language standard library it provides the following modules: array - functions to deal with arrays ( [] ) base64 - functions for base64 en and decoding binary - functions to deal with binary data ( << 1, 2, 3 >> ) float - functions to deal with floating point numbers integer - functions to deal with integer numbers json - functions to deal with JSON math - mathematical functions random - random related functions range - range related functions re - functions handeling regular expressions record - functions dealing with records ( {} ) string - functions dealing with strings test - test related functions type - functions dealing with strings url - url decoding/encoding functions","title":"std"},{"location":"tremor-script/stdlib/tremor/","text":"tremor \u00b6 Tremor runtime related libraries. This provides the following modules: chash - functions dealing with consitant hasing origin - functions providing access to onramp origin data system - functions related to the system running","title":"tremor"},{"location":"tremor-script/stdlib/tremor/#tremor","text":"Tremor runtime related libraries. This provides the following modules: chash - functions dealing with consitant hasing origin - functions providing access to onramp origin data system - functions related to the system running","title":"tremor"},{"location":"tremor-script/stdlib/cncf/otel/","text":"otel \u00b6 CNCF OpenTelemetry utility functions \u00b6 span_id - OpenTelemetry Span Id utilities trace_id - OpenTelemetry Trace Id utilities logs - OpenTelemetry log event utilities metrics - OpenTelemetry metrics event utilities trace - OpenTelemetry trace event utilities Functions \u00b6 gen_span_id_string() \u00b6 Generate a random span id using the hex string representation Returns a string gen_span_id_bytes() \u00b6 Generate a random span id using the binary representation Returns a binary gen_span_id_array() \u00b6 Generate a random span id using the int array representation Returns a array of int gen_trace_id_string() \u00b6 Generate a random trace id using the hex string representation Returns a string gen_trace_id_bytes() \u00b6 Generate a random trace id using the binary representation Returns a binary gen_trace_id_array() \u00b6 Generate a random trace id using the binary representation Returns a array of int","title":"cncf::otel"},{"location":"tremor-script/stdlib/cncf/otel/#otel","text":"","title":"otel"},{"location":"tremor-script/stdlib/cncf/otel/#cncf-opentelemetry-utility-functions","text":"span_id - OpenTelemetry Span Id utilities trace_id - OpenTelemetry Trace Id utilities logs - OpenTelemetry log event utilities metrics - OpenTelemetry metrics event utilities trace - OpenTelemetry trace event utilities","title":"CNCF OpenTelemetry utility functions"},{"location":"tremor-script/stdlib/cncf/otel/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/#gen_span_id_string","text":"Generate a random span id using the hex string representation Returns a string","title":"gen_span_id_string()"},{"location":"tremor-script/stdlib/cncf/otel/#gen_span_id_bytes","text":"Generate a random span id using the binary representation Returns a binary","title":"gen_span_id_bytes()"},{"location":"tremor-script/stdlib/cncf/otel/#gen_span_id_array","text":"Generate a random span id using the int array representation Returns a array of int","title":"gen_span_id_array()"},{"location":"tremor-script/stdlib/cncf/otel/#gen_trace_id_string","text":"Generate a random trace id using the hex string representation Returns a string","title":"gen_trace_id_string()"},{"location":"tremor-script/stdlib/cncf/otel/#gen_trace_id_bytes","text":"Generate a random trace id using the binary representation Returns a binary","title":"gen_trace_id_bytes()"},{"location":"tremor-script/stdlib/cncf/otel/#gen_trace_id_array","text":"Generate a random trace id using the binary representation Returns a array of int","title":"gen_trace_id_array()"},{"location":"tremor-script/stdlib/cncf/otel/logs/","text":"logs \u00b6 CNCF OpenTelemetry Log event utilities severity - Log severity traceflags - Log trace flags","title":"cncf::otel::logs"},{"location":"tremor-script/stdlib/cncf/otel/logs/#logs","text":"CNCF OpenTelemetry Log event utilities severity - Log severity traceflags - Log trace flags","title":"logs"},{"location":"tremor-script/stdlib/cncf/otel/metrics/","text":"metrics \u00b6 CNCF OpenTelemetry Metrics event utilities temporality - Metrics aggregation temporality","title":"cncf::otel::metrics"},{"location":"tremor-script/stdlib/cncf/otel/metrics/#metrics","text":"CNCF OpenTelemetry Metrics event utilities temporality - Metrics aggregation temporality","title":"metrics"},{"location":"tremor-script/stdlib/cncf/otel/span_id/","text":"span_id \u00b6 Span Identifiers Functions \u00b6 is_valid(span_id) \u00b6 Is the span_id valid Checks the span_id argument to see if it is a valid span id. A legal span id is one of: An array of integers in the range of [0..=255] of length 8 A binary 8 byte value An array of 8 int values A 16-byte hex-encoded string Regardless of representation, the value must not be all zeroes Returns a record when the representation is well-formed of the form: { \"kind\" : \"string\" | \"binary\" | \"array\" , # Depends on input \"valid\" : true | false , # True if well-formed and valid \"value\" : \"<span_id>\" # Representation depends on `kind` } Returns an empty record {} when the representation not well-formed","title":"cncf::otel::span_id"},{"location":"tremor-script/stdlib/cncf/otel/span_id/#span_id","text":"Span Identifiers","title":"span_id"},{"location":"tremor-script/stdlib/cncf/otel/span_id/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/span_id/#is_validspan_id","text":"Is the span_id valid Checks the span_id argument to see if it is a valid span id. A legal span id is one of: An array of integers in the range of [0..=255] of length 8 A binary 8 byte value An array of 8 int values A 16-byte hex-encoded string Regardless of representation, the value must not be all zeroes Returns a record when the representation is well-formed of the form: { \"kind\" : \"string\" | \"binary\" | \"array\" , # Depends on input \"valid\" : true | false , # True if well-formed and valid \"value\" : \"<span_id>\" # Representation depends on `kind` } Returns an empty record {} when the representation not well-formed","title":"is_valid(span_id)"},{"location":"tremor-script/stdlib/cncf/otel/trace/","text":"trace \u00b6 CNCF OpenTelemetry Trace event utilities spankind - Trace span kind status - Trace status","title":"cncf::otel::trace"},{"location":"tremor-script/stdlib/cncf/otel/trace/#trace","text":"CNCF OpenTelemetry Trace event utilities spankind - Trace span kind status - Trace status","title":"trace"},{"location":"tremor-script/stdlib/cncf/otel/trace_id/","text":"trace_id \u00b6 Trace Identifiers Functions \u00b6 is_valid(trace_id) \u00b6 Is the trace_id valid Checks the trace_id argument to see if it is a valid trace id. A legal trace id is one of: An array of integers in the range of [0..=255] of length 8 A binary 16 byte value A 32-byte hex-encoded string An array of 16 int values Regardless of representation, the value must not be all zeroes Returns a record when the representation is well-formed of the form: { \"kind\" : \"string\" | \"binary\" | \"array\" , # Depends on input \"valid\" : true | false , # True if well-formed and valid \"value\" : \"<trace_id>\" # Representation depends on `kind` } Returns an empty record {} when the representation not well-formed","title":"cncf::otel::trace_id"},{"location":"tremor-script/stdlib/cncf/otel/trace_id/#trace_id","text":"Trace Identifiers","title":"trace_id"},{"location":"tremor-script/stdlib/cncf/otel/trace_id/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/trace_id/#is_validtrace_id","text":"Is the trace_id valid Checks the trace_id argument to see if it is a valid trace id. A legal trace id is one of: An array of integers in the range of [0..=255] of length 8 A binary 16 byte value A 32-byte hex-encoded string An array of 16 int values Regardless of representation, the value must not be all zeroes Returns a record when the representation is well-formed of the form: { \"kind\" : \"string\" | \"binary\" | \"array\" , # Depends on input \"valid\" : true | false , # True if well-formed and valid \"value\" : \"<trace_id>\" # Representation depends on `kind` } Returns an empty record {} when the representation not well-formed","title":"is_valid(trace_id)"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/","text":"severity \u00b6 The severity module defines severity_number values and associated utility functions Constants \u00b6 unspecified \u00b6 type : I64 The severity wasn't specified trace \u00b6 type : I64 Trace - default level trace2 \u00b6 type : I64 Trace - level 2 trace3 \u00b6 type : I64 Trace - level 3 trace4 \u00b6 type : I64 Trace - level 4 debug \u00b6 type : I64 Debug - default level debug2 \u00b6 type : I64 Debug - level 2 debug3 \u00b6 type : I64 Debug - level 3 debug4 \u00b6 type : I64 Debug - level 4 info \u00b6 type : I64 Informational - default level info2 \u00b6 type : I64 Informational - level 2 info3 \u00b6 type : I64 Informational - level 3 info4 \u00b6 type : I64 Informational - level 4 warn \u00b6 type : I64 Warning - default level warn2 \u00b6 type : I64 Warning - level 2 warn3 \u00b6 type : I64 Warning - level 3 warn4 \u00b6 type : I64 Warning - level 4 error \u00b6 type : I64 Non-fatal ( recoverable ) Error - default level error2 \u00b6 type : I64 Non-fatal ( recoverable ) Error - level 2 error3 \u00b6 type : I64 Non-fatal ( recoverable ) Error - level 3 error4 \u00b6 type : I64 Non-fatal ( recoverable ) Error - level 4 fatal \u00b6 type : I64 Fatal ( recoverable ) Error - default level fatal2 \u00b6 type : I64 Fatal ( recoverable ) Error - level 2 fatal3 \u00b6 type : I64 Fatal ( recoverable ) Error - level 3 fatal4 \u00b6 type : I64 Fatal ( recoverable ) Error - level 4 Functions \u00b6 to_string(severity_number) \u00b6 Given a severity_number returns its normative string representation Returns a string indicates_error(severity_number) \u00b6 Given a severity_number is it indicative of a non-fatal or fatal error Returns a bool make_default() \u00b6 Returns the default severity_number The default severity_number","title":"cncf::otel::logs::severity"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#severity","text":"The severity module defines severity_number values and associated utility functions","title":"severity"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#constants","text":"","title":"Constants"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#unspecified","text":"type : I64 The severity wasn't specified","title":"unspecified"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#trace","text":"type : I64 Trace - default level","title":"trace"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#trace2","text":"type : I64 Trace - level 2","title":"trace2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#trace3","text":"type : I64 Trace - level 3","title":"trace3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#trace4","text":"type : I64 Trace - level 4","title":"trace4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#debug","text":"type : I64 Debug - default level","title":"debug"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#debug2","text":"type : I64 Debug - level 2","title":"debug2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#debug3","text":"type : I64 Debug - level 3","title":"debug3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#debug4","text":"type : I64 Debug - level 4","title":"debug4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#info","text":"type : I64 Informational - default level","title":"info"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#info2","text":"type : I64 Informational - level 2","title":"info2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#info3","text":"type : I64 Informational - level 3","title":"info3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#info4","text":"type : I64 Informational - level 4","title":"info4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#warn","text":"type : I64 Warning - default level","title":"warn"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#warn2","text":"type : I64 Warning - level 2","title":"warn2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#warn3","text":"type : I64 Warning - level 3","title":"warn3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#warn4","text":"type : I64 Warning - level 4","title":"warn4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#error","text":"type : I64 Non-fatal ( recoverable ) Error - default level","title":"error"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#error2","text":"type : I64 Non-fatal ( recoverable ) Error - level 2","title":"error2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#error3","text":"type : I64 Non-fatal ( recoverable ) Error - level 3","title":"error3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#error4","text":"type : I64 Non-fatal ( recoverable ) Error - level 4","title":"error4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#fatal","text":"type : I64 Fatal ( recoverable ) Error - default level","title":"fatal"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#fatal2","text":"type : I64 Fatal ( recoverable ) Error - level 2","title":"fatal2"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#fatal3","text":"type : I64 Fatal ( recoverable ) Error - level 3","title":"fatal3"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#fatal4","text":"type : I64 Fatal ( recoverable ) Error - level 4","title":"fatal4"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#to_stringseverity_number","text":"Given a severity_number returns its normative string representation Returns a string","title":"to_string(severity_number)"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#indicates_errorseverity_number","text":"Given a severity_number is it indicative of a non-fatal or fatal error Returns a bool","title":"indicates_error(severity_number)"},{"location":"tremor-script/stdlib/cncf/otel/logs/severity/#make_default","text":"Returns the default severity_number The default severity_number","title":"make_default()"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/","text":"traceflags \u00b6 The severity module defines severity_number values and associated utility functions OpenTelemetry Log Data Model - Trace Flags W3C Trace Context - Trace Flags Functions \u00b6 from_int(trace_flags) \u00b6 The from_int function interprets a trace_flags integer value argument to see if it is sampled-flag ( decimal 128 ) is set. All other flags are currently unused and SHOULD be 0 ( unset ) in conforming W3C Trace Context and conformant OpenTelemetry implementations is_valid(trace_flags) \u00b6 Checks if a trace_flags instance is correct and valid make_default() \u00b6 Returns the default configuration of traceflags","title":"cncf::otel::logs::traceflags"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/#traceflags","text":"The severity module defines severity_number values and associated utility functions OpenTelemetry Log Data Model - Trace Flags W3C Trace Context - Trace Flags","title":"traceflags"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/#from_inttrace_flags","text":"The from_int function interprets a trace_flags integer value argument to see if it is sampled-flag ( decimal 128 ) is set. All other flags are currently unused and SHOULD be 0 ( unset ) in conforming W3C Trace Context and conformant OpenTelemetry implementations","title":"from_int(trace_flags)"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/#is_validtrace_flags","text":"Checks if a trace_flags instance is correct and valid","title":"is_valid(trace_flags)"},{"location":"tremor-script/stdlib/cncf/otel/logs/traceflags/#make_default","text":"Returns the default configuration of traceflags","title":"make_default()"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/","text":"temporality \u00b6 This module is a mapping of the AggregationTemporality field in CNCF OpenTelemetry trace spans. The text of the specification has been copied into this mapping for convenience. Where text from the CNCF OpenTelemetry specification is used it is provided in block quotes. From the specification: AggregationTemporality defines how a metric aggregator reports aggregated values. It describes how those values relate to the time interval over which they are aggregated. Constants \u00b6 unspecified \u00b6 type : I64 The default AggregationTemparality - it must not be used conforming implementation will set the temporality to one of the valid enumeration values. delta \u00b6 type : I64 DELTA is an AggregationTemporality for a metric aggregator which reports changes since last report time. Successive metrics contain aggregation of values from continuous and non-overlapping intervals. The values for a DELTA metric are based only on the time interval associated with one measurement cycle. There is no dependency on previous measurements like is the case for CUMULATIVE metrics. For example, consider a system measuring the number of requests that it receives and reports the sum of these requests every second as a DELTA metric: The system starts receiving at time=t_0. A request is received, the system measures 1 request. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+1 with a value of 3. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0+1 to t_0+2 with a value of 2. cumulative \u00b6 type : I64 CUMULATIVE is an AggregationTemporality for a metric aggregator which reports changes since a fixed start time. This means that current values of a CUMULATIVE metric depend on all previous measurements since the start time. Because of this, the sender is required to retain this state in some form. If this state is lost or invalidated, the CUMULATIVE metric values MUST be reset and a new fixed start time following the last reported measurement time sent MUST be used. For example, consider a system measuring the number of requests that it receives and reports the sum of these requests every second as a CUMULATIVE metric: The system starts receiving at time=t_0. A request is received, the system measures 1 request. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+1 with a value of 3. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+2 with a value of 5. The system experiences a fault and loses state. The system recovers and resumes receiving at time=t_1. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_1 to t_0+1 with a value of 1. Note: Even though, when reporting changes since last report time, using CUMULATIVE is valid, it is not recommended. This may cause problems for systems that do not use start_time to determine when the aggregation value was reset (e.g. Prometheus).","title":"cncf::otel::metrics::temporality"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/#temporality","text":"This module is a mapping of the AggregationTemporality field in CNCF OpenTelemetry trace spans. The text of the specification has been copied into this mapping for convenience. Where text from the CNCF OpenTelemetry specification is used it is provided in block quotes. From the specification: AggregationTemporality defines how a metric aggregator reports aggregated values. It describes how those values relate to the time interval over which they are aggregated.","title":"temporality"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/#constants","text":"","title":"Constants"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/#unspecified","text":"type : I64 The default AggregationTemparality - it must not be used conforming implementation will set the temporality to one of the valid enumeration values.","title":"unspecified"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/#delta","text":"type : I64 DELTA is an AggregationTemporality for a metric aggregator which reports changes since last report time. Successive metrics contain aggregation of values from continuous and non-overlapping intervals. The values for a DELTA metric are based only on the time interval associated with one measurement cycle. There is no dependency on previous measurements like is the case for CUMULATIVE metrics. For example, consider a system measuring the number of requests that it receives and reports the sum of these requests every second as a DELTA metric: The system starts receiving at time=t_0. A request is received, the system measures 1 request. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+1 with a value of 3. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0+1 to t_0+2 with a value of 2.","title":"delta"},{"location":"tremor-script/stdlib/cncf/otel/metrics/temporality/#cumulative","text":"type : I64 CUMULATIVE is an AggregationTemporality for a metric aggregator which reports changes since a fixed start time. This means that current values of a CUMULATIVE metric depend on all previous measurements since the start time. Because of this, the sender is required to retain this state in some form. If this state is lost or invalidated, the CUMULATIVE metric values MUST be reset and a new fixed start time following the last reported measurement time sent MUST be used. For example, consider a system measuring the number of requests that it receives and reports the sum of these requests every second as a CUMULATIVE metric: The system starts receiving at time=t_0. A request is received, the system measures 1 request. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+1 with a value of 3. A request is received, the system measures 1 request. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_0 to t_0+2 with a value of 5. The system experiences a fault and loses state. The system recovers and resumes receiving at time=t_1. A request is received, the system measures 1 request. The 1 second collection cycle ends. A metric is exported for the number of requests received over the interval of time t_1 to t_0+1 with a value of 1. Note: Even though, when reporting changes since last report time, using CUMULATIVE is valid, it is not recommended. This may cause problems for systems that do not use start_time to determine when the aggregation value was reset (e.g. Prometheus).","title":"cumulative"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/","text":"spankind \u00b6 SpanKind is the type of span. Can be used to specify additional relationships between spans in addition to a parent/child relationship. Constants \u00b6 unspecified \u00b6 type : I64 Unspecified. Do NOT use as default. Implementations MAY assume SpanKind to be INTERNAL when receiving UNSPECIFIED . internal \u00b6 type : I64 Indicates that the span represents an internal operation within an application, as opposed to an operation happening at the boundaries. Default value. server \u00b6 type : I64 Indicates that the span covers server-side handling of an RPC or other remote network request. client \u00b6 type : I64 Indicates that the span describes a request to some remote service. producer \u00b6 type : I64 Indicates that the span describes a producer sending a message to a broker. Unlike CLIENT and SERVER , there is often no direct critical path latency relationship between producer and consumer spans. A PRODUCER span ends when the message was accepted by the broker while the logical processing of the message might span a much longer time. consumer \u00b6 type : I64 Indicates that the span describes consumer receiving a message from a broker. Like the PRODUCER kind, there is often no direct critical path latency relationship between producer and consumer spans. Functions \u00b6 make_default() \u00b6 The default function returns the preferred default spankind if/when none is specified","title":"cncf::otel::trace::spankind"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#spankind","text":"SpanKind is the type of span. Can be used to specify additional relationships between spans in addition to a parent/child relationship.","title":"spankind"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#constants","text":"","title":"Constants"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#unspecified","text":"type : I64 Unspecified. Do NOT use as default. Implementations MAY assume SpanKind to be INTERNAL when receiving UNSPECIFIED .","title":"unspecified"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#internal","text":"type : I64 Indicates that the span represents an internal operation within an application, as opposed to an operation happening at the boundaries. Default value.","title":"internal"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#server","text":"type : I64 Indicates that the span covers server-side handling of an RPC or other remote network request.","title":"server"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#client","text":"type : I64 Indicates that the span describes a request to some remote service.","title":"client"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#producer","text":"type : I64 Indicates that the span describes a producer sending a message to a broker. Unlike CLIENT and SERVER , there is often no direct critical path latency relationship between producer and consumer spans. A PRODUCER span ends when the message was accepted by the broker while the logical processing of the message might span a much longer time.","title":"producer"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#consumer","text":"type : I64 Indicates that the span describes consumer receiving a message from a broker. Like the PRODUCER kind, there is often no direct critical path latency relationship between producer and consumer spans.","title":"consumer"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/trace/spankind/#make_default","text":"The default function returns the preferred default spankind if/when none is specified","title":"make_default()"},{"location":"tremor-script/stdlib/cncf/otel/trace/status/","text":"status \u00b6 The Status type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. IMPORTANT: Backward compatibility notes: To ensure any pair of senders and receivers continues to correctly signal and interpret erroneous situations, the senders and receivers MUST follow these rules: Old senders and receivers that are not aware of code field will continue using the deprecated_code field to signal and interpret erroneous situation. New senders, which are aware of the code field MUST set both the deprecated_code and code fields according to the following rules: if code==STATUS_CODE_UNSET then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_OK. if code==STATUS_CODE_OK then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_OK. if code==STATUS_CODE_ERROR then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_UNKNOWN_ERROR. These rules allow old receivers to correctly interpret data received from new senders. New receivers MUST look at both the code and deprecated_code fields in order to interpret the overall status: If code==STATUS_CODE_UNSET then the value of deprecated_code is the carrier of the overall status according to these rules: if deprecated_code == DEPRECATED_STATUS_CODE_OK then the receiver MUST interpret the overall status to be STATUS_CODE_UNSET . if deprecated_code != DEPRECATED_STATUS_CODE_OK then the receiver MUST interpret the overall status to be STATUS_CODE_ERROR . If code!=STATUS_CODE_UNSET then the value of deprecated_code MUST be ignored, the code field is the sole carrier of the status. These rules allow new receivers to correctly interpret data received from old senders. Functions \u00b6 ok() \u00b6 Returns a success status code error(message) \u00b6 Returns an error status code, with user defined error messsage","title":"cncf::otel::trace::status"},{"location":"tremor-script/stdlib/cncf/otel/trace/status/#status","text":"The Status type defines a logical error model that is suitable for different programming environments, including REST APIs and RPC APIs. IMPORTANT: Backward compatibility notes: To ensure any pair of senders and receivers continues to correctly signal and interpret erroneous situations, the senders and receivers MUST follow these rules: Old senders and receivers that are not aware of code field will continue using the deprecated_code field to signal and interpret erroneous situation. New senders, which are aware of the code field MUST set both the deprecated_code and code fields according to the following rules: if code==STATUS_CODE_UNSET then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_OK. if code==STATUS_CODE_OK then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_OK. if code==STATUS_CODE_ERROR then deprecated_code MUST be set to DEPRECATED_STATUS_CODE_UNKNOWN_ERROR. These rules allow old receivers to correctly interpret data received from new senders. New receivers MUST look at both the code and deprecated_code fields in order to interpret the overall status: If code==STATUS_CODE_UNSET then the value of deprecated_code is the carrier of the overall status according to these rules: if deprecated_code == DEPRECATED_STATUS_CODE_OK then the receiver MUST interpret the overall status to be STATUS_CODE_UNSET . if deprecated_code != DEPRECATED_STATUS_CODE_OK then the receiver MUST interpret the overall status to be STATUS_CODE_ERROR . If code!=STATUS_CODE_UNSET then the value of deprecated_code MUST be ignored, the code field is the sole carrier of the status. These rules allow new receivers to correctly interpret data received from old senders.","title":"status"},{"location":"tremor-script/stdlib/cncf/otel/trace/status/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/cncf/otel/trace/status/#ok","text":"Returns a success status code","title":"ok()"},{"location":"tremor-script/stdlib/cncf/otel/trace/status/#errormessage","text":"Returns an error status code, with user defined error messsage","title":"error(message)"},{"location":"tremor-script/stdlib/std/array/","text":"array \u00b6 The array module contains functions to work for arrays. Functions \u00b6 len(array) \u00b6 Returns the length of array . Returns an integer . is_empty(array) \u00b6 Returns if array is empty. Returns an bool . contains(array, element) \u00b6 Returns if array contains an element . Returns an bool . push(array, element) \u00b6 Adds an element to the end of array . Returns an array . zip(left, right) \u00b6 Zips two arrays, returning a new array of tuples for the first element being part of the left array and the second element part of the right array. Note : left and right need to have the same length. let left = [ 1 , 2 , 3 ]; let right = [ \"a\" , \"b\" , \"c\" ]; array :: zip ( left , right ) == [[ 1 , \"a\" ], [ 2 , \"b\" ], [ 3 , \"c\" ]] Returns an array . unzip(array) \u00b6 Unzips an array of tuples into an array of two arrays. Note : array's elements need to be arrays of two elements. array :: unzip ([[ 1 , \"a\" ], [ 2 , \"b\" ], [ 3 , \"c\" ]]) == [[ 1 , 2 , 3 ], [ \"a\" , \"b\" , \"c\" ]] Returns an array . flatten(array) \u00b6 Flattens a nested array recursively. array :: flatten ([[ 1 , 2 , 3 ], [ \"a\" , \"b\" , \"c\" ]]) = [ 1 , 2 , 3 , \"a\" , \"b\" , \"c\" ] Returns an array . coalesce(array) \u00b6 Returns the array for null values removed. array :: coalesce ([ 1 , null , 2 , null , 3 ]) = [ 1 , 2 , 3 ] Returns an array . join(array, string) \u00b6 Joins the elements of an array (turing them into Strings) for a given separator. array : join ([ \"this\" , \"is\" , \"a\" , \"cake\" ], \" \" ) => \"this is a cake\" Returns a string .","title":"std::array"},{"location":"tremor-script/stdlib/std/array/#array","text":"The array module contains functions to work for arrays.","title":"array"},{"location":"tremor-script/stdlib/std/array/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/array/#lenarray","text":"Returns the length of array . Returns an integer .","title":"len(array)"},{"location":"tremor-script/stdlib/std/array/#is_emptyarray","text":"Returns if array is empty. Returns an bool .","title":"is_empty(array)"},{"location":"tremor-script/stdlib/std/array/#containsarray-element","text":"Returns if array contains an element . Returns an bool .","title":"contains(array, element)"},{"location":"tremor-script/stdlib/std/array/#pusharray-element","text":"Adds an element to the end of array . Returns an array .","title":"push(array, element)"},{"location":"tremor-script/stdlib/std/array/#zipleft-right","text":"Zips two arrays, returning a new array of tuples for the first element being part of the left array and the second element part of the right array. Note : left and right need to have the same length. let left = [ 1 , 2 , 3 ]; let right = [ \"a\" , \"b\" , \"c\" ]; array :: zip ( left , right ) == [[ 1 , \"a\" ], [ 2 , \"b\" ], [ 3 , \"c\" ]] Returns an array .","title":"zip(left, right)"},{"location":"tremor-script/stdlib/std/array/#unziparray","text":"Unzips an array of tuples into an array of two arrays. Note : array's elements need to be arrays of two elements. array :: unzip ([[ 1 , \"a\" ], [ 2 , \"b\" ], [ 3 , \"c\" ]]) == [[ 1 , 2 , 3 ], [ \"a\" , \"b\" , \"c\" ]] Returns an array .","title":"unzip(array)"},{"location":"tremor-script/stdlib/std/array/#flattenarray","text":"Flattens a nested array recursively. array :: flatten ([[ 1 , 2 , 3 ], [ \"a\" , \"b\" , \"c\" ]]) = [ 1 , 2 , 3 , \"a\" , \"b\" , \"c\" ] Returns an array .","title":"flatten(array)"},{"location":"tremor-script/stdlib/std/array/#coalescearray","text":"Returns the array for null values removed. array :: coalesce ([ 1 , null , 2 , null , 3 ]) = [ 1 , 2 , 3 ] Returns an array .","title":"coalesce(array)"},{"location":"tremor-script/stdlib/std/array/#joinarray-string","text":"Joins the elements of an array (turing them into Strings) for a given separator. array : join ([ \"this\" , \"is\" , \"a\" , \"cake\" ], \" \" ) => \"this is a cake\" Returns a string .","title":"join(array, string)"},{"location":"tremor-script/stdlib/std/base64/","text":"base64 \u00b6 The base64 module contains functions to work with base64 encoding and decoding Functions \u00b6 encode(input) \u00b6 Encodes a binary as a base64 encoded string Returns a string decode(input) \u00b6 Decodes a base64 ebcided string into it's bytes Returns a binary","title":"std::base64"},{"location":"tremor-script/stdlib/std/base64/#base64","text":"The base64 module contains functions to work with base64 encoding and decoding","title":"base64"},{"location":"tremor-script/stdlib/std/base64/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/base64/#encodeinput","text":"Encodes a binary as a base64 encoded string Returns a string","title":"encode(input)"},{"location":"tremor-script/stdlib/std/base64/#decodeinput","text":"Decodes a base64 ebcided string into it's bytes Returns a binary","title":"decode(input)"},{"location":"tremor-script/stdlib/std/binary/","text":"binary \u00b6 The binary module handles interacting with the binary/bytes type Functions \u00b6 len(input) \u00b6 Returns the number of bytes in a binary Returns a integer from_bytes(input) \u00b6 Turns an array of bytes into a binary Returns a binary into_bytes(input) \u00b6 Turns a binary into an array of bytes Returns an array","title":"std::binary"},{"location":"tremor-script/stdlib/std/binary/#binary","text":"The binary module handles interacting with the binary/bytes type","title":"binary"},{"location":"tremor-script/stdlib/std/binary/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/binary/#leninput","text":"Returns the number of bytes in a binary Returns a integer","title":"len(input)"},{"location":"tremor-script/stdlib/std/binary/#from_bytesinput","text":"Turns an array of bytes into a binary Returns a binary","title":"from_bytes(input)"},{"location":"tremor-script/stdlib/std/binary/#into_bytesinput","text":"Turns a binary into an array of bytes Returns an array","title":"into_bytes(input)"},{"location":"tremor-script/stdlib/std/float/","text":"float \u00b6 The float module contains functions to work with floats. Functions \u00b6 parse(string) \u00b6 Parses a string as a float. Returns an float .","title":"std::float"},{"location":"tremor-script/stdlib/std/float/#float","text":"The float module contains functions to work with floats.","title":"float"},{"location":"tremor-script/stdlib/std/float/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/float/#parsestring","text":"Parses a string as a float. Returns an float .","title":"parse(string)"},{"location":"tremor-script/stdlib/std/integer/","text":"integer \u00b6 The integer module contains functions to work with integers. Constants \u00b6 min \u00b6 type : I64 Minimum valid integer value. Same as signed::min max \u00b6 type : I64 Maximum valid integer value. Same as signed::max Functions \u00b6 parse(string) \u00b6 Parses a string as an integer. Returns an integer .","title":"std::integer"},{"location":"tremor-script/stdlib/std/integer/#integer","text":"The integer module contains functions to work with integers.","title":"integer"},{"location":"tremor-script/stdlib/std/integer/#constants","text":"","title":"Constants"},{"location":"tremor-script/stdlib/std/integer/#min","text":"type : I64 Minimum valid integer value. Same as signed::min","title":"min"},{"location":"tremor-script/stdlib/std/integer/#max","text":"type : I64 Maximum valid integer value. Same as signed::max","title":"max"},{"location":"tremor-script/stdlib/std/integer/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/integer/#parsestring","text":"Parses a string as an integer. Returns an integer .","title":"parse(string)"},{"location":"tremor-script/stdlib/std/json/","text":"json \u00b6 The json module contains functions that work with son structures. Functions \u00b6 decode(string) \u00b6 Decodes a string containing a JSON structure. json :: decode ( \"[1, 2, 3, 4]\" ) => [ 1 , 2 , 3 , 4 ] Returns any type encode(any) \u00b6 Encodes a data structure into a json string using minimal encoding. json :: encode ([ 1 , 2 , 3 , 4 ]) = \"[1,2,3,4]\" Returns a string encode_pretty(any) \u00b6 Encodes a data structure into a prettified json string. json :: encode_pretty ([ 1 , 2 , 3 , 4 ]) = \"[ 1, 2, 3, 4 ]\" Returns a string","title":"std::json"},{"location":"tremor-script/stdlib/std/json/#json","text":"The json module contains functions that work with son structures.","title":"json"},{"location":"tremor-script/stdlib/std/json/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/json/#decodestring","text":"Decodes a string containing a JSON structure. json :: decode ( \"[1, 2, 3, 4]\" ) => [ 1 , 2 , 3 , 4 ] Returns any type","title":"decode(string)"},{"location":"tremor-script/stdlib/std/json/#encodeany","text":"Encodes a data structure into a json string using minimal encoding. json :: encode ([ 1 , 2 , 3 , 4 ]) = \"[1,2,3,4]\" Returns a string","title":"encode(any)"},{"location":"tremor-script/stdlib/std/json/#encode_prettyany","text":"Encodes a data structure into a prettified json string. json :: encode_pretty ([ 1 , 2 , 3 , 4 ]) = \"[ 1, 2, 3, 4 ]\" Returns a string","title":"encode_pretty(any)"},{"location":"tremor-script/stdlib/std/math/","text":"math \u00b6 The math module contains functions for common mathematical operations. Functions \u00b6 floor(n) \u00b6 Returns the smallest integer value less than or equal to n. math :: floor ( 42.9 ) == 42 Returns an integer ceil(n) \u00b6 Returns the largest integer value greater than or equal to n. math :: ceil ( 41.1 ) == 42 Returns an integer round(n) \u00b6 Returns the integer nearest to. math :: round ( 41.4 ) == 41 math :: round ( 41.5 ) == 42 Returns an integer trunc(n) \u00b6 Returns the integer part of n . math :: trunc ( 41.4 ) == 41 math :: trunc ( 41.5 ) == 41 Returns an integer max(n1, n2) \u00b6 Returns the maximum of two numbers. math :: max ( 41 , 42 ) == 42 Returns a number ( integer or float ) min(n1, n2) \u00b6 Returns the minimum of two numbers. math :: min ( 41 , 42 ) == 41 Returns a number ( integer or float )","title":"std::math"},{"location":"tremor-script/stdlib/std/math/#math","text":"The math module contains functions for common mathematical operations.","title":"math"},{"location":"tremor-script/stdlib/std/math/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/math/#floorn","text":"Returns the smallest integer value less than or equal to n. math :: floor ( 42.9 ) == 42 Returns an integer","title":"floor(n)"},{"location":"tremor-script/stdlib/std/math/#ceiln","text":"Returns the largest integer value greater than or equal to n. math :: ceil ( 41.1 ) == 42 Returns an integer","title":"ceil(n)"},{"location":"tremor-script/stdlib/std/math/#roundn","text":"Returns the integer nearest to. math :: round ( 41.4 ) == 41 math :: round ( 41.5 ) == 42 Returns an integer","title":"round(n)"},{"location":"tremor-script/stdlib/std/math/#truncn","text":"Returns the integer part of n . math :: trunc ( 41.4 ) == 41 math :: trunc ( 41.5 ) == 41 Returns an integer","title":"trunc(n)"},{"location":"tremor-script/stdlib/std/math/#maxn1-n2","text":"Returns the maximum of two numbers. math :: max ( 41 , 42 ) == 42 Returns a number ( integer or float )","title":"max(n1, n2)"},{"location":"tremor-script/stdlib/std/math/#minn1-n2","text":"Returns the minimum of two numbers. math :: min ( 41 , 42 ) == 41 Returns a number ( integer or float )","title":"min(n1, n2)"},{"location":"tremor-script/stdlib/std/random/","text":"random \u00b6 The random module contains functions for generating random values of various types. The generated values are uniformly distributed over the specified type (and range, where applicable). Useful for doing percentage drops of events, for example. The random number generator under the hood is seeded based on tremor's event ingestion time and thus the output here is deterministic. Should we choose to replay a tremor data dump (something to be added in future tremor versions), we will always get the same value for an event. Functions \u00b6 bool() \u00b6 Generates a random boolean. Returns an bool ; string(length) \u00b6 Generates a random string of given length with ASCII letters and numbers: a - z , A - Z and 0 - 9 . The argument must be an integer greater than or equal to zero -- otherwise the function errors out. random :: string ( 16 ) # 16 alphanumeric characters. eg: \"QuSFjpW8PBNewRml\" random :: string ( 0 ) # \"\" Returns an string integer() \u00b6 Generates a random integer , with the functionality changing based on the number of arguments passed. random :: integer ( 0 , 2 ) # either 0 or 1 random :: integer ( 42 , 43 ) # always 42 random :: integer ( 0 , 100 ) # one of 0-99 random :: integer (- 1 , 1 ) # either -1 or 0 random :: integer (- 42 , - 41 ) # always -42 random :: integer ( i ) -> integer random :: integer ( 100 ) # one of 0-99. same as random::integer(0, 100) random :: integer () -> integer float() \u00b6 Generates a random float, with the functionality changing based on the number of arguments passed. random :: float ( 0.0 , 100.0 ) # >= 0.0 and < 100.0 random :: float (- 1.0 , 1.0 ) # >= -1.0 and < 1.0 random :: float (- 3.0 , - 2.0 ) # >= -3.0 and < -2.0 random :: float ( 100.0 ) # same as random::float(0.0, 100.0) random :: float () -> float","title":"std::random"},{"location":"tremor-script/stdlib/std/random/#random","text":"The random module contains functions for generating random values of various types. The generated values are uniformly distributed over the specified type (and range, where applicable). Useful for doing percentage drops of events, for example. The random number generator under the hood is seeded based on tremor's event ingestion time and thus the output here is deterministic. Should we choose to replay a tremor data dump (something to be added in future tremor versions), we will always get the same value for an event.","title":"random"},{"location":"tremor-script/stdlib/std/random/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/random/#bool","text":"Generates a random boolean. Returns an bool ;","title":"bool()"},{"location":"tremor-script/stdlib/std/random/#stringlength","text":"Generates a random string of given length with ASCII letters and numbers: a - z , A - Z and 0 - 9 . The argument must be an integer greater than or equal to zero -- otherwise the function errors out. random :: string ( 16 ) # 16 alphanumeric characters. eg: \"QuSFjpW8PBNewRml\" random :: string ( 0 ) # \"\" Returns an string","title":"string(length)"},{"location":"tremor-script/stdlib/std/random/#integer","text":"Generates a random integer , with the functionality changing based on the number of arguments passed. random :: integer ( 0 , 2 ) # either 0 or 1 random :: integer ( 42 , 43 ) # always 42 random :: integer ( 0 , 100 ) # one of 0-99 random :: integer (- 1 , 1 ) # either -1 or 0 random :: integer (- 42 , - 41 ) # always -42 random :: integer ( i ) -> integer random :: integer ( 100 ) # one of 0-99. same as random::integer(0, 100) random :: integer () -> integer","title":"integer()"},{"location":"tremor-script/stdlib/std/random/#float","text":"Generates a random float, with the functionality changing based on the number of arguments passed. random :: float ( 0.0 , 100.0 ) # >= 0.0 and < 100.0 random :: float (- 1.0 , 1.0 ) # >= -1.0 and < 1.0 random :: float (- 3.0 , - 2.0 ) # >= -3.0 and < -2.0 random :: float ( 100.0 ) # same as random::float(0.0, 100.0) random :: float () -> float","title":"float()"},{"location":"tremor-script/stdlib/std/range/","text":"range \u00b6 The range module contains functions for common range generator operations. Functions \u00b6 range(a, b) \u00b6 Returns an array from a min-inclusive to b max-inclusive. range :: range ( 0 , 3 ) == [ 0 , 1 , 2 ] Returns [integer] contains(r, n) \u00b6 Checks if an element is within a range. range :: contains ( range :: range ( 0 , 3 ), 2 ) == true Returns [integer]","title":"std::range"},{"location":"tremor-script/stdlib/std/range/#range","text":"The range module contains functions for common range generator operations.","title":"range"},{"location":"tremor-script/stdlib/std/range/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/range/#rangea-b","text":"Returns an array from a min-inclusive to b max-inclusive. range :: range ( 0 , 3 ) == [ 0 , 1 , 2 ] Returns [integer]","title":"range(a, b)"},{"location":"tremor-script/stdlib/std/range/#containsr-n","text":"Checks if an element is within a range. range :: contains ( range :: range ( 0 , 3 ), 2 ) == true Returns [integer]","title":"contains(r, n)"},{"location":"tremor-script/stdlib/std/re/","text":"re \u00b6 The re module contains functions for regular expression handing. Please note that if applicable literal regular expressions are faster. Functions \u00b6 replace(regex, input, to) \u00b6 Replaces the first occurrence of regex in the input string with to. References to match groups can be done using $`` as either numbered references like $1 inserting the first capture or named using $foo` inserting the capture named foo. Returns a string replace_all(regex, input, to) \u00b6 Replaces all occurrences of regex in the input string with to. References to match groups can be done using $ as either numbered references like $1 inserting the first capture or named using $foo inserting the capture named foo. Returns a string is_match(regex, input) \u00b6 Returns if the regex machines input. Returns a bool split(regex, input) \u00b6 Splits the input string using the provided regular expression regex as separator. re :: split ( \" \" , \"this is a test\" ) == [ \"this\" , \"is\" , \"a\" , \"string\" ]. Returns a [string]","title":"std::re"},{"location":"tremor-script/stdlib/std/re/#re","text":"The re module contains functions for regular expression handing. Please note that if applicable literal regular expressions are faster.","title":"re"},{"location":"tremor-script/stdlib/std/re/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/re/#replaceregex-input-to","text":"Replaces the first occurrence of regex in the input string with to. References to match groups can be done using $`` as either numbered references like $1 inserting the first capture or named using $foo` inserting the capture named foo. Returns a string","title":"replace(regex, input, to)"},{"location":"tremor-script/stdlib/std/re/#replace_allregex-input-to","text":"Replaces all occurrences of regex in the input string with to. References to match groups can be done using $ as either numbered references like $1 inserting the first capture or named using $foo inserting the capture named foo. Returns a string","title":"replace_all(regex, input, to)"},{"location":"tremor-script/stdlib/std/re/#is_matchregex-input","text":"Returns if the regex machines input. Returns a bool","title":"is_match(regex, input)"},{"location":"tremor-script/stdlib/std/re/#splitregex-input","text":"Splits the input string using the provided regular expression regex as separator. re :: split ( \" \" , \"this is a test\" ) == [ \"this\" , \"is\" , \"a\" , \"string\" ]. Returns a [string]","title":"split(regex, input)"},{"location":"tremor-script/stdlib/std/record/","text":"record \u00b6 The record module contains functions to work with records. Functions \u00b6 len(record) \u00b6 Returns the length of an record (number of key value pairs). Returns an integer is_empty(record) \u00b6 Returns if an record is empty. Returns a bool contains(record, key) \u00b6 Returns if an record contains a given key. Returns a bool keys(record) \u00b6 Returns an array of record keys. record :: keys ( { \"a\" : 1 , \"b\" : 2 } ) == [ \"a\" , \"b\" ] Returns a [string] values(record) \u00b6 Returns an array of record values. record :: values ( { \"a\" : 1 , \"b\" : 2 } ) == [ 1 , 2 ] Returns a [any] to_array(record) \u00b6 Turns the record into an array of key value pairs. record :: to_array ( { \"a\" : 1 , \"b\" : 2 } ) == [[ \"a\" , 1 ], [ \"b\" , 2 ]] Returns a [(string, any)] from_array(array) \u00b6 Turns an array of key value pairs into an record. Note: array's elements need to be arrays of two elements with the first element being a string. record :: from_array ([[ \"a\" , 1 ], [ \"b\" , 2 ]]) == { \"a\" : 1 , \"b\" : 2 } Returns a record select(record, array) \u00b6 'Selects' a given set of field from an record, removing all others. record :: select ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } , [ \"a\" , \"c\" ]) == { \"a\" : 1 , \"c\" : 3 } Returns a record merge(left, right) \u00b6 Merges the two records left and right overwriting existing values in left with those provided in right record :: merge ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 4 } , { \"c\" : 3 , \"d\" : 4 } ) == { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 , \"d\" : 4 } Returns a record rename(target, changes) \u00b6 Renames the keys in the record target based on the key value pairs in the record changes where the key is the current name and the value is the new name. record :: rename ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 4 } , { \"a\" : \"A\" , \"b\" : \"B\" } ) == { \"A\" : 1 , \"B\" : 2 , \"c\" : 4 } Returns a record","title":"std::record"},{"location":"tremor-script/stdlib/std/record/#record","text":"The record module contains functions to work with records.","title":"record"},{"location":"tremor-script/stdlib/std/record/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/record/#lenrecord","text":"Returns the length of an record (number of key value pairs). Returns an integer","title":"len(record)"},{"location":"tremor-script/stdlib/std/record/#is_emptyrecord","text":"Returns if an record is empty. Returns a bool","title":"is_empty(record)"},{"location":"tremor-script/stdlib/std/record/#containsrecord-key","text":"Returns if an record contains a given key. Returns a bool","title":"contains(record, key)"},{"location":"tremor-script/stdlib/std/record/#keysrecord","text":"Returns an array of record keys. record :: keys ( { \"a\" : 1 , \"b\" : 2 } ) == [ \"a\" , \"b\" ] Returns a [string]","title":"keys(record)"},{"location":"tremor-script/stdlib/std/record/#valuesrecord","text":"Returns an array of record values. record :: values ( { \"a\" : 1 , \"b\" : 2 } ) == [ 1 , 2 ] Returns a [any]","title":"values(record)"},{"location":"tremor-script/stdlib/std/record/#to_arrayrecord","text":"Turns the record into an array of key value pairs. record :: to_array ( { \"a\" : 1 , \"b\" : 2 } ) == [[ \"a\" , 1 ], [ \"b\" , 2 ]] Returns a [(string, any)]","title":"to_array(record)"},{"location":"tremor-script/stdlib/std/record/#from_arrayarray","text":"Turns an array of key value pairs into an record. Note: array's elements need to be arrays of two elements with the first element being a string. record :: from_array ([[ \"a\" , 1 ], [ \"b\" , 2 ]]) == { \"a\" : 1 , \"b\" : 2 } Returns a record","title":"from_array(array)"},{"location":"tremor-script/stdlib/std/record/#selectrecord-array","text":"'Selects' a given set of field from an record, removing all others. record :: select ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } , [ \"a\" , \"c\" ]) == { \"a\" : 1 , \"c\" : 3 } Returns a record","title":"select(record, array)"},{"location":"tremor-script/stdlib/std/record/#mergeleft-right","text":"Merges the two records left and right overwriting existing values in left with those provided in right record :: merge ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 4 } , { \"c\" : 3 , \"d\" : 4 } ) == { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 , \"d\" : 4 } Returns a record","title":"merge(left, right)"},{"location":"tremor-script/stdlib/std/record/#renametarget-changes","text":"Renames the keys in the record target based on the key value pairs in the record changes where the key is the current name and the value is the new name. record :: rename ( { \"a\" : 1 , \"b\" : 2 , \"c\" : 4 } , { \"a\" : \"A\" , \"b\" : \"B\" } ) == { \"A\" : 1 , \"B\" : 2 , \"c\" : 4 } Returns a record","title":"rename(target, changes)"},{"location":"tremor-script/stdlib/std/string/","text":"string \u00b6 The string module contains functions that primarily work with strings. Functions \u00b6 format(format) \u00b6 The placeholder {} is replaced by the arguments in the list in order. string :: format ( \"the {} is {}.\" , \"meaning of life\" , 42 ) would result in the string \"the meaning of life is 42\" To use { or } as string literals in your format string, it needs to be escapedby adding another parenthesis of the same type. string :: format ( \"{{ this is a string format in parenthesis }}\" ) this will output: \"{ this is a string format in parenthesis }\" Returns a string is_empty(input) \u00b6 Returns if the input string is empty or not. Returns a bool len(input) \u00b6 Returns the length of the input string (counted as utf8 characters not bytes!). Returns an integer bytes(input) \u00b6 Returns the number of bytes composing the input string (may not be equivalent to the number of characters!). Returns an integer replace(input, from, to) \u00b6 Replaces all occurrences of from in Input to to. Returns a string trim(input) \u00b6 Trims whitespaces both at the start and end of the input string. Returns a string trim_start(input) \u00b6 Trims whitespaces at the start of the input string. Returns a string trim_end(input) \u00b6 Trims whitespaces at the end of the input string. Returns a string lowercase(input) \u00b6 Turns all characters in the input string to lower case. Returns a string uppercase(input) \u00b6 Turns all characters in the input string to upper case. Returns a string capitalize(input) \u00b6 Turns the first character in the input string to upper case. This does not ignore leading non letters! Returns a string substr(input, start, end) \u00b6 Get all characters from index start to end-1. Returns a string split(input, separator) \u00b6 Splits the input string at every occurrence of the separator string and turns the result in an array. Returns a string contains(input, string) \u00b6 Returns if the input string contains another string or not. Returns a bool from_utf8_lossy(bytes) \u00b6 Turns a binary into a utf8 string, potentally discarding invalid codepoints Returns a string into_binary(bytes) \u00b6 Turns a string into it's binary representation Returns a binary","title":"std::string"},{"location":"tremor-script/stdlib/std/string/#string","text":"The string module contains functions that primarily work with strings.","title":"string"},{"location":"tremor-script/stdlib/std/string/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/string/#formatformat","text":"The placeholder {} is replaced by the arguments in the list in order. string :: format ( \"the {} is {}.\" , \"meaning of life\" , 42 ) would result in the string \"the meaning of life is 42\" To use { or } as string literals in your format string, it needs to be escapedby adding another parenthesis of the same type. string :: format ( \"{{ this is a string format in parenthesis }}\" ) this will output: \"{ this is a string format in parenthesis }\" Returns a string","title":"format(format)"},{"location":"tremor-script/stdlib/std/string/#is_emptyinput","text":"Returns if the input string is empty or not. Returns a bool","title":"is_empty(input)"},{"location":"tremor-script/stdlib/std/string/#leninput","text":"Returns the length of the input string (counted as utf8 characters not bytes!). Returns an integer","title":"len(input)"},{"location":"tremor-script/stdlib/std/string/#bytesinput","text":"Returns the number of bytes composing the input string (may not be equivalent to the number of characters!). Returns an integer","title":"bytes(input)"},{"location":"tremor-script/stdlib/std/string/#replaceinput-from-to","text":"Replaces all occurrences of from in Input to to. Returns a string","title":"replace(input, from, to)"},{"location":"tremor-script/stdlib/std/string/#triminput","text":"Trims whitespaces both at the start and end of the input string. Returns a string","title":"trim(input)"},{"location":"tremor-script/stdlib/std/string/#trim_startinput","text":"Trims whitespaces at the start of the input string. Returns a string","title":"trim_start(input)"},{"location":"tremor-script/stdlib/std/string/#trim_endinput","text":"Trims whitespaces at the end of the input string. Returns a string","title":"trim_end(input)"},{"location":"tremor-script/stdlib/std/string/#lowercaseinput","text":"Turns all characters in the input string to lower case. Returns a string","title":"lowercase(input)"},{"location":"tremor-script/stdlib/std/string/#uppercaseinput","text":"Turns all characters in the input string to upper case. Returns a string","title":"uppercase(input)"},{"location":"tremor-script/stdlib/std/string/#capitalizeinput","text":"Turns the first character in the input string to upper case. This does not ignore leading non letters! Returns a string","title":"capitalize(input)"},{"location":"tremor-script/stdlib/std/string/#substrinput-start-end","text":"Get all characters from index start to end-1. Returns a string","title":"substr(input, start, end)"},{"location":"tremor-script/stdlib/std/string/#splitinput-separator","text":"Splits the input string at every occurrence of the separator string and turns the result in an array. Returns a string","title":"split(input, separator)"},{"location":"tremor-script/stdlib/std/string/#containsinput-string","text":"Returns if the input string contains another string or not. Returns a bool","title":"contains(input, string)"},{"location":"tremor-script/stdlib/std/string/#from_utf8_lossybytes","text":"Turns a binary into a utf8 string, potentally discarding invalid codepoints Returns a string","title":"from_utf8_lossy(bytes)"},{"location":"tremor-script/stdlib/std/string/#into_binarybytes","text":"Turns a string into it's binary representation Returns a binary","title":"into_binary(bytes)"},{"location":"tremor-script/stdlib/std/test/","text":"test \u00b6 The test module is used for writing tremor unit tests. Functions \u00b6 assert(name, expected, got) \u00b6 Runs an assertion for a test, ensures that expected and got are the same. If not errors. WARNING : Do not run assertions in production code! assert ( \"one equals one\" , 1 , 1 ) == true ; # suces assert ( \"one equals one\" , 1 , 2 ); # errors Returns an bool . suite() \u00b6 The suite function is an entrypoint into tremor's integrated unit testing framework test() \u00b6 The test function is an entrypoint into tremor's integrated unit testing framework","title":"std::test"},{"location":"tremor-script/stdlib/std/test/#test","text":"The test module is used for writing tremor unit tests.","title":"test"},{"location":"tremor-script/stdlib/std/test/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/test/#assertname-expected-got","text":"Runs an assertion for a test, ensures that expected and got are the same. If not errors. WARNING : Do not run assertions in production code! assert ( \"one equals one\" , 1 , 1 ) == true ; # suces assert ( \"one equals one\" , 1 , 2 ); # errors Returns an bool .","title":"assert(name, expected, got)"},{"location":"tremor-script/stdlib/std/test/#suite","text":"The suite function is an entrypoint into tremor's integrated unit testing framework","title":"suite()"},{"location":"tremor-script/stdlib/std/test/#test_1","text":"The test function is an entrypoint into tremor's integrated unit testing framework","title":"test()"},{"location":"tremor-script/stdlib/std/type/","text":"type \u00b6 The type module contains functions that help inspecting types of values. Functions \u00b6 as_string(value) \u00b6 Returns a string representation for the value type: \"null\" \"bool\" \"integer\" \"float\" \"string\" \"array\" \"record\" Returns a string is_null(value) \u00b6 Returns if the value is null. Returns a bool is_bool(value) \u00b6 Returns if the value is a boolean. Returns a bool is_integer(value) \u00b6 Returns if the value is an integer. Returns a bool is_float(value) \u00b6 Returns if the value is a float. Returns a bool is_number(value) \u00b6 Returns if the value is either a float or an integer. Returns a bool is_string(value) \u00b6 Returns if the value is a string. Returns a bool is_array(value) \u00b6 Returns if the value is an array. Returns a bool is_record(value) \u00b6 Returns if the value is a record. Returns a bool is_binary(value) \u00b6 Returns if the value is a binary. Returns a bool","title":"std::type"},{"location":"tremor-script/stdlib/std/type/#type","text":"The type module contains functions that help inspecting types of values.","title":"type"},{"location":"tremor-script/stdlib/std/type/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/type/#as_stringvalue","text":"Returns a string representation for the value type: \"null\" \"bool\" \"integer\" \"float\" \"string\" \"array\" \"record\" Returns a string","title":"as_string(value)"},{"location":"tremor-script/stdlib/std/type/#is_nullvalue","text":"Returns if the value is null. Returns a bool","title":"is_null(value)"},{"location":"tremor-script/stdlib/std/type/#is_boolvalue","text":"Returns if the value is a boolean. Returns a bool","title":"is_bool(value)"},{"location":"tremor-script/stdlib/std/type/#is_integervalue","text":"Returns if the value is an integer. Returns a bool","title":"is_integer(value)"},{"location":"tremor-script/stdlib/std/type/#is_floatvalue","text":"Returns if the value is a float. Returns a bool","title":"is_float(value)"},{"location":"tremor-script/stdlib/std/type/#is_numbervalue","text":"Returns if the value is either a float or an integer. Returns a bool","title":"is_number(value)"},{"location":"tremor-script/stdlib/std/type/#is_stringvalue","text":"Returns if the value is a string. Returns a bool","title":"is_string(value)"},{"location":"tremor-script/stdlib/std/type/#is_arrayvalue","text":"Returns if the value is an array. Returns a bool","title":"is_array(value)"},{"location":"tremor-script/stdlib/std/type/#is_recordvalue","text":"Returns if the value is a record. Returns a bool","title":"is_record(value)"},{"location":"tremor-script/stdlib/std/type/#is_binaryvalue","text":"Returns if the value is a binary. Returns a bool","title":"is_binary(value)"},{"location":"tremor-script/stdlib/std/url/","text":"url \u00b6 The url module contains functions to work on urls Functions \u00b6 encode(str) \u00b6 Returns a url encoded UTF-8 string Returns a string decode(str) \u00b6 Returns a decoded UTF-8 url encoded string Returns a string","title":"std::url"},{"location":"tremor-script/stdlib/std/url/#url","text":"The url module contains functions to work on urls","title":"url"},{"location":"tremor-script/stdlib/std/url/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/std/url/#encodestr","text":"Returns a url encoded UTF-8 string Returns a string","title":"encode(str)"},{"location":"tremor-script/stdlib/std/url/#decodestr","text":"Returns a decoded UTF-8 url encoded string Returns a string","title":"decode(str)"},{"location":"tremor-script/stdlib/tremor/chash/","text":"chash \u00b6 The chash module contains functions for consistent hashing of values. This can be used to achieve consistent routing over multiple outputs. Functions \u00b6 jump(key, slot_count) \u00b6 Hashes an input key (string) and determine its placement in a slot list. For example It can be used to pick a routing destination using an array of hosts: let hosts = [ \"host1\" , \"host2\" , \"host3\" , \"host4\" , \"host5\" ]; { \"key1\" : hosts [ chash :: jump ( \"key1\" , array :: len ( hosts ))], \"key1_again\" : hosts [ chash :: jump ( \"key1\" , array :: len ( hosts ))], \"key2\" : hosts [ chash :: jump ( \"key2\" , array :: len ( hosts ))], \"key3\" : hosts [ chash :: jump ( \"key3\" , array :: len ( hosts ))], \"key4\" : hosts [ chash :: jump ( \"key4\" , array :: len ( hosts ))], } Returns an integer . jump_with_keys(k1, k2, key, slot_count) \u00b6 The same as chash::jump but uses the integers k1 and k2 to initialise the hashing instead of using default values. Returns an integer sorted_serialize(any) \u00b6 serialised the given data in a sorted and repeatable fashion no matter how data is internally stored. In comparison, the normal serialisation functions do not ensure order for performance reasons. Their behaviour is well suited for encoding data on the wire, but in the context of consistent hashing we need to guarantee that data is always encoded to the same serialisation on a byte level not only on the logical level Returns an string","title":"tremor::chash"},{"location":"tremor-script/stdlib/tremor/chash/#chash","text":"The chash module contains functions for consistent hashing of values. This can be used to achieve consistent routing over multiple outputs.","title":"chash"},{"location":"tremor-script/stdlib/tremor/chash/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/tremor/chash/#jumpkey-slot_count","text":"Hashes an input key (string) and determine its placement in a slot list. For example It can be used to pick a routing destination using an array of hosts: let hosts = [ \"host1\" , \"host2\" , \"host3\" , \"host4\" , \"host5\" ]; { \"key1\" : hosts [ chash :: jump ( \"key1\" , array :: len ( hosts ))], \"key1_again\" : hosts [ chash :: jump ( \"key1\" , array :: len ( hosts ))], \"key2\" : hosts [ chash :: jump ( \"key2\" , array :: len ( hosts ))], \"key3\" : hosts [ chash :: jump ( \"key3\" , array :: len ( hosts ))], \"key4\" : hosts [ chash :: jump ( \"key4\" , array :: len ( hosts ))], } Returns an integer .","title":"jump(key, slot_count)"},{"location":"tremor-script/stdlib/tremor/chash/#jump_with_keysk1-k2-key-slot_count","text":"The same as chash::jump but uses the integers k1 and k2 to initialise the hashing instead of using default values. Returns an integer","title":"jump_with_keys(k1, k2, key, slot_count)"},{"location":"tremor-script/stdlib/tremor/chash/#sorted_serializeany","text":"serialised the given data in a sorted and repeatable fashion no matter how data is internally stored. In comparison, the normal serialisation functions do not ensure order for performance reasons. Their behaviour is well suited for encoding data on the wire, but in the context of consistent hashing we need to guarantee that data is always encoded to the same serialisation on a byte level not only on the logical level Returns an string","title":"sorted_serialize(any)"},{"location":"tremor-script/stdlib/tremor/origin/","text":"origin \u00b6 The origin module contains functions for retrieving relevant origin metadata about events (eg: host sending the event). The metadata is generated by onramps and is passed down to the tremor pipeline as a URI. Internally, the URI is composed of these basic fields: scheme host port path The URI format was chosen so that onramps can expose origin information in a structured manner, without sacrificing the ability to encode information that can vary from onramp to onramp. Since the kind and amount of origin information varies based on onramp (especially for the path field), the specifics of what each onramp exposes is documented in the onramps page. Functions \u00b6 scheme() \u00b6 Returns the origin URI scheme, or null value if URI is not set. Encodes the source of events within tremor (i.e. onramp name). For example, with udp onramp: origin :: scheme () == \"tremor-udp\" Returns string or null host() \u00b6 Returns the origin URI host, or null value if URI is not set. Encodes the source host (usually IP) that sent the event. For example, with udp onramp and for a test event sent from the same host as tremor: origin :: host () # returns \"127.0.0.1\" Returns string or null port() \u00b6 Returns the origin URI port, or null value if not set (either the whole URI or just the port). Encodes the source port on the host that sent the event. For example, with udp onramp: origin :: port () # returns an ephemeral port on the sender host (eg: 41371) Returns integer or null path() \u00b6 Returns the origin URI path as an array (with path segments constituting the array members), or null value if URI is not set. Encodes information specific to the onramp. The array structure here allows for capturing multiple details about the origin, in a manner that's easily accessible from tremor-script (position-based retrieval). For example, with udp onramp receiving events on port 12202: origin :: path () # returns [\"12202\"] as_uri_string() \u00b6 Returns the full origin URI as a string, or null value if URI is not set. The string is of the following standard form (with port as optional): <scheme>://<host>[:<port>]/<path> For example, with udp onramp receiving events on port 12202 from the same host as tremor: origin :: as_uri_string () # returns \"tremor-udp://127.0.0.1:41371/12202\", # where 41371 is the ephemeral port on the sending # side as_uri_record() \u00b6 Returns the full origin URI as a record, or null value if URI is not set. For example, with udp onramp receiving events on port 12202 from the same host as tremor: origin :: as_uri_record () == { \"scheme\" : \"tremor-udp\" , \"host\" : \"127.0.0.1\" , \"port\" : 41371 , # where 41371 is the ephemeral port on the sending side \"path\" :[ \"12202\" ] }","title":"tremor::origin"},{"location":"tremor-script/stdlib/tremor/origin/#origin","text":"The origin module contains functions for retrieving relevant origin metadata about events (eg: host sending the event). The metadata is generated by onramps and is passed down to the tremor pipeline as a URI. Internally, the URI is composed of these basic fields: scheme host port path The URI format was chosen so that onramps can expose origin information in a structured manner, without sacrificing the ability to encode information that can vary from onramp to onramp. Since the kind and amount of origin information varies based on onramp (especially for the path field), the specifics of what each onramp exposes is documented in the onramps page.","title":"origin"},{"location":"tremor-script/stdlib/tremor/origin/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/tremor/origin/#scheme","text":"Returns the origin URI scheme, or null value if URI is not set. Encodes the source of events within tremor (i.e. onramp name). For example, with udp onramp: origin :: scheme () == \"tremor-udp\" Returns string or null","title":"scheme()"},{"location":"tremor-script/stdlib/tremor/origin/#host","text":"Returns the origin URI host, or null value if URI is not set. Encodes the source host (usually IP) that sent the event. For example, with udp onramp and for a test event sent from the same host as tremor: origin :: host () # returns \"127.0.0.1\" Returns string or null","title":"host()"},{"location":"tremor-script/stdlib/tremor/origin/#port","text":"Returns the origin URI port, or null value if not set (either the whole URI or just the port). Encodes the source port on the host that sent the event. For example, with udp onramp: origin :: port () # returns an ephemeral port on the sender host (eg: 41371) Returns integer or null","title":"port()"},{"location":"tremor-script/stdlib/tremor/origin/#path","text":"Returns the origin URI path as an array (with path segments constituting the array members), or null value if URI is not set. Encodes information specific to the onramp. The array structure here allows for capturing multiple details about the origin, in a manner that's easily accessible from tremor-script (position-based retrieval). For example, with udp onramp receiving events on port 12202: origin :: path () # returns [\"12202\"]","title":"path()"},{"location":"tremor-script/stdlib/tremor/origin/#as_uri_string","text":"Returns the full origin URI as a string, or null value if URI is not set. The string is of the following standard form (with port as optional): <scheme>://<host>[:<port>]/<path> For example, with udp onramp receiving events on port 12202 from the same host as tremor: origin :: as_uri_string () # returns \"tremor-udp://127.0.0.1:41371/12202\", # where 41371 is the ephemeral port on the sending # side","title":"as_uri_string()"},{"location":"tremor-script/stdlib/tremor/origin/#as_uri_record","text":"Returns the full origin URI as a record, or null value if URI is not set. For example, with udp onramp receiving events on port 12202 from the same host as tremor: origin :: as_uri_record () == { \"scheme\" : \"tremor-udp\" , \"host\" : \"127.0.0.1\" , \"port\" : 41371 , # where 41371 is the ephemeral port on the sending side \"path\" :[ \"12202\" ] }","title":"as_uri_record()"},{"location":"tremor-script/stdlib/tremor/system/","text":"system \u00b6 The system namespace contains functions that provide information about the tremor runtime system. Functions \u00b6 hostname() \u00b6 Returns the name of the host where tremor is running. Returns a string ingest_ns() \u00b6 Returns the ingest time into tremor of the current event. Returns a string instance() \u00b6 Returns the instance of tremor. Returns a string nanotime() \u00b6 Returns the current time in epoch nanoseconds version() \u00b6 Returns the tremor version","title":"tremor::system"},{"location":"tremor-script/stdlib/tremor/system/#system","text":"The system namespace contains functions that provide information about the tremor runtime system.","title":"system"},{"location":"tremor-script/stdlib/tremor/system/#functions","text":"","title":"Functions"},{"location":"tremor-script/stdlib/tremor/system/#hostname","text":"Returns the name of the host where tremor is running. Returns a string","title":"hostname()"},{"location":"tremor-script/stdlib/tremor/system/#ingest_ns","text":"Returns the ingest time into tremor of the current event. Returns a string","title":"ingest_ns()"},{"location":"tremor-script/stdlib/tremor/system/#instance","text":"Returns the instance of tremor. Returns a string","title":"instance()"},{"location":"tremor-script/stdlib/tremor/system/#nanotime","text":"Returns the current time in epoch nanoseconds","title":"nanotime()"},{"location":"tremor-script/stdlib/tremor/system/#version","text":"Returns the tremor version","title":"version()"},{"location":"workshop/","text":"Tremor Workshop \u00b6 This workshop serves as a getting started laboratory for downloading, compiling and running tremor on development machines and developing tremor-based solutions. Table of Contents \u00b6 Development Environment Download and environment setup \u00b6 Download and build quick start \u00b6 Make sure your environment is setup for building tremor: Tremor Quick Start Setup support for your IDE \u00b6 Tremor supports language server extensions for VS Code and VIM text editors/IDEs: Tremor Language Server Make sure tremor binary is on your PATH \u00b6 $ export PATH = /Path/to/tremor-src-repo/target/debug/: $PATH $ tremor --version tremor 0 .9.0","title":"Tremor Workshop"},{"location":"workshop/#tremor-workshop","text":"This workshop serves as a getting started laboratory for downloading, compiling and running tremor on development machines and developing tremor-based solutions.","title":"Tremor Workshop"},{"location":"workshop/#table-of-contents","text":"Development Environment","title":"Table of Contents"},{"location":"workshop/#download-and-environment-setup","text":"","title":"Download and environment setup"},{"location":"workshop/#download-and-build-quick-start","text":"Make sure your environment is setup for building tremor: Tremor Quick Start","title":"Download and build quick start"},{"location":"workshop/#setup-support-for-your-ide","text":"Tremor supports language server extensions for VS Code and VIM text editors/IDEs: Tremor Language Server","title":"Setup support for your IDE"},{"location":"workshop/#make-sure-tremor-binary-is-on-your-path","text":"$ export PATH = /Path/to/tremor-src-repo/target/debug/: $PATH $ tremor --version tremor 0 .9.0","title":"Make sure tremor binary is on your PATH"},{"location":"workshop/examples/00_passthrough/","text":"Passthrough \u00b6 The passthrough example is the simplest possible configuration of tremor. It shows the very basic building blocks: Onramp, Offramp, Binding, Mapping and Pipeline. Environment \u00b6 The onramp we use is a websocket onramp listening on port 4242, it receives JSON formatted messages. onramp : - id : ws-input # A unique id for binding/mapping type : ws # The unique type descriptor for the onramp ( websocket server here) codec : json # The underlying data format expected for application payload data config : port : 4242 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to the pipeline example in the example.trickle file using the trickle query language to express its logic. The offramp we used is a console offramp producing to standard output. It receives JSON formatted messages. offramp : - id : stdout-output # The unique id for binding/mapping type : stdout # The unique type descriptor for the offramp ( stdout here ) codec : json # The underlying data format expected for application payload data config : prefix : \">> \" # A prefix for data emitted on standard output by this offramp The binding expresses those relations and gives the graph of onramp, pipeline and offramp. binding : - id : example # The unique name of this binding template links : \"/onramp/ws-input/{instance}/out\" : # Connect the input to the pipeline - \"/pipeline/example/{instance}/in\" \"/pipeline/example/{instance}/out\" : # Connect the pipeline to the output - \"/offramp/stdout-output/{instance}/in\" Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Run a the passthrough query against a sample input.json $ tremor run -i input.json etc/tremor/config/example.trickle { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"world\" , \"selected\" : false } >> { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ...","title":"Passthrough"},{"location":"workshop/examples/00_passthrough/#passthrough","text":"The passthrough example is the simplest possible configuration of tremor. It shows the very basic building blocks: Onramp, Offramp, Binding, Mapping and Pipeline.","title":"Passthrough"},{"location":"workshop/examples/00_passthrough/#environment","text":"The onramp we use is a websocket onramp listening on port 4242, it receives JSON formatted messages. onramp : - id : ws-input # A unique id for binding/mapping type : ws # The unique type descriptor for the onramp ( websocket server here) codec : json # The underlying data format expected for application payload data config : port : 4242 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to the pipeline example in the example.trickle file using the trickle query language to express its logic. The offramp we used is a console offramp producing to standard output. It receives JSON formatted messages. offramp : - id : stdout-output # The unique id for binding/mapping type : stdout # The unique type descriptor for the offramp ( stdout here ) codec : json # The underlying data format expected for application payload data config : prefix : \">> \" # A prefix for data emitted on standard output by this offramp The binding expresses those relations and gives the graph of onramp, pipeline and offramp. binding : - id : example # The unique name of this binding template links : \"/onramp/ws-input/{instance}/out\" : # Connect the input to the pipeline - \"/pipeline/example/{instance}/in\" \"/pipeline/example/{instance}/out\" : # Connect the pipeline to the output - \"/offramp/stdout-output/{instance}/in\" Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding.","title":"Environment"},{"location":"workshop/examples/00_passthrough/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/00_passthrough/#command-line-testing-during-logic-development","text":"Run a the passthrough query against a sample input.json $ tremor run -i input.json etc/tremor/config/example.trickle { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"world\" , \"selected\" : false } >> { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ...","title":"Command line testing during logic development"},{"location":"workshop/examples/01_filter/","text":"Filter \u00b6 The filter builds on the passthrough example and extends the example.trickle by adding a filter on the field selected . Only if this field is set to true the event will pass. Environment \u00b6 It connects to the pipeline example in the example.trickle file using the trickle query language to express the filtering logic. All other configuration is the same as per the passthrough example, and is elided here for brevity. Business Logic \u00b6 select event from in where event . selected into out Command line testing during logic development \u00b6 Run a the passthrough query against a sample input.json $ tremor run -i input.json etc/tremor/config/example.trickle { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ... Discussion \u00b6 Filters in tremor query ( trickle ) can be any legal predicate expression ( boolean returning expression or function call ). For example: Where clause \u00b6 Events are selected on the inbound event if the numeric field on the inbound event is less than or equal to 10 or greater than or equal to 100 . select event from in where event . numeric <= 10 or event . numeric >= 100 into out Having clause \u00b6 Events are selected after processing them if the selected field on the outbound event is true. select event from in into out having event . selected Where and Having clauses \u00b6 Events are selected on the inbound event if the numeric field on the inbound event is less than or equal to 10 or greater than or equal to 100 and after processing them, the selected field on the outbound event is true. select event from in where event . numeric <= 10 or event . numeric >= 100 into out having event . selected Tip The where clause has to be located after the from section in a trickle select expression! The where clause is evaluated on the incoming event. The having clause can be used to filter events, but it has to appear after the into expression and will be evaluated on the resulting produced event prior to passing it on. The where and having clauses are optional in trickle select query statements.","title":"Filter"},{"location":"workshop/examples/01_filter/#filter","text":"The filter builds on the passthrough example and extends the example.trickle by adding a filter on the field selected . Only if this field is set to true the event will pass.","title":"Filter"},{"location":"workshop/examples/01_filter/#environment","text":"It connects to the pipeline example in the example.trickle file using the trickle query language to express the filtering logic. All other configuration is the same as per the passthrough example, and is elided here for brevity.","title":"Environment"},{"location":"workshop/examples/01_filter/#business-logic","text":"select event from in where event . selected into out","title":"Business Logic"},{"location":"workshop/examples/01_filter/#command-line-testing-during-logic-development","text":"Run a the passthrough query against a sample input.json $ tremor run -i input.json etc/tremor/config/example.trickle { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ...","title":"Command line testing during logic development"},{"location":"workshop/examples/01_filter/#discussion","text":"Filters in tremor query ( trickle ) can be any legal predicate expression ( boolean returning expression or function call ). For example:","title":"Discussion"},{"location":"workshop/examples/01_filter/#where-clause","text":"Events are selected on the inbound event if the numeric field on the inbound event is less than or equal to 10 or greater than or equal to 100 . select event from in where event . numeric <= 10 or event . numeric >= 100 into out","title":"Where clause"},{"location":"workshop/examples/01_filter/#having-clause","text":"Events are selected after processing them if the selected field on the outbound event is true. select event from in into out having event . selected","title":"Having clause"},{"location":"workshop/examples/01_filter/#where-and-having-clauses","text":"Events are selected on the inbound event if the numeric field on the inbound event is less than or equal to 10 or greater than or equal to 100 and after processing them, the selected field on the outbound event is true. select event from in where event . numeric <= 10 or event . numeric >= 100 into out having event . selected Tip The where clause has to be located after the from section in a trickle select expression! The where clause is evaluated on the incoming event. The having clause can be used to filter events, but it has to appear after the into expression and will be evaluated on the resulting produced event prior to passing it on. The where and having clauses are optional in trickle select query statements.","title":"Where and Having clauses"},{"location":"workshop/examples/02_transform/","text":"Transform \u00b6 The transform example builds on the filter example and extends the example.trickle by adding a transformation that modifies the incoming event. The produced event from this query statement has a different structure than the incoming event. Environment \u00b6 It connects to the pipeline example in the example.trickle file using the tremor script language to change the json for the log. All other configuration is the same as per the passthrough example, and is elided here for brevity. Business Logic \u00b6 select { # 1. We can inline new json-like document structures \"hello\" : \"hi there #{ event . hello} \" , # 2. Tremor supports flexible string interpolation useful for templating \"world\" : event . hello } from in where event . selected into out Command line testing during logic development \u00b6 Run a the passthrough query against a sample input.json $ tremor run -i input.json ./etc/tremor/config/example.trickle { \"hello\" : \"hi there world\" , \"world\" : \"world\" } Change the input.json and toggle the selected filed to true and run again. Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"hi there again\" , \"world\" : \"again\" } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ... Discussion \u00b6 Transformations in tremor query ( trickle ) can be any legal type / value supported by the tremor family of languages: A boolean value An integer A floating point value A UTF-8 encoded string An array of any legal value A record of field / value pairs where the field name is a string, and the value is any legal value Examples \u00b6 Templating percentile estimates from HDR Histogram \u00b6 In this example, we restructure output from the tremor aggr::stats::hdr aggregate function and use string interpolation to generate record templates with a field naming scheme and structure this is compatible with tremor's influx data offramp. A nice advantage of tremor, is that the business logic is separate from any externalising factors. However, one drawback with unstructured transformations is there is no explicit validation by schema supported by tremor out of the box - although, there are patterns in use to validate against external schema formats in use in production. select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"timestamp\" : event . timestamp , \"fields\" : { # the following fields are generated by templating / string interpolation \"count_ #{ event . class} \" : event . stats . count , \"min_ #{ event . class} \" : event . stats . min , \"max_ #{ event . class} \" : event . stats . max , \"mean_ #{ event . class} \" : event . stats . mean , \"stdev_ #{ event . class} \" : event . stats . stdev , \"var_ #{ event . class} \" : event . stats . var , \"p50_ #{ event . class} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . class} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . class} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . class} \" : event . stats . percentiles [ \"0.999\" ] } } from normalize into batch ; Tip Not all tremor script ideoms are allowed in the select statement. Most notably we do not allow any mutating operations such as let or control flow such as emit or drop . Those constructs can however still be used inside a script block on their own.","title":"Transform"},{"location":"workshop/examples/02_transform/#transform","text":"The transform example builds on the filter example and extends the example.trickle by adding a transformation that modifies the incoming event. The produced event from this query statement has a different structure than the incoming event.","title":"Transform"},{"location":"workshop/examples/02_transform/#environment","text":"It connects to the pipeline example in the example.trickle file using the tremor script language to change the json for the log. All other configuration is the same as per the passthrough example, and is elided here for brevity.","title":"Environment"},{"location":"workshop/examples/02_transform/#business-logic","text":"select { # 1. We can inline new json-like document structures \"hello\" : \"hi there #{ event . hello} \" , # 2. Tremor supports flexible string interpolation useful for templating \"world\" : event . hello } from in where event . selected into out","title":"Business Logic"},{"location":"workshop/examples/02_transform/#command-line-testing-during-logic-development","text":"Run a the passthrough query against a sample input.json $ tremor run -i input.json ./etc/tremor/config/example.trickle { \"hello\" : \"hi there world\" , \"world\" : \"world\" } Change the input.json and toggle the selected filed to true and run again. Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"hi there again\" , \"world\" : \"again\" } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ...","title":"Command line testing during logic development"},{"location":"workshop/examples/02_transform/#discussion","text":"Transformations in tremor query ( trickle ) can be any legal type / value supported by the tremor family of languages: A boolean value An integer A floating point value A UTF-8 encoded string An array of any legal value A record of field / value pairs where the field name is a string, and the value is any legal value","title":"Discussion"},{"location":"workshop/examples/02_transform/#examples","text":"","title":"Examples"},{"location":"workshop/examples/02_transform/#templating-percentile-estimates-from-hdr-histogram","text":"In this example, we restructure output from the tremor aggr::stats::hdr aggregate function and use string interpolation to generate record templates with a field naming scheme and structure this is compatible with tremor's influx data offramp. A nice advantage of tremor, is that the business logic is separate from any externalising factors. However, one drawback with unstructured transformations is there is no explicit validation by schema supported by tremor out of the box - although, there are patterns in use to validate against external schema formats in use in production. select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"timestamp\" : event . timestamp , \"fields\" : { # the following fields are generated by templating / string interpolation \"count_ #{ event . class} \" : event . stats . count , \"min_ #{ event . class} \" : event . stats . min , \"max_ #{ event . class} \" : event . stats . max , \"mean_ #{ event . class} \" : event . stats . mean , \"stdev_ #{ event . class} \" : event . stats . stdev , \"var_ #{ event . class} \" : event . stats . var , \"p50_ #{ event . class} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . class} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . class} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . class} \" : event . stats . percentiles [ \"0.999\" ] } } from normalize into batch ; Tip Not all tremor script ideoms are allowed in the select statement. Most notably we do not allow any mutating operations such as let or control flow such as emit or drop . Those constructs can however still be used inside a script block on their own.","title":"Templating percentile estimates from HDR Histogram"},{"location":"workshop/examples/03_validate/","text":"Transform \u00b6 The validate example adds the concept of scripts to the trickle file. In this script we validate the schema of the input json against some requirements and only let events through that do satisfy them. Other events are dropped. Those changes are entirely inside the example.trickle . Environment \u00b6 In the example.trickle we define a script that validates the schema that the field hello is a string and the field selected is a boolean. If both conditions are true we pass it on, otherwise it'll drop. All other configuration is the same as per the previous example, and is elided here for brevity. Business Logic \u00b6 define script validate # define the script script match event of case % { present hello , present selected } # Record pattern, validating field presence when type :: is_string ( event . hello ) and type :: is_bool ( event . selected ) # guards => emit event # propagate if valid default => drop # discard or reroute end end ; Command line testing during logic development \u00b6 Run a the query against a sample input.json $ tremor run -i input.json example.trickle >> { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ... Injecting bad messages to verify validation. $ cat invalids.txt | websocat ws://localhost:4242 ... Discussion \u00b6 We introduce the declare script and create script query language features. delcare script lets declare a template for a script to be run while create script instanciates it as a part of the graph. create script takes an additional as <name> argument if it is omitted the operator will have the same name as the declaration. Tip Scripts themselves can not connect to elements inside the graph, a select statement is needed to glue scripts and other logic together.","title":"Validate"},{"location":"workshop/examples/03_validate/#transform","text":"The validate example adds the concept of scripts to the trickle file. In this script we validate the schema of the input json against some requirements and only let events through that do satisfy them. Other events are dropped. Those changes are entirely inside the example.trickle .","title":"Transform"},{"location":"workshop/examples/03_validate/#environment","text":"In the example.trickle we define a script that validates the schema that the field hello is a string and the field selected is a boolean. If both conditions are true we pass it on, otherwise it'll drop. All other configuration is the same as per the previous example, and is elided here for brevity.","title":"Environment"},{"location":"workshop/examples/03_validate/#business-logic","text":"define script validate # define the script script match event of case % { present hello , present selected } # Record pattern, validating field presence when type :: is_string ( event . hello ) and type :: is_bool ( event . selected ) # guards => emit event # propagate if valid default => drop # discard or reroute end end ;","title":"Business Logic"},{"location":"workshop/examples/03_validate/#command-line-testing-during-logic-development","text":"Run a the query against a sample input.json $ tremor run -i input.json example.trickle >> { \"hello\" : \"world\" } Deploy the solution into a docker environment $ docker-compose up >> { \"hello\" : \"again\" , \"selected\" : true } Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ cat inputs.txt | websocat ws://localhost:4242 ... Injecting bad messages to verify validation. $ cat invalids.txt | websocat ws://localhost:4242 ...","title":"Command line testing during logic development"},{"location":"workshop/examples/03_validate/#discussion","text":"We introduce the declare script and create script query language features. delcare script lets declare a template for a script to be run while create script instanciates it as a part of the graph. create script takes an additional as <name> argument if it is omitted the operator will have the same name as the declaration. Tip Scripts themselves can not connect to elements inside the graph, a select statement is needed to glue scripts and other logic together.","title":"Discussion"},{"location":"workshop/examples/10_logstash/","text":"Transform \u00b6 This example shows how handling apache logs with a tremor and elastic search could work. The example is a lot more complex than the initial showcases and combines three components. Kibana, which once started with docker-compose can be reached locally . It allows browsing through the logs. If you have never used Kibana before you can get started by clicking on Management then in the Elasticsearch section on Index Management . Elastic Search, which stores the logs submitted. Tremor, which takes the apache logs, parses and classifies them then submits them to indexes in elastic search. In addition the file demo/data/apache_access_logs.xz Link is used as example payload. Environment \u00b6 In the example.trickle we define scripts that extract and categorize apache logs. Any log that is not conforming ther predefined format will be dropped. All other configuration is the same as per the previous example and is elided here for brevity. Business Logic \u00b6 define script extract # define the script that parses our apache logs script match { \"raw\" : event } of # we use the dissect extractor to parse the apache log case r = % { raw ~= dissect |% {ip} % {} % {} [% {timestamp} ] \"%{method} %{path} %{proto}\" % {code : int} % {cost : int} \\\\ n | } => r . raw # this first case is hit if the log includes an execution time (cost) for the request case r = % { raw ~= dissect |% {ip} % {} % {} [% {timestamp} ] \"%{method} %{path} %{proto}\" % {code : int} % {} \\\\ n | } => r . raw default => emit => \"bad\" end end ; define script categorize # define the script that classifies the logs with user_error_index = \"errors\" , # we use \"with\" here to default some configuration for server_error_index = \"errors\" , # the script, we could then re-use this script in multiple ok_index = \"requests\" , # places with different indexes other_index = \"requests\" script let $ doc_type = \"log\" ; # doc_type is used by the offramp, the $ denotes this is stored in event metadata let $ index = match event of case e = % { present code} when e . code >= 200 and e . code < 400 # for http codes between 200 and 400 (exclusive) - those are success codes => args . ok_index case e = % { present code} when e . code >= 400 and e . code < 500 # 400 to 500 (exclusive) are client side errors => args . user_error_index case e = % { present code} when e . code >= 500 and e . code < 600 => args . server_error_index # 500 to 500 (exclusive) are server side errors default => args . other_index # if we get any other code we just use a default index end ; event # emit the event with it's new metadata end ; Command line testing during logic development \u00b6 $ docker-compose up ... lots of logs ... Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ xzcat demo/data/apache_access_logs.xz | websocat ws://localhost:4242 ... Open the Kibana index management and create indexes to view the data. Discussion \u00b6 This is a fairly complex example that combines everything we've seen in the prior examples and a bit more. It should serve as a starting point of how to use tremor to ingest, process, filter and classify data with tremor into an upstream system. Tip When using this as a baseline be aware that around things like batching tuning will be involved to make the numbers fit with the infrastructure it is pointed at. Also since it is not an ongoing data stream we omitted backpressure or classification based rate limiting from the example.","title":"Logstash"},{"location":"workshop/examples/10_logstash/#transform","text":"This example shows how handling apache logs with a tremor and elastic search could work. The example is a lot more complex than the initial showcases and combines three components. Kibana, which once started with docker-compose can be reached locally . It allows browsing through the logs. If you have never used Kibana before you can get started by clicking on Management then in the Elasticsearch section on Index Management . Elastic Search, which stores the logs submitted. Tremor, which takes the apache logs, parses and classifies them then submits them to indexes in elastic search. In addition the file demo/data/apache_access_logs.xz Link is used as example payload.","title":"Transform"},{"location":"workshop/examples/10_logstash/#environment","text":"In the example.trickle we define scripts that extract and categorize apache logs. Any log that is not conforming ther predefined format will be dropped. All other configuration is the same as per the previous example and is elided here for brevity.","title":"Environment"},{"location":"workshop/examples/10_logstash/#business-logic","text":"define script extract # define the script that parses our apache logs script match { \"raw\" : event } of # we use the dissect extractor to parse the apache log case r = % { raw ~= dissect |% {ip} % {} % {} [% {timestamp} ] \"%{method} %{path} %{proto}\" % {code : int} % {cost : int} \\\\ n | } => r . raw # this first case is hit if the log includes an execution time (cost) for the request case r = % { raw ~= dissect |% {ip} % {} % {} [% {timestamp} ] \"%{method} %{path} %{proto}\" % {code : int} % {} \\\\ n | } => r . raw default => emit => \"bad\" end end ; define script categorize # define the script that classifies the logs with user_error_index = \"errors\" , # we use \"with\" here to default some configuration for server_error_index = \"errors\" , # the script, we could then re-use this script in multiple ok_index = \"requests\" , # places with different indexes other_index = \"requests\" script let $ doc_type = \"log\" ; # doc_type is used by the offramp, the $ denotes this is stored in event metadata let $ index = match event of case e = % { present code} when e . code >= 200 and e . code < 400 # for http codes between 200 and 400 (exclusive) - those are success codes => args . ok_index case e = % { present code} when e . code >= 400 and e . code < 500 # 400 to 500 (exclusive) are client side errors => args . user_error_index case e = % { present code} when e . code >= 500 and e . code < 600 => args . server_error_index # 500 to 500 (exclusive) are server side errors default => args . other_index # if we get any other code we just use a default index end ; event # emit the event with it's new metadata end ;","title":"Business Logic"},{"location":"workshop/examples/10_logstash/#command-line-testing-during-logic-development","text":"$ docker-compose up ... lots of logs ... Inject test messages via websocat Note Can be installed via cargo install websocat for the lazy/impatient amongst us $ xzcat demo/data/apache_access_logs.xz | websocat ws://localhost:4242 ... Open the Kibana index management and create indexes to view the data.","title":"Command line testing during logic development"},{"location":"workshop/examples/10_logstash/#discussion","text":"This is a fairly complex example that combines everything we've seen in the prior examples and a bit more. It should serve as a starting point of how to use tremor to ingest, process, filter and classify data with tremor into an upstream system. Tip When using this as a baseline be aware that around things like batching tuning will be involved to make the numbers fit with the infrastructure it is pointed at. Also since it is not an ongoing data stream we omitted backpressure or classification based rate limiting from the example.","title":"Discussion"},{"location":"workshop/examples/11_influx/","text":"Transform \u00b6 This example demonstrates using Tremor as a proxy and aggregator for InfluxDB data. As such it coveres three topics. Ingesting and decoding influx data is the first part. Then grouping this data and aggregating over it. The demo starts up a local Chronograf . This allows browsing the data stored in influxdb. When first connecting you'll be asked to specify the database to use, please change the **Connection URL** to http://influxdb:8086 . For all other questions select Skip as we do not need to configure those. Once in Chronograf, look at the tremor database to see the metrics and rollups. Since rollups do roll up over time you might have to wait a few minutes untill aggregated data propagates. Depending on the performance of the system the demo is run on metrics may be shed due to tremors over load protection. Environment \u00b6 In the example.trickle we process the data in multiple steps, since this is somewhat more complex then the prior examples we'll discuss each step in the Business Logic section. Business Logic \u00b6 Grouping \u00b6 select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); This step groups the data for aggregation. This is required since the Influx Line protocol allows for multiple values within one message. The grouping step ensures that we do not aggregate cpu_idle and cpu_user into the same value despite them being in the same result. In other words we normalise an event like this measurement tag1=value1,tag2=value2 field1=42,field2=\"snot\",field3=0.2 123587512345513 into the three distinct series it represents, namely: measurement tag1=value1,tag2=value2 field1=42 123587512345513 measurement tag1=value1,tag2=value2 field2=\"snot\" 123587512345513 measurement tag1=value1,tag2=value2 field3=0.2 123587512345513 The second part that happens in this query is removing non numeric values from our aggregated series since they are not able to be aggregated. Aggregation \u00b6 select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : aggr :: win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; In this section we aggregate the different serieses we created in the previous section. Most notably are the aggr::stats::hdr and aggr::win::first functions which do the aggregation. aggr::stats::hdr uses a optimized HDR Histogram algorithm to generate the values requested of it. aggr::win::first gives the timestamp of the first event in the window. Normalisation to Influx Line Protocol \u00b6 select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . field} \" : event . stats . count , \"min_ #{ event . field} \" : event . stats . min , \"max_ #{ event . field} \" : event . stats . max , \"mean_ #{ event . field} \" : event . stats . mean , \"stdev_ #{ event . field} \" : event . stats . stdev , \"var_ #{ event . field} \" : event . stats . var , \"p50_ #{ event . field} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . field} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . field} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . field} \" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; The last part normalises the data to a format that can be encoded into influx line protocol. And name the fields accordingly. This uses string interpolation for the recortd fields and simle value access for their values. Command line testing during logic development \u00b6 $ docker-compose up ... lots of logs ... Open the Chronograf and connect the database. Discussion \u00b6 It is noteworthy that in the aggregation context only aggr::stats::hdr and aggr::win::first are being evaluated for events, resulting record and the associated logic is only ever evaluated on emit. We are using having in the goruping step, however this could also be done with a where clause on the aggregation step. In this example we choose having over were as it is worth discarding events as early as possible. If the requirement were to handle non numeric fields in a different manner routing the output of the grouping step to two different select statements we would have used where instead. Tip Using aggr::win::first over aggr::stats::min is a debatable choice as we use the timestamp of the first event not the minimal timestamp. Inside of tremor we do not re-order events so those two would result in the same result with aggr::win::first being cheaper to run. In addition stats functions are currently implemented to return floating point numbers so aggr::stats::min could lead incorrect timestamps we'd rather avoid.","title":"Influx"},{"location":"workshop/examples/11_influx/#transform","text":"This example demonstrates using Tremor as a proxy and aggregator for InfluxDB data. As such it coveres three topics. Ingesting and decoding influx data is the first part. Then grouping this data and aggregating over it. The demo starts up a local Chronograf . This allows browsing the data stored in influxdb. When first connecting you'll be asked to specify the database to use, please change the **Connection URL** to http://influxdb:8086 . For all other questions select Skip as we do not need to configure those. Once in Chronograf, look at the tremor database to see the metrics and rollups. Since rollups do roll up over time you might have to wait a few minutes untill aggregated data propagates. Depending on the performance of the system the demo is run on metrics may be shed due to tremors over load protection.","title":"Transform"},{"location":"workshop/examples/11_influx/#environment","text":"In the example.trickle we process the data in multiple steps, since this is somewhat more complex then the prior examples we'll discuss each step in the Business Logic section.","title":"Environment"},{"location":"workshop/examples/11_influx/#business-logic","text":"","title":"Business Logic"},{"location":"workshop/examples/11_influx/#grouping","text":"select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); This step groups the data for aggregation. This is required since the Influx Line protocol allows for multiple values within one message. The grouping step ensures that we do not aggregate cpu_idle and cpu_user into the same value despite them being in the same result. In other words we normalise an event like this measurement tag1=value1,tag2=value2 field1=42,field2=\"snot\",field3=0.2 123587512345513 into the three distinct series it represents, namely: measurement tag1=value1,tag2=value2 field1=42 123587512345513 measurement tag1=value1,tag2=value2 field2=\"snot\" 123587512345513 measurement tag1=value1,tag2=value2 field3=0.2 123587512345513 The second part that happens in this query is removing non numeric values from our aggregated series since they are not able to be aggregated.","title":"Grouping"},{"location":"workshop/examples/11_influx/#aggregation","text":"select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : aggr :: win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; In this section we aggregate the different serieses we created in the previous section. Most notably are the aggr::stats::hdr and aggr::win::first functions which do the aggregation. aggr::stats::hdr uses a optimized HDR Histogram algorithm to generate the values requested of it. aggr::win::first gives the timestamp of the first event in the window.","title":"Aggregation"},{"location":"workshop/examples/11_influx/#normalisation-to-influx-line-protocol","text":"select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . field} \" : event . stats . count , \"min_ #{ event . field} \" : event . stats . min , \"max_ #{ event . field} \" : event . stats . max , \"mean_ #{ event . field} \" : event . stats . mean , \"stdev_ #{ event . field} \" : event . stats . stdev , \"var_ #{ event . field} \" : event . stats . var , \"p50_ #{ event . field} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . field} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . field} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . field} \" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; The last part normalises the data to a format that can be encoded into influx line protocol. And name the fields accordingly. This uses string interpolation for the recortd fields and simle value access for their values.","title":"Normalisation to Influx Line Protocol"},{"location":"workshop/examples/11_influx/#command-line-testing-during-logic-development","text":"$ docker-compose up ... lots of logs ... Open the Chronograf and connect the database.","title":"Command line testing during logic development"},{"location":"workshop/examples/11_influx/#discussion","text":"It is noteworthy that in the aggregation context only aggr::stats::hdr and aggr::win::first are being evaluated for events, resulting record and the associated logic is only ever evaluated on emit. We are using having in the goruping step, however this could also be done with a where clause on the aggregation step. In this example we choose having over were as it is worth discarding events as early as possible. If the requirement were to handle non numeric fields in a different manner routing the output of the grouping step to two different select statements we would have used where instead. Tip Using aggr::win::first over aggr::stats::min is a debatable choice as we use the timestamp of the first event not the minimal timestamp. Inside of tremor we do not re-order events so those two would result in the same result with aggr::win::first being cheaper to run. In addition stats functions are currently implemented to return floating point numbers so aggr::stats::min could lead incorrect timestamps we'd rather avoid.","title":"Discussion"},{"location":"workshop/examples/12_postgres_timescaledb/","text":"Transform \u00b6 This example demonstrates extracting data from a Postgres database and inserting data to TimescaleDB. The demo starts up said PostgreSQL, TimescaleDB, Tremor and pgweb . Environment \u00b6 In 00_ramps.yaml we pass in a configuration for an onramp of type postgres along with typical connection string requirements. Additionally, we are required to specify interval_ms which stands for frequency of polling that Tremor is performing on Postgres database with the given query . Query will be passed two parameters: * $1 is the TIMESTAMPTZ that indicates the start time and date for the range * $2 is the TIMESTAMPTZ that indicates the ending time and date for the range The initial range is formed by taking consume_from configuration setting and the current time and date. This will effectivelly backfill data. From then on, Tremor will poll in regular interval_ms . In addition to a postgres onramp, we also utilize a crononome onramp. The intention is to demonstrate intermediate record format which is accepted by postgres offramp. Business Logic \u00b6 In 01_pipeline.yaml we register two pipelines: postgres-things for data coming from a PostgreSQL database and crononome-things coming from crononome at regular interval of 5s . Command line testing during logic development \u00b6 $ docker-compose up ... lots of logs ... Open the pgweb to browse through received rows in TimescaleDB. Discussion \u00b6","title":"Timescale"},{"location":"workshop/examples/12_postgres_timescaledb/#transform","text":"This example demonstrates extracting data from a Postgres database and inserting data to TimescaleDB. The demo starts up said PostgreSQL, TimescaleDB, Tremor and pgweb .","title":"Transform"},{"location":"workshop/examples/12_postgres_timescaledb/#environment","text":"In 00_ramps.yaml we pass in a configuration for an onramp of type postgres along with typical connection string requirements. Additionally, we are required to specify interval_ms which stands for frequency of polling that Tremor is performing on Postgres database with the given query . Query will be passed two parameters: * $1 is the TIMESTAMPTZ that indicates the start time and date for the range * $2 is the TIMESTAMPTZ that indicates the ending time and date for the range The initial range is formed by taking consume_from configuration setting and the current time and date. This will effectivelly backfill data. From then on, Tremor will poll in regular interval_ms . In addition to a postgres onramp, we also utilize a crononome onramp. The intention is to demonstrate intermediate record format which is accepted by postgres offramp.","title":"Environment"},{"location":"workshop/examples/12_postgres_timescaledb/#business-logic","text":"In 01_pipeline.yaml we register two pipelines: postgres-things for data coming from a PostgreSQL database and crononome-things coming from crononome at regular interval of 5s .","title":"Business Logic"},{"location":"workshop/examples/12_postgres_timescaledb/#command-line-testing-during-logic-development","text":"$ docker-compose up ... lots of logs ... Open the pgweb to browse through received rows in TimescaleDB.","title":"Command line testing during logic development"},{"location":"workshop/examples/12_postgres_timescaledb/#discussion","text":"","title":"Discussion"},{"location":"workshop/examples/13_grafana/","text":"Transform \u00b6 This example demonstrates using Tremor as a proxy and aggregator for InfluxDB data. As such it coveres three topics. Ingesting and decoding influx data is the first part. Then grouping this data and aggregating over it. The demo starts up a local Chronograf . This allows browsing the data stored in influxdb. When first connecting you'll be asked to specify the database to use, please change the **Connection URL** to http://influxdb:8086 . For all other questions select Skip as we do not need to configure those. Once in Chronograf, look at the tremor database to see the metrics and rollups. Since rollups do roll up over time you might have to wait a few minutes untill aggregated data propagates. Depending on the performance of the system the demo is run on metrics may be shed due to tremors over load protection. Environment \u00b6 In the example.trickle we process the data in multiple steps, since this is somewhat more complex then the prior examples we'll discuss each step in the Business Logic section. Business Logic \u00b6 Grouping \u00b6 select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); This step groups the data for aggregation. This is required since the Influx Line protocol allows for multiple values within one message. The grouping step ensures that we do not aggregate cpu_idle and cpu_user into the same value despite them being in the same result. In other words we normalise an event like this measurement tag1=value1,tag2=value2 field1=42,field2=\"snot\",field3=0.2 123587512345513 into the three distinct series it represents, namely: measurement tag1=value1,tag2=value2 field1=42 123587512345513 measurement tag1=value1,tag2=value2 field2=\"snot\" 123587512345513 measurement tag1=value1,tag2=value2 field3=0.2 123587512345513 The second part that happens in this query is removing non numeric values from our aggregated series since they are not able to be aggregated. Aggregation \u00b6 select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : aggr :: win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; In this section we aggregate the different serieses we created in the previous section. Most notably are the aggr::stats::hdr and aggr::win::first functions which do the aggregation. aggr::stats::hdr uses a optimized HDR Histogram algorithm to generate the values requested of it. aggr::win::first gives the timestamp of the first event in the window. Normalisation to Influx Line Protocol \u00b6 select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . field} \" : event . stats . count , \"min_ #{ event . field} \" : event . stats . min , \"max_ #{ event . field} \" : event . stats . max , \"mean_ #{ event . field} \" : event . stats . mean , \"stdev_ #{ event . field} \" : event . stats . stdev , \"var_ #{ event . field} \" : event . stats . var , \"p50_ #{ event . field} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . field} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . field} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . field} \" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; The last part normalises the data to a format that can be encoded into influx line protocol. And name the fields accordingly. This uses string interpolation for the recortd fields and simle value access for their values. Command line testing during logic development \u00b6 $ docker-compose up ... lots of logs ... Open the Chronograf and connect the database. Visualization with Grafana \u00b6 The docker compose file deploys a grafana image that can be used with the influx data source configured to make a server connection to http://influxdb:8086 and explored via the logs or metrics explorer modes. Discussion \u00b6 It is noteworthy that in the aggregation context only aggr::stats::hdr and aggr::win::first are being evaluated for events, resulting record and the associated logic is only ever evaluated on emit. We are using having in the goruping step, however this could also be done with a where clause on the aggregation step. In this example we choose having over were as it is worth discarding events as early as possible. If the requirement were to handle non numeric fields in a different manner routing the output of the grouping step to two different select statements we would have used where instead. Tip Using aggr::win::first over aggr::stats::min is a debatable choice as we use the timestamp of the first event not the minimal timestamp. Inside of tremor we do not re-order events so those two would result in the same result with aggr::win::first being cheaper to run. In addition stats functions are currently implemented to return floating point numbers so aggr::stats::min could lead incorrect timestamps we'd rather avoid.","title":"Grafana"},{"location":"workshop/examples/13_grafana/#transform","text":"This example demonstrates using Tremor as a proxy and aggregator for InfluxDB data. As such it coveres three topics. Ingesting and decoding influx data is the first part. Then grouping this data and aggregating over it. The demo starts up a local Chronograf . This allows browsing the data stored in influxdb. When first connecting you'll be asked to specify the database to use, please change the **Connection URL** to http://influxdb:8086 . For all other questions select Skip as we do not need to configure those. Once in Chronograf, look at the tremor database to see the metrics and rollups. Since rollups do roll up over time you might have to wait a few minutes untill aggregated data propagates. Depending on the performance of the system the demo is run on metrics may be shed due to tremors over load protection.","title":"Transform"},{"location":"workshop/examples/13_grafana/#environment","text":"In the example.trickle we process the data in multiple steps, since this is somewhat more complex then the prior examples we'll discuss each step in the Business Logic section.","title":"Environment"},{"location":"workshop/examples/13_grafana/#business-logic","text":"","title":"Business Logic"},{"location":"workshop/examples/13_grafana/#grouping","text":"select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"field\" : group [ 2 ], \"value\" : event . fields [ group [ 2 ]], \"timestamp\" : event . timestamp , } from in group by set ( event . measurement , event . tags , each ( record :: keys ( event . fields ))) into aggregate having type :: is_number ( event . value ); This step groups the data for aggregation. This is required since the Influx Line protocol allows for multiple values within one message. The grouping step ensures that we do not aggregate cpu_idle and cpu_user into the same value despite them being in the same result. In other words we normalise an event like this measurement tag1=value1,tag2=value2 field1=42,field2=\"snot\",field3=0.2 123587512345513 into the three distinct series it represents, namely: measurement tag1=value1,tag2=value2 field1=42 123587512345513 measurement tag1=value1,tag2=value2 field2=\"snot\" 123587512345513 measurement tag1=value1,tag2=value2 field3=0.2 123587512345513 The second part that happens in this query is removing non numeric values from our aggregated series since they are not able to be aggregated.","title":"Grouping"},{"location":"workshop/examples/13_grafana/#aggregation","text":"select { \"measurement\" : event . measurement , \"tags\" : patch event . tags of insert \"window\" => window end , \"stats\" : aggr :: stats :: hdr ( event . value , [ \"0.5\" , \"0.9\" , \"0.99\" , \"0.999\" ]), \"field\" : event . field , \"timestamp\" : aggr :: win :: first ( event . timestamp ), # we can't use min since it's a float } from aggregate [` 10 secs `, ` 1 min `, ] group by set ( event . measurement , event . tags , event . field ) into normalize ; In this section we aggregate the different serieses we created in the previous section. Most notably are the aggr::stats::hdr and aggr::win::first functions which do the aggregation. aggr::stats::hdr uses a optimized HDR Histogram algorithm to generate the values requested of it. aggr::win::first gives the timestamp of the first event in the window.","title":"Aggregation"},{"location":"workshop/examples/13_grafana/#normalisation-to-influx-line-protocol","text":"select { \"measurement\" : event . measurement , \"tags\" : event . tags , \"fields\" : { \"count_ #{ event . field} \" : event . stats . count , \"min_ #{ event . field} \" : event . stats . min , \"max_ #{ event . field} \" : event . stats . max , \"mean_ #{ event . field} \" : event . stats . mean , \"stdev_ #{ event . field} \" : event . stats . stdev , \"var_ #{ event . field} \" : event . stats . var , \"p50_ #{ event . field} \" : event . stats . percentiles [ \"0.5\" ], \"p90_ #{ event . field} \" : event . stats . percentiles [ \"0.9\" ], \"p99_ #{ event . field} \" : event . stats . percentiles [ \"0.99\" ], \"p99.9_ #{ event . field} \" : event . stats . percentiles [ \"0.999\" ] } , \"timestamp\" : event . timestamp , } from normalize into batch ; The last part normalises the data to a format that can be encoded into influx line protocol. And name the fields accordingly. This uses string interpolation for the recortd fields and simle value access for their values.","title":"Normalisation to Influx Line Protocol"},{"location":"workshop/examples/13_grafana/#command-line-testing-during-logic-development","text":"$ docker-compose up ... lots of logs ... Open the Chronograf and connect the database.","title":"Command line testing during logic development"},{"location":"workshop/examples/13_grafana/#visualization-with-grafana","text":"The docker compose file deploys a grafana image that can be used with the influx data source configured to make a server connection to http://influxdb:8086 and explored via the logs or metrics explorer modes.","title":"Visualization with Grafana"},{"location":"workshop/examples/13_grafana/#discussion","text":"It is noteworthy that in the aggregation context only aggr::stats::hdr and aggr::win::first are being evaluated for events, resulting record and the associated logic is only ever evaluated on emit. We are using having in the goruping step, however this could also be done with a where clause on the aggregation step. In this example we choose having over were as it is worth discarding events as early as possible. If the requirement were to handle non numeric fields in a different manner routing the output of the grouping step to two different select statements we would have used where instead. Tip Using aggr::win::first over aggr::stats::min is a debatable choice as we use the timestamp of the first event not the minimal timestamp. Inside of tremor we do not re-order events so those two would result in the same result with aggr::win::first being cheaper to run. In addition stats functions are currently implemented to return floating point numbers so aggr::stats::min could lead incorrect timestamps we'd rather avoid.","title":"Discussion"},{"location":"workshop/examples/20_transient_gd/","text":"Transient Write-Ahead Log \u00b6 The write-ahead log builds on circuit breaker and acknowledgement mechanisms to provide guaranteed delivery. The write-ahead log is useful in situations where sources/onramps do not offer guaranteed delivery themselves, but the data being distributed downstream can benefit from protection against loss and duplication. In the configuration in this tutorial we configure a transient in-memory WAL. Environment \u00b6 We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. # File: etc/tremor/config/transient_gd.trickle use tremor :: system ; define qos :: wal operator in_memory_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; create operator in_memory_wal ; select patch event of insert hostname = system :: hostname () end from in into in_memory_wal ; select event from in_memory_wal into out ; We then distribute the metronome events downstream to another websocket listener. We use websocat for this purpose in this example. We can invoke the server as follows: $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ We configure the sink/offramp as follows: offramp : - id : ws type : ws config : url : ws://localhost:8080/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/main/{instance}/in\"] \"/pipeline/main/{instance}/out\": [\"/offramp/ws/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor client as follows: $ tremor server run -f etc/tremor/config/* Insights \u00b6 If the tremor process restarts we sequence from the beginning. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689100122526000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689101122912000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689102124688000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689104854927000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689105855314000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689106855645000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689107856271000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689202887145000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689203888395000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689204889220000 } Note Notice that we start from sequence 0 3 times, so we restarted tremor 3 times. If the downstream websocket service restarts we can recover up to 1000 events. We may lose in flight events that were sending at the time the server went down. However, for fast restarts of the downstream service the losses should be minimal. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :17, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689219933167000 } { \"onramp\" : \"metronome\" , \"id\" :18, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689220936343000 } { \"onramp\" : \"metronome\" , \"id\" :19, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689221937353000 } { \"onramp\" : \"metronome\" , \"id\" :20, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689222942518000 } { \"onramp\" : \"metronome\" , \"id\" :21, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689223945736000 } { \"onramp\" : \"metronome\" , \"id\" :22, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689224949145000 } $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :25, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689227960081000 } { \"onramp\" : \"metronome\" , \"id\" :26, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689228960247000 } { \"onramp\" : \"metronome\" , \"id\" :27, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689229960449000 } { \"onramp\" : \"metronome\" , \"id\" :28, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689230962355000 } { \"onramp\" : \"metronome\" , \"id\" :29, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689231962934000 } $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :31, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689233968332000 } { \"onramp\" : \"metronome\" , \"id\" :32, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689234973058000 } { \"onramp\" : \"metronome\" , \"id\" :33, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689235974217000 } { \"onramp\" : \"metronome\" , \"id\" :34, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689236975746000 } { \"onramp\" : \"metronome\" , \"id\" :35, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689237976774000 } { \"onramp\" : \"metronome\" , \"id\" :36, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689238980380000 } { \"onramp\" : \"metronome\" , \"id\" :37, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689239985447000 } Note Note that we recover most but now all of the data. As the downstream websocket connection is not a guaranteed delivery connection the recovery and protection against data loss is best effort in this case In short, the transient in memory wal can assist with partial recovery and will actively reduce data loss within the configured retention but it is not lossless.","title":"Transient GD"},{"location":"workshop/examples/20_transient_gd/#transient-write-ahead-log","text":"The write-ahead log builds on circuit breaker and acknowledgement mechanisms to provide guaranteed delivery. The write-ahead log is useful in situations where sources/onramps do not offer guaranteed delivery themselves, but the data being distributed downstream can benefit from protection against loss and duplication. In the configuration in this tutorial we configure a transient in-memory WAL.","title":"Transient Write-Ahead Log"},{"location":"workshop/examples/20_transient_gd/#environment","text":"We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. # File: etc/tremor/config/transient_gd.trickle use tremor :: system ; define qos :: wal operator in_memory_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; create operator in_memory_wal ; select patch event of insert hostname = system :: hostname () end from in into in_memory_wal ; select event from in_memory_wal into out ; We then distribute the metronome events downstream to another websocket listener. We use websocat for this purpose in this example. We can invoke the server as follows: $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ We configure the sink/offramp as follows: offramp : - id : ws type : ws config : url : ws://localhost:8080/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/main/{instance}/in\"] \"/pipeline/main/{instance}/out\": [\"/offramp/ws/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor client as follows: $ tremor server run -f etc/tremor/config/*","title":"Environment"},{"location":"workshop/examples/20_transient_gd/#insights","text":"If the tremor process restarts we sequence from the beginning. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689100122526000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689101122912000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689102124688000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689104854927000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689105855314000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689106855645000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689107856271000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689202887145000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689203888395000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689204889220000 } Note Notice that we start from sequence 0 3 times, so we restarted tremor 3 times. If the downstream websocket service restarts we can recover up to 1000 events. We may lose in flight events that were sending at the time the server went down. However, for fast restarts of the downstream service the losses should be minimal. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :17, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689219933167000 } { \"onramp\" : \"metronome\" , \"id\" :18, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689220936343000 } { \"onramp\" : \"metronome\" , \"id\" :19, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689221937353000 } { \"onramp\" : \"metronome\" , \"id\" :20, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689222942518000 } { \"onramp\" : \"metronome\" , \"id\" :21, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689223945736000 } { \"onramp\" : \"metronome\" , \"id\" :22, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689224949145000 } $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :25, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689227960081000 } { \"onramp\" : \"metronome\" , \"id\" :26, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689228960247000 } { \"onramp\" : \"metronome\" , \"id\" :27, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689229960449000 } { \"onramp\" : \"metronome\" , \"id\" :28, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689230962355000 } { \"onramp\" : \"metronome\" , \"id\" :29, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689231962934000 } $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :31, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689233968332000 } { \"onramp\" : \"metronome\" , \"id\" :32, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689234973058000 } { \"onramp\" : \"metronome\" , \"id\" :33, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689235974217000 } { \"onramp\" : \"metronome\" , \"id\" :34, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689236975746000 } { \"onramp\" : \"metronome\" , \"id\" :35, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689237976774000 } { \"onramp\" : \"metronome\" , \"id\" :36, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689238980380000 } { \"onramp\" : \"metronome\" , \"id\" :37, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689239985447000 } Note Note that we recover most but now all of the data. As the downstream websocket connection is not a guaranteed delivery connection the recovery and protection against data loss is best effort in this case In short, the transient in memory wal can assist with partial recovery and will actively reduce data loss within the configured retention but it is not lossless.","title":"Insights"},{"location":"workshop/examples/21_persistent_gd/","text":"Persistent Write-Ahead Log \u00b6 The write-ahead log ( WAL ) builds on circuit breaker and acknowledgement mechanisms to provide guaranteed delivery. The write-ahead log is useful in situations where sources/onramps do not offer guaranteed delivery themselves, but the data being distributed downstream can benefit from protection against loss and duplication. In the configuration in this tutorial we configure a persistent WAL. Environment \u00b6 We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. # File: etc/tremor/config/persistent.trickle use tremor :: system ; define qos :: wal operator on_disk_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; create operator on_disk_wal ; select patch event of insert hostname = system :: hostname () end from in into on_disk_wal ; select event from on_disk_wal into out ; We then distribute the metronome events downstream to another websocket listener. We use websocat for this purpose in this example. We can invoke the server as follows: $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ We configure the sink/offramp as follows: offramp : - id : ws type : ws config : url : ws://localhost:8080/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/main/{instance}/in\"] \"/pipeline/main/{instance}/out\": [\"/offramp/ws/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor cli as follows: $ tremor server run -f etc/tremor/config/* Insights \u00b6 If the tremor process restarts we pick up and re-send events that have not been acknowledged by the ws offramp and then carry on with new events coming from the metronome: $ websocat -s 8080 ; websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860720749137000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860721751965000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860722756684000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860723761037000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860724764683000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860723761037000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860724764683000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860730353260000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860731355463000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860732357883000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860733362429000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860734364277000 } { \"onramp\" : \"metronome\" , \"id\" :5, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860735367967000 } { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860736373137000 } Note We restarted tremor after sending event with id 4 . It did resend events 3 and 4 as they have not been acked from the perspective of the WAL yet. If the downstream websocket service restarts we can recover up to 1000 events or any number of events worth 1MB. We may lose in flight events that were already acknowledged at the time the server went down and thus not fully delivered by the downstream system. $ websocat -s 8080 ; websocat -s 8080 { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861519788231000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861520790241000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861521792297000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861522797476000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861523802114000 } ^C $ websocat -s 8080 ; websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861525809835000 } { \"onramp\" : \"metronome\" , \"id\" :7, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861526813574000 } { \"onramp\" : \"metronome\" , \"id\" :8, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861527817722000 } { \"onramp\" : \"metronome\" , \"id\" :9, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861528822667000 } { \"onramp\" : \"metronome\" , \"id\" :10, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861529826521000 } { \"onramp\" : \"metronome\" , \"id\" :11, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861530830497000 } Note We killed the websocket server and restarted right afterwards. We in fact lost 1 event (id 5 ) which was acked inside tremor but not yet fully delivered to the console by websocat. Other events that the offramp was unable to send will be resent once the ws offramp can connect again. In short, the persistent in memory wal can assist with partial recovery of downstream system or tremor itself and will actively reduce data loss within the configured retention but it is not guarenteed to be lossless.","title":"Persistent GD"},{"location":"workshop/examples/21_persistent_gd/#persistent-write-ahead-log","text":"The write-ahead log ( WAL ) builds on circuit breaker and acknowledgement mechanisms to provide guaranteed delivery. The write-ahead log is useful in situations where sources/onramps do not offer guaranteed delivery themselves, but the data being distributed downstream can benefit from protection against loss and duplication. In the configuration in this tutorial we configure a persistent WAL.","title":"Persistent Write-Ahead Log"},{"location":"workshop/examples/21_persistent_gd/#environment","text":"We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. # File: etc/tremor/config/persistent.trickle use tremor :: system ; define qos :: wal operator on_disk_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; create operator on_disk_wal ; select patch event of insert hostname = system :: hostname () end from in into on_disk_wal ; select event from on_disk_wal into out ; We then distribute the metronome events downstream to another websocket listener. We use websocat for this purpose in this example. We can invoke the server as follows: $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ We configure the sink/offramp as follows: offramp : - id : ws type : ws config : url : ws://localhost:8080/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/main/{instance}/in\"] \"/pipeline/main/{instance}/out\": [\"/offramp/ws/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor cli as follows: $ tremor server run -f etc/tremor/config/*","title":"Environment"},{"location":"workshop/examples/21_persistent_gd/#insights","text":"If the tremor process restarts we pick up and re-send events that have not been acknowledged by the ws offramp and then carry on with new events coming from the metronome: $ websocat -s 8080 ; websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860720749137000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860721751965000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860722756684000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860723761037000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860724764683000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860723761037000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860724764683000 } { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860730353260000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860731355463000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860732357883000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860733362429000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860734364277000 } { \"onramp\" : \"metronome\" , \"id\" :5, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860735367967000 } { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"localhost\" , \"ingest_ns\" :1600860736373137000 } Note We restarted tremor after sending event with id 4 . It did resend events 3 and 4 as they have not been acked from the perspective of the WAL yet. If the downstream websocket service restarts we can recover up to 1000 events or any number of events worth 1MB. We may lose in flight events that were already acknowledged at the time the server went down and thus not fully delivered by the downstream system. $ websocat -s 8080 ; websocat -s 8080 { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861519788231000 } { \"onramp\" : \"metronome\" , \"id\" :1, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861520790241000 } { \"onramp\" : \"metronome\" , \"id\" :2, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861521792297000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861522797476000 } { \"onramp\" : \"metronome\" , \"id\" :4, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861523802114000 } ^C $ websocat -s 8080 ; websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861525809835000 } { \"onramp\" : \"metronome\" , \"id\" :7, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861526813574000 } { \"onramp\" : \"metronome\" , \"id\" :8, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861527817722000 } { \"onramp\" : \"metronome\" , \"id\" :9, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861528822667000 } { \"onramp\" : \"metronome\" , \"id\" :10, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861529826521000 } { \"onramp\" : \"metronome\" , \"id\" :11, \"hostname\" : \"ALT01828\" , \"ingest_ns\" :1600861530830497000 } Note We killed the websocket server and restarted right afterwards. We in fact lost 1 event (id 5 ) which was acked inside tremor but not yet fully delivered to the console by websocat. Other events that the offramp was unable to send will be resent once the ws offramp can connect again. In short, the persistent in memory wal can assist with partial recovery of downstream system or tremor itself and will actively reduce data loss within the configured retention but it is not guarenteed to be lossless.","title":"Insights"},{"location":"workshop/examples/22_roundrobin/","text":"Round Robin \u00b6 The roundrobin distribution demo builds from the best-effort transient guaranteed delivery demo and adds round-robin load balancing to a fixed number of downstream consumers. In this configuration we build a transient in-memory WAL with round-robin load-balancing dispatch to three downstream distribution endpoints. Environment \u00b6 We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. use tremor :: system ; define qos :: wal operator in_memory_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; define qos :: roundrobin operator roundrobin with outputs = [ \"ws0\" , \"ws1\" , \"ws2\" ] end ; # create operator in_memory_wal; create operator roundrobin ; select merge event of { \"hostname\" : system :: hostname () } end from in into in_memory_wal ; select event from in_memory_wal into roundrobin ; select event from roundrobin / ws0 into out / ws0 ; select event from roundrobin / ws1 into out / ws1 ; select event from roundrobin / ws2 into out / ws2 ; We then distribute the metronome events downstream to three downstream websocket servers and round robin load balance across them Server 1, in first shell $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ Server 2, in second shell $ websocat -s 8081 Listening on ws://127.0.0.1:8081/ Server 3, in third shell $ websocat -s 8082 Listening on ws://127.0.0.1:8082/ We configure the sink/offramp instances as follows: offramp : - id : ws0 type : ws config : url : ws://localhost:8080/ - id : ws1 type : ws config : url : ws://localhost:8081/ - id : ws2 type : ws config : url : ws://localhost:8082/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/roundrobin/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws0\": [ \"/offramp/ws0/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws1\": [ \"/offramp/ws1/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws2\": [ \"/offramp/ws2/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor client as follows: $ tremor server run -f etc/tremor/config/* Insights \u00b6 If the tremor process restarts we sequence from the beginning. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689100122526000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689101122912000 } { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689102124688000 } ... Otherwise, we should see sequences distribute across our downstream round-robin distribution set If we lose a downstream instance we load-balance across the remainder If we lose all downstream instances, we buffer up to our rentention limit of 1000 events or 1MB of event data. Note Notice that we recover most but now all of the data. As the downstream websocket connection is not a guaranteed delivery connection the recovery and protection against data loss is best effort in this case In short, the transient in memory wal can assist with partial recovery and will actively reduce data loss to within the configured retention but it is not lossless. We can also use redundant downstream distribution endpoints to further insulate against catastrophic unrecoverable errors by adding the round robin dispatch strategy and configuring multiple downstream endpoints.","title":"Round Robin"},{"location":"workshop/examples/22_roundrobin/#round-robin","text":"The roundrobin distribution demo builds from the best-effort transient guaranteed delivery demo and adds round-robin load balancing to a fixed number of downstream consumers. In this configuration we build a transient in-memory WAL with round-robin load-balancing dispatch to three downstream distribution endpoints.","title":"Round Robin"},{"location":"workshop/examples/22_roundrobin/#environment","text":"We configure a metronome as a source of data. # File: etc/tremor/config/metronome.yaml onramp : - id : metronome type : metronome config : interval : 1000 # Every second We configure a straight forward passthrough query to distribute the data to connected downstream sinks. use tremor :: system ; define qos :: wal operator in_memory_wal with read_count = 20 , max_elements = 1000 , # Capacity limit of 1000 stored events max_bytes = 10485760 # Capacity limit of 1MB of events end ; define qos :: roundrobin operator roundrobin with outputs = [ \"ws0\" , \"ws1\" , \"ws2\" ] end ; # create operator in_memory_wal; create operator roundrobin ; select merge event of { \"hostname\" : system :: hostname () } end from in into in_memory_wal ; select event from in_memory_wal into roundrobin ; select event from roundrobin / ws0 into out / ws0 ; select event from roundrobin / ws1 into out / ws1 ; select event from roundrobin / ws2 into out / ws2 ; We then distribute the metronome events downstream to three downstream websocket servers and round robin load balance across them Server 1, in first shell $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ Server 2, in second shell $ websocat -s 8081 Listening on ws://127.0.0.1:8081/ Server 3, in third shell $ websocat -s 8082 Listening on ws://127.0.0.1:8082/ We configure the sink/offramp instances as follows: offramp : - id : ws0 type : ws config : url : ws://localhost:8080/ - id : ws1 type : ws config : url : ws://localhost:8081/ - id : ws2 type : ws config : url : ws://localhost:8082/ Finally, we interconnect the source, sink and pipeline logic into an active flow: binding: - id: default links: \"/onramp/metronome/{instance}/out\": [\"/pipeline/roundrobin/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws0\": [ \"/offramp/ws0/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws1\": [ \"/offramp/ws1/{instance}/in\"] \"/pipeline/roundrobin/{instance}/ws2\": [ \"/offramp/ws2/{instance}/in\"] mapping: /binding/default/01: instance: \"01\" Running the example via the tremor client as follows: $ tremor server run -f etc/tremor/config/*","title":"Environment"},{"location":"workshop/examples/22_roundrobin/#insights","text":"If the tremor process restarts we sequence from the beginning. $ websocat -s 8080 Listening on ws://127.0.0.1:8080/ { \"onramp\" : \"metronome\" , \"id\" :0, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689100122526000 } { \"onramp\" : \"metronome\" , \"id\" :3, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689101122912000 } { \"onramp\" : \"metronome\" , \"id\" :6, \"hostname\" : \"ALT01827\" , \"ingest_ns\" :1600689102124688000 } ... Otherwise, we should see sequences distribute across our downstream round-robin distribution set If we lose a downstream instance we load-balance across the remainder If we lose all downstream instances, we buffer up to our rentention limit of 1000 events or 1MB of event data. Note Notice that we recover most but now all of the data. As the downstream websocket connection is not a guaranteed delivery connection the recovery and protection against data loss is best effort in this case In short, the transient in memory wal can assist with partial recovery and will actively reduce data loss to within the configured retention but it is not lossless. We can also use redundant downstream distribution endpoints to further insulate against catastrophic unrecoverable errors by adding the round robin dispatch strategy and configuring multiple downstream endpoints.","title":"Insights"},{"location":"workshop/examples/23_kafka_gd/","text":"Kafka delivery guarantees \u00b6 This example goes into how tremors delivery guarantees work in combination with a sink and source that also support delivery guarantees. The scenario isn't all-encompassing but looks at the following specific use-case: A tremor source that does not hold guarantees. A WAL to persist data from the source A Kafka instance were data is send to A secondary tremor instance with a Kafka source that reads the data and prints it to stdout With this we can demonstrate the recovery and delivery guarantees using a WAL Kafka. We can start the example using docker-compose up . Then with docker ps we find the Kafka instance and can introduce an artificial error using docker pause <container> . We will see the messages stopping if we wait for a while we can reenable kafka with docker unpause <container> and will see the message flow resuming with a number of duplicated but no lost messages.","title":"Kafka"},{"location":"workshop/examples/23_kafka_gd/#kafka-delivery-guarantees","text":"This example goes into how tremors delivery guarantees work in combination with a sink and source that also support delivery guarantees. The scenario isn't all-encompassing but looks at the following specific use-case: A tremor source that does not hold guarantees. A WAL to persist data from the source A Kafka instance were data is send to A secondary tremor instance with a Kafka source that reads the data and prints it to stdout With this we can demonstrate the recovery and delivery guarantees using a WAL Kafka. We can start the example using docker-compose up . Then with docker ps we find the Kafka instance and can introduce an artificial error using docker pause <container> . We will see the messages stopping if we wait for a while we can reenable kafka with docker unpause <container> and will see the message flow resuming with a number of duplicated but no lost messages.","title":"Kafka delivery guarantees"},{"location":"workshop/examples/30_servers_lt_http/","text":"HTTP Server \u00b6 Example HTTP server application built on top of Tremor and meant to be a demonstration of linked transports . Setup \u00b6 Tip All the code here is available in the git repository as well. Sources \u00b6 We configure a rest onramp listening on port 8139: onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 8139 Request flow \u00b6 Incoming requests from clients are directed to the pipeline named request_processing pipeline, and the output of the pipeline is fed back again to the onramp -- this now becomes the server response to the incoming request. binding : - id : main links : \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and send back the response \"/pipeline/request_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] Processing logic \u00b6 In the request_processing pipeline, we are free to process the incoming request using tremor-script/tremor-query, and leveraging the various request and response metadata variables for the rest onramp . The event flow within the pipeline is captured below: create script process ; # main request processing select event from in into process ; select event from process into out ; # our defined app errors (still succesful processing from tremor's perspective) # useful to track these from different port (app_error) for metrics select event from process / app_error into out ; # tremor runtime errors from the processing script select event from process / err into err ; Example section of the process script here, demonstrating how the index page for the HTTP server is implemented (also parses the url query params to demonstrate dynamic responses based on provided user input): case \"/index\" => let request_data = { \"body\": event, \"meta\": $request, }, # determine the name to greet let name = match $request.url of case %{present query} => let query_parsed = utils::parse_query($request.url.query), let request_data.url_query_parsed = query_parsed, match query_parsed of case %{present name} => query_parsed.name default => \"world\" end default => \"world\" end, # serve html! let $response.headers[\"content-type\"] = \"text/html\", emit \"\"\" <h1>Hello, #{name}!</h1> <p>Your request:</p> <pre> #{json::encode_pretty(request_data)} </pre> \"\"\" We don't include the whole pipeline logic here for brevity, but you can view it in full here . Error handling \u00b6 Of special interest is the binding specific for error handling -- we make sure to link the err ports from all the involved onramp/pipeline aretefacts and also ensure that the error events from those artefacts are bubbled up to the client appropriately, with proper HTTP status code (the latter is done via routing them all to the central internal_error_processing pipeline). - id : error links : \"/onramp/http/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/request_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] # send back errors as response as well \"/pipeline/internal_error_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] # respond on errors during error processing too \"/pipeline/internal_error_processing/{instance}/err\" : [ \"/onramp/http/{instance}/in\" ] Testing \u00b6 Assuming you have all the code from the git repository , run the following to start our application: docker-compose up Status checks \u00b6 To verify that the server is up and running: $ curl -v http://localhost:8139/snot \"badger\" # or a traditional ping path $ curl -s -o /dev/null -w \"\" % { http_code } http://localhost:8139/ping 200 HTML pages \u00b6 If you navigate to http://localhost:8139/ from your browser, you should be redirected to http://localhost:8139/index first (part of the request_processing pipeline logic), and then you should be able to see all the request attributes that your browser sent to the server, pretty-printed. Also try something like http://localhost:8139/index?name=badger -- we have a very simple dynamic web application now! Request body decoding \u00b6 The default codec for the onramp is string but if we set the Content-Type header at request time, the rest onramp uses it to decode the request body instead. $ curl -v -XPOST -H 'Content-Type:application/json' http://localhost:8139/echo -d '{\"snot\": \"badger\"}' Note: Unnecessary use of -X or --request, POST is already inferred. * Trying 127 .0.0.1:8139... * TCP_NODELAY set * Connected to localhost ( 127 .0.0.1 ) port 8139 ( #0) > POST /echo HTTP/1.1 > Host: localhost:8139 > User-Agent: curl/7.65.3 > Accept: */* > Content-Type:application/json > Content-Length: 18 > * upload completely sent off: 18 out of 18 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 265 < date: Thu, 15 Oct 2020 03 :11:09 GMT < content-type: application/json < x-powered-by: Tremor < * Connection #0 to host localhost left intact { \"body\" : { \"snot\" : \"badger\" } , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/json\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \" localhost:8139\" ] , \"user-agent\" : [ \"curl/7.65.3\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}} # without the content-type header, `body` in the output would be an escaped json string here $ curl -XPOST http://localhost:8139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:8139\" ] , \"user-agent\" : [ \"curl/7.65.3\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}} Stateful logic \u00b6 To see the no of requests processed so far: $ curl http://localhost:8139/stats { \"requests_processed\" :7 } This is utilizing the pipeline state mechanism under the hood -- a simple yet powerful way to build stateful applications. Error handling \u00b6 For the application-layer errors, the server allows for defining custom error responses and bubbling them up with proper HTTP status error code. Example with non-existent paths: $ curl -i http://localhost:8139/non-existent-path HTTP/1.1 404 Not Found content-length: 57 date: Thu, 15 Oct 2020 03 :00:05 GMT content-type: application/json x-powered-by: Tremor { \"error\" : \"Path not found: /non-existent-path\" , \"event\" : \"\" } Internal tremor errors are also handled gracefully (via the internal_error_processing pipeline): # testing an endpoint that intentionally uses an undefined var: throws a runtime error $ curl -i http://localhost:8139/error-test HTTP/1.1 500 Internal Server Error content-length: 202 date: Thu, 15 Oct 2020 03 :06:09 GMT content-type: application/json { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error: \\n 73 | emit \\\"\\\"\\n | ^^^^^^^^^^^^^^^^ Trying to access a non existing local key `non_existent_var`\\n\\n\" , \"event\" : \"\" } # similarly, for onramp-level error when invalid data is sent (non-json here when the request content-type header is set to be json) $ curl -H 'Content-Type:application/json' http://localhost:8139/echo -d '{' { \"error\" : \"Oh no, we ran into something unexpected :(\\n [Codec] Syntax at character 0 ('{')\" , \"event_id\" :6, \"source_id\" : \"tremor://localhost/onramp/http/01/in\" }","title":"HTTP Server"},{"location":"workshop/examples/30_servers_lt_http/#http-server","text":"Example HTTP server application built on top of Tremor and meant to be a demonstration of linked transports .","title":"HTTP Server"},{"location":"workshop/examples/30_servers_lt_http/#setup","text":"Tip All the code here is available in the git repository as well.","title":"Setup"},{"location":"workshop/examples/30_servers_lt_http/#sources","text":"We configure a rest onramp listening on port 8139: onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 8139","title":"Sources"},{"location":"workshop/examples/30_servers_lt_http/#request-flow","text":"Incoming requests from clients are directed to the pipeline named request_processing pipeline, and the output of the pipeline is fed back again to the onramp -- this now becomes the server response to the incoming request. binding : - id : main links : \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and send back the response \"/pipeline/request_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ]","title":"Request flow"},{"location":"workshop/examples/30_servers_lt_http/#processing-logic","text":"In the request_processing pipeline, we are free to process the incoming request using tremor-script/tremor-query, and leveraging the various request and response metadata variables for the rest onramp . The event flow within the pipeline is captured below: create script process ; # main request processing select event from in into process ; select event from process into out ; # our defined app errors (still succesful processing from tremor's perspective) # useful to track these from different port (app_error) for metrics select event from process / app_error into out ; # tremor runtime errors from the processing script select event from process / err into err ; Example section of the process script here, demonstrating how the index page for the HTTP server is implemented (also parses the url query params to demonstrate dynamic responses based on provided user input): case \"/index\" => let request_data = { \"body\": event, \"meta\": $request, }, # determine the name to greet let name = match $request.url of case %{present query} => let query_parsed = utils::parse_query($request.url.query), let request_data.url_query_parsed = query_parsed, match query_parsed of case %{present name} => query_parsed.name default => \"world\" end default => \"world\" end, # serve html! let $response.headers[\"content-type\"] = \"text/html\", emit \"\"\" <h1>Hello, #{name}!</h1> <p>Your request:</p> <pre> #{json::encode_pretty(request_data)} </pre> \"\"\" We don't include the whole pipeline logic here for brevity, but you can view it in full here .","title":"Processing logic"},{"location":"workshop/examples/30_servers_lt_http/#error-handling","text":"Of special interest is the binding specific for error handling -- we make sure to link the err ports from all the involved onramp/pipeline aretefacts and also ensure that the error events from those artefacts are bubbled up to the client appropriately, with proper HTTP status code (the latter is done via routing them all to the central internal_error_processing pipeline). - id : error links : \"/onramp/http/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/request_processing/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] # send back errors as response as well \"/pipeline/internal_error_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] # respond on errors during error processing too \"/pipeline/internal_error_processing/{instance}/err\" : [ \"/onramp/http/{instance}/in\" ]","title":"Error handling"},{"location":"workshop/examples/30_servers_lt_http/#testing","text":"Assuming you have all the code from the git repository , run the following to start our application: docker-compose up","title":"Testing"},{"location":"workshop/examples/30_servers_lt_http/#status-checks","text":"To verify that the server is up and running: $ curl -v http://localhost:8139/snot \"badger\" # or a traditional ping path $ curl -s -o /dev/null -w \"\" % { http_code } http://localhost:8139/ping 200","title":"Status checks"},{"location":"workshop/examples/30_servers_lt_http/#html-pages","text":"If you navigate to http://localhost:8139/ from your browser, you should be redirected to http://localhost:8139/index first (part of the request_processing pipeline logic), and then you should be able to see all the request attributes that your browser sent to the server, pretty-printed. Also try something like http://localhost:8139/index?name=badger -- we have a very simple dynamic web application now!","title":"HTML pages"},{"location":"workshop/examples/30_servers_lt_http/#request-body-decoding","text":"The default codec for the onramp is string but if we set the Content-Type header at request time, the rest onramp uses it to decode the request body instead. $ curl -v -XPOST -H 'Content-Type:application/json' http://localhost:8139/echo -d '{\"snot\": \"badger\"}' Note: Unnecessary use of -X or --request, POST is already inferred. * Trying 127 .0.0.1:8139... * TCP_NODELAY set * Connected to localhost ( 127 .0.0.1 ) port 8139 ( #0) > POST /echo HTTP/1.1 > Host: localhost:8139 > User-Agent: curl/7.65.3 > Accept: */* > Content-Type:application/json > Content-Length: 18 > * upload completely sent off: 18 out of 18 bytes * Mark bundle as not supporting multiuse < HTTP/1.1 200 OK < content-length: 265 < date: Thu, 15 Oct 2020 03 :11:09 GMT < content-type: application/json < x-powered-by: Tremor < * Connection #0 to host localhost left intact { \"body\" : { \"snot\" : \"badger\" } , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/json\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \" localhost:8139\" ] , \"user-agent\" : [ \"curl/7.65.3\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}} # without the content-type header, `body` in the output would be an escaped json string here $ curl -XPOST http://localhost:8139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:8139\" ] , \"user-agent\" : [ \"curl/7.65.3\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}}","title":"Request body decoding"},{"location":"workshop/examples/30_servers_lt_http/#stateful-logic","text":"To see the no of requests processed so far: $ curl http://localhost:8139/stats { \"requests_processed\" :7 } This is utilizing the pipeline state mechanism under the hood -- a simple yet powerful way to build stateful applications.","title":"Stateful logic"},{"location":"workshop/examples/30_servers_lt_http/#error-handling_1","text":"For the application-layer errors, the server allows for defining custom error responses and bubbling them up with proper HTTP status error code. Example with non-existent paths: $ curl -i http://localhost:8139/non-existent-path HTTP/1.1 404 Not Found content-length: 57 date: Thu, 15 Oct 2020 03 :00:05 GMT content-type: application/json x-powered-by: Tremor { \"error\" : \"Path not found: /non-existent-path\" , \"event\" : \"\" } Internal tremor errors are also handled gracefully (via the internal_error_processing pipeline): # testing an endpoint that intentionally uses an undefined var: throws a runtime error $ curl -i http://localhost:8139/error-test HTTP/1.1 500 Internal Server Error content-length: 202 date: Thu, 15 Oct 2020 03 :06:09 GMT content-type: application/json { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error: \\n 73 | emit \\\"\\\"\\n | ^^^^^^^^^^^^^^^^ Trying to access a non existing local key `non_existent_var`\\n\\n\" , \"event\" : \"\" } # similarly, for onramp-level error when invalid data is sent (non-json here when the request content-type header is set to be json) $ curl -H 'Content-Type:application/json' http://localhost:8139/echo -d '{' { \"error\" : \"Oh no, we ran into something unexpected :(\\n [Codec] Syntax at character 0 ('{')\" , \"event_id\" :6, \"source_id\" : \"tremor://localhost/onramp/http/01/in\" }","title":"Error handling"},{"location":"workshop/examples/31_servers_lt_ws/","text":"Websocket Server \u00b6 Example Websocket server application built on top of Tremor and meant to be a demonstration of linked transports . Setup \u00b6 Tip All the code here is available in the git repository as well. Sources \u00b6 We configure a websocket onramp listening on port 8139: - id : ws type : ws linked : true codec : string preprocessors : - lines config : host : 0.0.0.0 port : 8139 Message flow \u00b6 Incoming websocket messages from a client's websocket connection are sent to the pipeline echo and the output of it is sent back again from the same connection. binding : - id : main links : \"/onramp/ws/{instance}/out\" : [ \"/pipeline/echo/{instance}/in\" ] \"/pipeline/echo/{instance}/out\" : [ \"/onramp/ws/{instance}/in\" ] Processing logic \u00b6 Implementation for the echo pipeline (techincally, echo with one special twist): define script process script match event of # snot is a special snowflake case \"snot\" => \"badger\" default => event end end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; Testing \u00b6 Assuming you have all the code from the git repository , run the following to start our application: docker-compose up Test the websocket server with a tool like websocat . Note Can be installed via cargo install websocat for the lazy/impatient amongst us # anything you type and enter, will be echoed back # except snot which begets badger $ websocat ws://localhost:8139 hello hello world world snot badger goodbye goodbye If there's internal tremor error while processing the incoming message (eg: codec or preprocessor failure), the error should be bubbled up to the client. To test this out, change the codec in the onramp configuration to be json from string and send an invalid json input: # after changing the onramp codec to json $ echo \"{\" | websocat -n1 ws://localhost:8139 { \"error\" : \"[Codec] Syntax at character 0 ('{')\" , \"event_id\" :1, \"source_id\" : \"tremor://localhost/onramp/ws/01/in\" }","title":"Websocket Server"},{"location":"workshop/examples/31_servers_lt_ws/#websocket-server","text":"Example Websocket server application built on top of Tremor and meant to be a demonstration of linked transports .","title":"Websocket Server"},{"location":"workshop/examples/31_servers_lt_ws/#setup","text":"Tip All the code here is available in the git repository as well.","title":"Setup"},{"location":"workshop/examples/31_servers_lt_ws/#sources","text":"We configure a websocket onramp listening on port 8139: - id : ws type : ws linked : true codec : string preprocessors : - lines config : host : 0.0.0.0 port : 8139","title":"Sources"},{"location":"workshop/examples/31_servers_lt_ws/#message-flow","text":"Incoming websocket messages from a client's websocket connection are sent to the pipeline echo and the output of it is sent back again from the same connection. binding : - id : main links : \"/onramp/ws/{instance}/out\" : [ \"/pipeline/echo/{instance}/in\" ] \"/pipeline/echo/{instance}/out\" : [ \"/onramp/ws/{instance}/in\" ]","title":"Message flow"},{"location":"workshop/examples/31_servers_lt_ws/#processing-logic","text":"Implementation for the echo pipeline (techincally, echo with one special twist): define script process script match event of # snot is a special snowflake case \"snot\" => \"badger\" default => event end end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ;","title":"Processing logic"},{"location":"workshop/examples/31_servers_lt_ws/#testing","text":"Assuming you have all the code from the git repository , run the following to start our application: docker-compose up Test the websocket server with a tool like websocat . Note Can be installed via cargo install websocat for the lazy/impatient amongst us # anything you type and enter, will be echoed back # except snot which begets badger $ websocat ws://localhost:8139 hello hello world world snot badger goodbye goodbye If there's internal tremor error while processing the incoming message (eg: codec or preprocessor failure), the error should be bubbled up to the client. To test this out, change the codec in the onramp configuration to be json from string and send an invalid json input: # after changing the onramp codec to json $ echo \"{\" | websocat -n1 ws://localhost:8139 { \"error\" : \"[Codec] Syntax at character 0 ('{')\" , \"event_id\" :1, \"source_id\" : \"tremor://localhost/onramp/ws/01/in\" }","title":"Testing"},{"location":"workshop/examples/32_proxies_lt_http/","text":"HTTP Proxy \u00b6 Example HTTP proxy application built on top of Tremor and meant to be a demonstration of linked transports . Setup \u00b6 Tip All the code here is available in the git repository as well. Sources and sinks \u00b6 We configure a rest onramp listening on port 9139, that is meant to be a proxy for our example HTTP server (configured as en endpoint in the rest offramp here). onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 9139 offramp : - id : upstream type : rest linked : true codec : string config : endpoint : host : tremor-server port : 8139 Request flow \u00b6 Incoming requests from clients are forwarded to the request_processing pipeline, from where it goes to the upstream server. The resulting response is then returned back to the client which initiated the request (after any needed processing from the response_processing pipeline). binding : - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to upstream \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] # send the response from upstream for processing \"/offramp/upstream/{instance}/out\" : [ \"/pipeline/response_processing/{instance}/in\" ] # process upstream response and send it back as a response to incoming \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] Processing logic \u00b6 Implementation for the request_processing pipeline: define script process script # erase the host/port from request url so that requests are routed # to the endpoint configured as part of the rest offramp # can set endpoint concretely here too, depending on the need # (eg: different endpoint based on request path/headers) let $ endpoint = patch $ request . url of erase \"host\" , erase \"port\" end ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; This example demonstrates the minimal processing needed for the proxying logic to work, but you can do any arbitrary processsing on the incoming request as needed (eg: deciding a different upstream based on certain incoming request attributes like headers or request paths). The response_processing pipeline is similarly minimal -- it just adds an entry to the x-powered-by header for showing response modifications (if you don't need it, you can just use a passthrough pipeline, or even rely on the default system::passthrough pipeline which eliminates the need to create this new pipeline). Testing \u00b6 Assuming you have all the code from the git repository , run the following to start our application (along with the tremor http server example that is the upstream for our proxy): docker-compose up Now let's try to access an endpoint that we know is available in the upstream server: # via the proxy. note the additional entry we have in the x-powered-by header $ curl -i http://localhost:9139/snot HTTP/1.1 200 OK content-length: 8 x-powered-by: Tremor, Tremor ( As Proxy ) content-type: application/json date: Thu, 15 Oct 2020 05 :00:06 GMT \"badger\" # just the upstream $ curl -i http://localhost:8139/snot HTTP/1.1 200 OK content-length: 8 date: Thu, 15 Oct 2020 05 :00:44 GMT content-type: application/json x-powered-by: Tremor \"badger\" All the testing examples for the example HTTP server should work from here as well, with the port 8139 there swapped to our proxy application port 9139 .","title":"HTTP Proxy"},{"location":"workshop/examples/32_proxies_lt_http/#http-proxy","text":"Example HTTP proxy application built on top of Tremor and meant to be a demonstration of linked transports .","title":"HTTP Proxy"},{"location":"workshop/examples/32_proxies_lt_http/#setup","text":"Tip All the code here is available in the git repository as well.","title":"Setup"},{"location":"workshop/examples/32_proxies_lt_http/#sources-and-sinks","text":"We configure a rest onramp listening on port 9139, that is meant to be a proxy for our example HTTP server (configured as en endpoint in the rest offramp here). onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 9139 offramp : - id : upstream type : rest linked : true codec : string config : endpoint : host : tremor-server port : 8139","title":"Sources and sinks"},{"location":"workshop/examples/32_proxies_lt_http/#request-flow","text":"Incoming requests from clients are forwarded to the request_processing pipeline, from where it goes to the upstream server. The resulting response is then returned back to the client which initiated the request (after any needed processing from the response_processing pipeline). binding : - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to upstream \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] # send the response from upstream for processing \"/offramp/upstream/{instance}/out\" : [ \"/pipeline/response_processing/{instance}/in\" ] # process upstream response and send it back as a response to incoming \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ]","title":"Request flow"},{"location":"workshop/examples/32_proxies_lt_http/#processing-logic","text":"Implementation for the request_processing pipeline: define script process script # erase the host/port from request url so that requests are routed # to the endpoint configured as part of the rest offramp # can set endpoint concretely here too, depending on the need # (eg: different endpoint based on request path/headers) let $ endpoint = patch $ request . url of erase \"host\" , erase \"port\" end ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; This example demonstrates the minimal processing needed for the proxying logic to work, but you can do any arbitrary processsing on the incoming request as needed (eg: deciding a different upstream based on certain incoming request attributes like headers or request paths). The response_processing pipeline is similarly minimal -- it just adds an entry to the x-powered-by header for showing response modifications (if you don't need it, you can just use a passthrough pipeline, or even rely on the default system::passthrough pipeline which eliminates the need to create this new pipeline).","title":"Processing logic"},{"location":"workshop/examples/32_proxies_lt_http/#testing","text":"Assuming you have all the code from the git repository , run the following to start our application (along with the tremor http server example that is the upstream for our proxy): docker-compose up Now let's try to access an endpoint that we know is available in the upstream server: # via the proxy. note the additional entry we have in the x-powered-by header $ curl -i http://localhost:9139/snot HTTP/1.1 200 OK content-length: 8 x-powered-by: Tremor, Tremor ( As Proxy ) content-type: application/json date: Thu, 15 Oct 2020 05 :00:06 GMT \"badger\" # just the upstream $ curl -i http://localhost:8139/snot HTTP/1.1 200 OK content-length: 8 date: Thu, 15 Oct 2020 05 :00:44 GMT content-type: application/json x-powered-by: Tremor \"badger\" All the testing examples for the example HTTP server should work from here as well, with the port 8139 there swapped to our proxy application port 9139 .","title":"Testing"},{"location":"workshop/examples/33_proxies_lt_ws/","text":"Websocket Proxy \u00b6 Example Websocket proxy application built on top of Tremor and meant to be a demonstration of linked transports . Setup \u00b6 Tip All the code here is available in the git repository as well. Sources and sinks \u00b6 We configure a websocket onramp listening on port 9139, that is meant to be a proxy for our example websocket server (configured as en endpoint in the websocket offramp here). onramp : - id : ws type : ws linked : true codec : string preprocessors : - lines config : host : 0.0.0.0 port : 9139 offramp : - id : upstream type : ws linked : true codec : string postprocessors : - lines config : url : \"ws://tremor-server:8139\" Message flow \u00b6 Incoming websocket messages from a client's websocket connection are forwarded to the upstream websocket server (via the pass_incoming pipeline which just lives up to its name). The resulting upstream reply is then returned back to the client reusing its connection (after a quick pass through the pass_outgoing pipeline). binding : - id : main links : \"/onramp/ws/{instance}/out\" : [ \"/pipeline/pass_incoming/{instance}/in\" ] \"/pipeline/pass_incoming/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] \"/offramp/upstream/{instance}/out\" : [ \"/pipeline/pass_outgoing/{instance}/in\" ] \"/pipeline/pass_outgoing/{instance}/out\" : [ \"/onramp/ws/{instance}/in\" ] Processing logic \u00b6 Implementation for the pass_incoming (as well as pass_outgoing ) pipeline: select event from in into out ; This example is intentionally light on the processing but you can imagine doing arbitrary processing based on the event data here (as well as dynamically changing the confiuration for the websocket offramp via its metadata variables -- eg: things like the server url). Testing \u00b6 Assuming you have all the code from the git repository , run the following to start our application (along with the tremor websocket server example that is the upstream for our proxy): docker-compose up Now let's try to test the echo capabilities of our upstream server, via a tool like websocat . Note Can be installed via cargo install websocat for the lazy/impatient amongst us # via proxy $ echo \"hello\" | websocat -n1 ws://localhost:9139 hello # just the upstream $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello Our special snot-handling works as well: $ echo \"snot\" | websocat -n1 ws://localhost:9139 badger And if there's an internal tremor error while processing both the incoming message and the upstream reply to it (eg: codec or pre/post-processor failure), or if the upstream server is just down, an error will be bubbled up to the client. Example: # stop the upstream server $ docker stop 33_proxies_lt_ws_tremor-server_1 # upstream connection now gets closed from the proxy $ echo \"hello\" | websocat -n1 ws://localhost:9139 { \"error\" : \"Error receiving reply from server ws://localhost:8139: WebSocket protocol error: Connection reset without closing handshake\" , \"event_id\" : \" 1: 2\" } # sending further messages results in errors $ echo \"hello\" | websocat -n1 ws://localhost:9139 $ echo \"hello\" | websocat -n1 ws://localhost:9139 { \"error\" : \"Error sending event to server ws://localhost:8139: Trying to work with closed connection\" , \"event_id\" : \"1: 3\" }","title":"Websocket Proxy"},{"location":"workshop/examples/33_proxies_lt_ws/#websocket-proxy","text":"Example Websocket proxy application built on top of Tremor and meant to be a demonstration of linked transports .","title":"Websocket Proxy"},{"location":"workshop/examples/33_proxies_lt_ws/#setup","text":"Tip All the code here is available in the git repository as well.","title":"Setup"},{"location":"workshop/examples/33_proxies_lt_ws/#sources-and-sinks","text":"We configure a websocket onramp listening on port 9139, that is meant to be a proxy for our example websocket server (configured as en endpoint in the websocket offramp here). onramp : - id : ws type : ws linked : true codec : string preprocessors : - lines config : host : 0.0.0.0 port : 9139 offramp : - id : upstream type : ws linked : true codec : string postprocessors : - lines config : url : \"ws://tremor-server:8139\"","title":"Sources and sinks"},{"location":"workshop/examples/33_proxies_lt_ws/#message-flow","text":"Incoming websocket messages from a client's websocket connection are forwarded to the upstream websocket server (via the pass_incoming pipeline which just lives up to its name). The resulting upstream reply is then returned back to the client reusing its connection (after a quick pass through the pass_outgoing pipeline). binding : - id : main links : \"/onramp/ws/{instance}/out\" : [ \"/pipeline/pass_incoming/{instance}/in\" ] \"/pipeline/pass_incoming/{instance}/out\" : [ \"/offramp/upstream/{instance}/in\" ] \"/offramp/upstream/{instance}/out\" : [ \"/pipeline/pass_outgoing/{instance}/in\" ] \"/pipeline/pass_outgoing/{instance}/out\" : [ \"/onramp/ws/{instance}/in\" ]","title":"Message flow"},{"location":"workshop/examples/33_proxies_lt_ws/#processing-logic","text":"Implementation for the pass_incoming (as well as pass_outgoing ) pipeline: select event from in into out ; This example is intentionally light on the processing but you can imagine doing arbitrary processing based on the event data here (as well as dynamically changing the confiuration for the websocket offramp via its metadata variables -- eg: things like the server url).","title":"Processing logic"},{"location":"workshop/examples/33_proxies_lt_ws/#testing","text":"Assuming you have all the code from the git repository , run the following to start our application (along with the tremor websocket server example that is the upstream for our proxy): docker-compose up Now let's try to test the echo capabilities of our upstream server, via a tool like websocat . Note Can be installed via cargo install websocat for the lazy/impatient amongst us # via proxy $ echo \"hello\" | websocat -n1 ws://localhost:9139 hello # just the upstream $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello Our special snot-handling works as well: $ echo \"snot\" | websocat -n1 ws://localhost:9139 badger And if there's an internal tremor error while processing both the incoming message and the upstream reply to it (eg: codec or pre/post-processor failure), or if the upstream server is just down, an error will be bubbled up to the client. Example: # stop the upstream server $ docker stop 33_proxies_lt_ws_tremor-server_1 # upstream connection now gets closed from the proxy $ echo \"hello\" | websocat -n1 ws://localhost:9139 { \"error\" : \"Error receiving reply from server ws://localhost:8139: WebSocket protocol error: Connection reset without closing handshake\" , \"event_id\" : \" 1: 2\" } # sending further messages results in errors $ echo \"hello\" | websocat -n1 ws://localhost:9139 $ echo \"hello\" | websocat -n1 ws://localhost:9139 { \"error\" : \"Error sending event to server ws://localhost:8139: Trying to work with closed connection\" , \"event_id\" : \"1: 3\" }","title":"Testing"},{"location":"workshop/examples/34_bridges_lt_http_ws/","text":"HTTP -> Websocket Bridge \u00b6 Example HTTP -> Websocket bridge application built on top of Tremor and meant to be a demonstration of linked transports . Setup \u00b6 Tip All the code here is available in the git repository as well. Sources and sinks \u00b6 We configure a rest onramp listening on port 9139, that is meant to be a bridge for our example websocket server (configured as en endpoint in the websocket offramp here). onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 9139 offramp : - id : ws type : ws linked : true codec : string postprocessors : - lines config : url : \"ws://tremor-server:8139\" Request flow \u00b6 Incoming HTTP requests from clients are forwarded to the request_processing pipeline, from where it goes to the websocket server. The resulting websocket message reply is then sent back as HTTP response to the client which initiated the request (after some needed processing from the response_processing pipeline). binding : - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to ws offramp \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/ws/{instance}/in\" ] # send the response from ws offramp to the passthrough pipeline # this works well as long as the passthrough pipeline is not used # by anything else (which is the case for this example) \"/offramp/ws/{instance}/out\" : [ \"/pipeline/response_processing/{instance}/in\" ] # send the ws repsonse back as a response to incoming \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ] Processing logic \u00b6 Implementation for the request_processing pipeline: define script process script match $ request . url . path of # only pass requests to /bridge case \"/bridge\" => null default => # can send this to a different port than the default err port too emit { \"error\" : \"Unsupported url path: {$request.url.path}\" , \"event\" : event } => \"err\" end ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; Implementation for the response_processing pipeline: define script process script # defaults for the server response let $ response = { \"status\" : 200 , \"headers\" : { \"x-powered-by\" : \"Tremor\" , } } ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; Testing \u00b6 Assuming you have all the code from the git repository , run the following to start our application (along with the tremor websocket server example that our application is bridging to): docker-compose up Now let's try to test the echo capabilities of the websocket server example, via the HTTP bridge: # via the HTTP bridge $ curl -i http://localhost:9139/bridge -d \"hello\" HTTP/1.1 200 OK content-length: 5 date: Thu, 15 Oct 2020 05 :24:23 GMT content-type: text/plain x-powered-by: Tremor hello # just the websocket server $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello Our special snot-handling works as well: $ curl -i http://localhost:9139/bridge -d \"snot\" badger Only the /bridge path (as setup from the pipeline) works for the bridging: $ curl http://localhost:9139/some_path -d \"snot\" { \"error\" : \"Oh no, we ran into something unexpected :(\\n Unsupported url path: /some_path\" , \"event\" : \"snot\" } And if there's an internal tremor error while processing both the incoming HTTP request and the websocket reply to it (eg: codec or pre/post-processor failure), or if the websocket server is just down, an error will be bubbled up to the client. Example: # stop the websocket server $ docker stop 34_bridges_lt_http_ws_tremor-server_1 # websocket server connection now gets closed from the bridge $ curl -i http://localhost:9139/bridge -d \"hello\" HTTP/1.1 500 Internal Server Error content-length: 198 date: Fri, 16 Oct 2020 04 :12:11 GMT content-type: application/json { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error receiving reply from server ws://localhost:8139: WebSocket protocol error: Connection reset without closing handshake\" , \"event_id\" : \"1: 3\" } # sending further messages results in errors $ curl http://localhost:9139/bridge -d \"hello\" { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error sending event to server ws://localhost:8139: Trying to work with closed connection\" , \"e vent_id\" : \"1: 4\" }","title":"HTTP -> WS Bridge"},{"location":"workshop/examples/34_bridges_lt_http_ws/#http-websocket-bridge","text":"Example HTTP -> Websocket bridge application built on top of Tremor and meant to be a demonstration of linked transports .","title":"HTTP -&gt; Websocket Bridge"},{"location":"workshop/examples/34_bridges_lt_http_ws/#setup","text":"Tip All the code here is available in the git repository as well.","title":"Setup"},{"location":"workshop/examples/34_bridges_lt_http_ws/#sources-and-sinks","text":"We configure a rest onramp listening on port 9139, that is meant to be a bridge for our example websocket server (configured as en endpoint in the websocket offramp here). onramp : - id : http type : rest linked : true codec : string config : host : 0.0.0.0 port : 9139 offramp : - id : ws type : ws linked : true codec : string postprocessors : - lines config : url : \"ws://tremor-server:8139\"","title":"Sources and sinks"},{"location":"workshop/examples/34_bridges_lt_http_ws/#request-flow","text":"Incoming HTTP requests from clients are forwarded to the request_processing pipeline, from where it goes to the websocket server. The resulting websocket message reply is then sent back as HTTP response to the client which initiated the request (after some needed processing from the response_processing pipeline). binding : - id : main links : # send incoming requests for processing \"/onramp/http/{instance}/out\" : [ \"/pipeline/request_processing/{instance}/in\" ] # process incoming requests and relay it to ws offramp \"/pipeline/request_processing/{instance}/out\" : [ \"/offramp/ws/{instance}/in\" ] # send the response from ws offramp to the passthrough pipeline # this works well as long as the passthrough pipeline is not used # by anything else (which is the case for this example) \"/offramp/ws/{instance}/out\" : [ \"/pipeline/response_processing/{instance}/in\" ] # send the ws repsonse back as a response to incoming \"/pipeline/response_processing/{instance}/out\" : [ \"/onramp/http/{instance}/in\" ]","title":"Request flow"},{"location":"workshop/examples/34_bridges_lt_http_ws/#processing-logic","text":"Implementation for the request_processing pipeline: define script process script match $ request . url . path of # only pass requests to /bridge case \"/bridge\" => null default => # can send this to a different port than the default err port too emit { \"error\" : \"Unsupported url path: {$request.url.path}\" , \"event\" : event } => \"err\" end ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ; Implementation for the response_processing pipeline: define script process script # defaults for the server response let $ response = { \"status\" : 200 , \"headers\" : { \"x-powered-by\" : \"Tremor\" , } } ; event ; end ; create script process ; # main request processing select event from in into process ; select event from process into out ; # tremor runtime errors from the processing script select event from process / err into err ;","title":"Processing logic"},{"location":"workshop/examples/34_bridges_lt_http_ws/#testing","text":"Assuming you have all the code from the git repository , run the following to start our application (along with the tremor websocket server example that our application is bridging to): docker-compose up Now let's try to test the echo capabilities of the websocket server example, via the HTTP bridge: # via the HTTP bridge $ curl -i http://localhost:9139/bridge -d \"hello\" HTTP/1.1 200 OK content-length: 5 date: Thu, 15 Oct 2020 05 :24:23 GMT content-type: text/plain x-powered-by: Tremor hello # just the websocket server $ echo \"hello\" | websocat -n1 ws://localhost:8139 hello Our special snot-handling works as well: $ curl -i http://localhost:9139/bridge -d \"snot\" badger Only the /bridge path (as setup from the pipeline) works for the bridging: $ curl http://localhost:9139/some_path -d \"snot\" { \"error\" : \"Oh no, we ran into something unexpected :(\\n Unsupported url path: /some_path\" , \"event\" : \"snot\" } And if there's an internal tremor error while processing both the incoming HTTP request and the websocket reply to it (eg: codec or pre/post-processor failure), or if the websocket server is just down, an error will be bubbled up to the client. Example: # stop the websocket server $ docker stop 34_bridges_lt_http_ws_tremor-server_1 # websocket server connection now gets closed from the bridge $ curl -i http://localhost:9139/bridge -d \"hello\" HTTP/1.1 500 Internal Server Error content-length: 198 date: Fri, 16 Oct 2020 04 :12:11 GMT content-type: application/json { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error receiving reply from server ws://localhost:8139: WebSocket protocol error: Connection reset without closing handshake\" , \"event_id\" : \"1: 3\" } # sending further messages results in errors $ curl http://localhost:9139/bridge -d \"hello\" { \"error\" : \"Oh no, we ran into something unexpected :(\\n Error sending event to server ws://localhost:8139: Trying to work with closed connection\" , \"e vent_id\" : \"1: 4\" }","title":"Testing"},{"location":"workshop/examples/35_reverse_proxy_load_balancing/","text":"Reverse proxy with Load Balancing \u00b6 This example shows how to setup tremor as a reverse proxy for HTTP/1.1 that load balances between multiple upstream servers in a round-robin fashion. We are going to make use of the new linked transport and Quality of Service features in tremor 0.9 . Setting up multiple web-servers for testing purposes \u00b6 We use the server behind https://httpbin.org to have three endpoints ready to proxy to and to inspect what we sent vs. what the upstream servers received. Three httpbin servers are set up in the accompagnying docker-compose.yml . Setting up Tremor as a reverse proxy \u00b6 Tip All the code here is available in the git repository as well. To issue incoming HTTP requests to an upstream HTTP server a REST onramp needs to be configured in config.yaml to listen on a port of our choice: onramp : - id : http_in type : rest linked : true # with this (new) setting, this onramp will be able to receive and send out responses to each request codec : json config : host : 0.0.0.0 port : 65535 To forward received requests to the httpbin upstream servers a REST offramp needs to be configured in config.yaml to point at each of it: offramp : - id : upstream01 type : rest linked : true codec : json config : endpoint : host : webserver01 concurrency : 9 # allo max 9 concurrent in-flight requests - id : upstream02 type : rest linked : true codec : json config : endpoint : host : webserver02 method : POST # set a default method if no $request.method is set - id : upstream03 type : rest linked : true codec : json config : endpoint : host : webserver03 headers : # add some headers \"X-Upstream\" : \"upstream03\" Then we need to do the actual proxying in a pipeline that receives requests from the http_in onramp, inspects it, manipulates it and forwards it to one of the configured offramps: define script request_handling script let host = match $ request . headers of case % { present host } => $ request . headers . host [ 0 ] default => \"UNDEFINED\" end ; let forwarded = \"by=localhost:65535;host={host};proto=http\" ; let $ request . headers [ \"Forwarded\" ] = forwarded ; # fiddle with the event let event = patch event of insert \"forwarded\" => forwarded end ; # set request url parts for forwarding it to the rest offramp # stripping out the host and port, as they will be set by the offramp let $ endpoint = patch $ request . url of erase \"host\" , erase \"port\" end ; event ; end ; # ensure we distribute the load evenly define qos :: roundrobin operator rr with outputs = [ \"ws01\" , \"ws02\" , \"ws03\" ] end ; create operator rr ; # apply backpressure if a server cannot keep up # or is not reachable define qos :: backpressure operator bp with timeout = 10000 , # max timeout before triggering this operator - 10s steps = [ 100 , 500 , 1000 , 10000 ] # backoff steps in ms end ; create operator bp01 from bp ; create operator bp02 from bp ; create operator bp03 from bp ; create script request_handling ; # wire everything together select event from in into request_handling ; select event from request_handling into rr ; # connect each upstream throught the round-robin operator and a distinct backpressure operator select event from rr / ws01 into bp01 ; select event from rr / ws02 into bp02 ; select event from rr / ws03 into bp03 ; # create three outputs - one for each upstream server select event from bp01 into out / ws01 ; select event from bp02 into out / ws02 ; select event from bp03 into out / ws03 ; select event from request_handling / err into err ; # report error to its own port With the qos::roundrobin and qos::backpressure we distribute the load evenly and back off if a server is overloaded or events continue to fail (result in HTTP status coded >= 400 or are unable to establish a connection etc.). But this is only half a proxy without response handling getting back from the offramp, which is only now possible with the dawn of linked transports . Handling the responses coming back from the upstreams is implemented in the following pipeline: define script response_handling script use std :: array ; use tremor :: system ; # see: https://tools.ietf.org/html/rfc7230#section-5.7.1 let via_value = \"1.1 #{system :: hostname () } /tremor\" ; match $ response . headers of case % { present via } => let $ response . headers . via = array :: push ($ response . headers . via , via_value ) default => let $ response . headers . via = via_value end ; event ; end ; create script response_handling ; select event from in into response_handling ; select event from response_handling into out ; select event from response_handling / err into err ; Here we only set the Via response header. Now the single bits need to be connected in order to complete the flow back and forth between client and upstream. When linking REST offramps and onramps together it is important to take care that any error that might happen on the way is reported back to the REST onramp http_in as otherwise clients would not receive any response. Luckily with Linked Transports we can connect all error outputs easily in our binding and thus will receive proper error messages as HTTP responses. Again, we do it in config.yaml : binding : - id : main links : \"/onramp/http_in/{instance}/out\" : [ \"/pipeline/request_handling/{instance}/in\" ] # connect the three pipeline outputs to the offramps to our upstream servers \"/pipeline/request_handling/{instance}/ws01\" : [ \"/offramp/upstream01/{instance}/in\" ] \"/pipeline/request_handling/{instance}/ws02\" : [ \"/offramp/upstream02/{instance}/in\" ] \"/pipeline/request_handling/{instance}/ws03\" : [ \"/offramp/upstream03/{instance}/in\" ] # send responses from upstreams through the response handling pipeline \"/offramp/upstream01/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] \"/offramp/upstream02/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] \"/offramp/upstream03/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] # send responses back to the http_in onramp \"/pipeline/response_handling/{instance}/out\" : [ \"/onramp/http_in/{instance}/in\" ] # error handling - send errors back to the http_in onramp \"/pipeline/request_handling/{instance}/err\" : [ \"/onramp/http_in/{instance}/in\" ] \"/pipeline/response_handling/{instance}/err\" : [ \"/onramp/http_in/{instance}/in\" ] \"/offramp/upstream01/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream02/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream03/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/internal_error_processing/{instance}/out\" : [ \"onramp/http_in/{instance}/in\" ] \"/pipeline/internal_error_processing/{instance}/err\" : [ \"onramp/http_in/{instance}/in\" ] mapping : /binding/main/01 : instance : \"01\" Start the Reverse Proxy and test it \u00b6 We set up 3 upstream servers and tremor itself in the docker-compose.yml . Starting them is straight-forward: $ docker compose up In another shell, we fire up curl and send requests through our reverse proxy: $ curl -v -XGET http://localhost:65535/anything -H 'Content-Type: appliaction/json' -d '{\"snot\": \"badger\"}' * Trying ::1... * TCP_NODELAY set * Connected to localhost ( ::1 ) port 65535 ( #0) > GET /anything HTTP/1.1 > Host: localhost:65535 > User-Agent: curl/7.64.1 > Accept: */* > Content-Type: application/json > Content-Length: 18 > * upload completely sent off: 18 out of 18 bytes < HTTP/1.1 200 OK < content-length: 549 < access-control-allow-origin: * < content-type: application/json < connection: keep-alive < server: gunicorn/19.9.0 < date: Tue, 06 Oct 2020 15 :05:22 GMT < access-control-allow-credentials: true < via: 1 .1 789e85f38adc/tremor < * Connection #0 to host localhost left intact { \"args\" : {} , \"data\" : \"{\\\"snot\\\":\\\"badger\\\",\\\"forwarded\\\":\\\"by=localhost:65535;host=localhost:65535;proto=http\\\"}\" , \"files\" : {} , \"form\" : {} , \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"deflate, gzip\" , \"Content-Length\" : \"82\" , \"Content-Type\" : \"application/json\" , \"Expect\" : \"100-continue\" , \"Forwarded\" : \"by=localhost:65535;host=localhost:65535;proto=http\" , \"Host\" : \"webserver01\" , \"User-Agent\" : \"curl/7.64.1\" } , \"json\" : { \"forwarded\" : \"by=localhost:65535;host=localhost:65535;proto=http\" , \"snot\" : \"badger\" } , \"method\" : \"GET\" , \"origin\" : \"172.19.0.5\" , \"url\" : \"http://webserver01/anything\" } The tremor reverse-proxy added the forwarded field and header to the request (See the data amd headers fields of the response body) and passed through the response body from the upstream. In the case of an upstream failing, the qos::backpressure operators will kick in and discard events for the failed upstream. Here is an example response for the case an upstream is not reachable: { \"error\" : \"ConnectFailed: failed to connect to the server\" , \"event_id\" : \"1: 4\" }","title":"HTTP Load Balancing"},{"location":"workshop/examples/35_reverse_proxy_load_balancing/#reverse-proxy-with-load-balancing","text":"This example shows how to setup tremor as a reverse proxy for HTTP/1.1 that load balances between multiple upstream servers in a round-robin fashion. We are going to make use of the new linked transport and Quality of Service features in tremor 0.9 .","title":"Reverse proxy with Load Balancing"},{"location":"workshop/examples/35_reverse_proxy_load_balancing/#setting-up-multiple-web-servers-for-testing-purposes","text":"We use the server behind https://httpbin.org to have three endpoints ready to proxy to and to inspect what we sent vs. what the upstream servers received. Three httpbin servers are set up in the accompagnying docker-compose.yml .","title":"Setting up multiple web-servers for testing purposes"},{"location":"workshop/examples/35_reverse_proxy_load_balancing/#setting-up-tremor-as-a-reverse-proxy","text":"Tip All the code here is available in the git repository as well. To issue incoming HTTP requests to an upstream HTTP server a REST onramp needs to be configured in config.yaml to listen on a port of our choice: onramp : - id : http_in type : rest linked : true # with this (new) setting, this onramp will be able to receive and send out responses to each request codec : json config : host : 0.0.0.0 port : 65535 To forward received requests to the httpbin upstream servers a REST offramp needs to be configured in config.yaml to point at each of it: offramp : - id : upstream01 type : rest linked : true codec : json config : endpoint : host : webserver01 concurrency : 9 # allo max 9 concurrent in-flight requests - id : upstream02 type : rest linked : true codec : json config : endpoint : host : webserver02 method : POST # set a default method if no $request.method is set - id : upstream03 type : rest linked : true codec : json config : endpoint : host : webserver03 headers : # add some headers \"X-Upstream\" : \"upstream03\" Then we need to do the actual proxying in a pipeline that receives requests from the http_in onramp, inspects it, manipulates it and forwards it to one of the configured offramps: define script request_handling script let host = match $ request . headers of case % { present host } => $ request . headers . host [ 0 ] default => \"UNDEFINED\" end ; let forwarded = \"by=localhost:65535;host={host};proto=http\" ; let $ request . headers [ \"Forwarded\" ] = forwarded ; # fiddle with the event let event = patch event of insert \"forwarded\" => forwarded end ; # set request url parts for forwarding it to the rest offramp # stripping out the host and port, as they will be set by the offramp let $ endpoint = patch $ request . url of erase \"host\" , erase \"port\" end ; event ; end ; # ensure we distribute the load evenly define qos :: roundrobin operator rr with outputs = [ \"ws01\" , \"ws02\" , \"ws03\" ] end ; create operator rr ; # apply backpressure if a server cannot keep up # or is not reachable define qos :: backpressure operator bp with timeout = 10000 , # max timeout before triggering this operator - 10s steps = [ 100 , 500 , 1000 , 10000 ] # backoff steps in ms end ; create operator bp01 from bp ; create operator bp02 from bp ; create operator bp03 from bp ; create script request_handling ; # wire everything together select event from in into request_handling ; select event from request_handling into rr ; # connect each upstream throught the round-robin operator and a distinct backpressure operator select event from rr / ws01 into bp01 ; select event from rr / ws02 into bp02 ; select event from rr / ws03 into bp03 ; # create three outputs - one for each upstream server select event from bp01 into out / ws01 ; select event from bp02 into out / ws02 ; select event from bp03 into out / ws03 ; select event from request_handling / err into err ; # report error to its own port With the qos::roundrobin and qos::backpressure we distribute the load evenly and back off if a server is overloaded or events continue to fail (result in HTTP status coded >= 400 or are unable to establish a connection etc.). But this is only half a proxy without response handling getting back from the offramp, which is only now possible with the dawn of linked transports . Handling the responses coming back from the upstreams is implemented in the following pipeline: define script response_handling script use std :: array ; use tremor :: system ; # see: https://tools.ietf.org/html/rfc7230#section-5.7.1 let via_value = \"1.1 #{system :: hostname () } /tremor\" ; match $ response . headers of case % { present via } => let $ response . headers . via = array :: push ($ response . headers . via , via_value ) default => let $ response . headers . via = via_value end ; event ; end ; create script response_handling ; select event from in into response_handling ; select event from response_handling into out ; select event from response_handling / err into err ; Here we only set the Via response header. Now the single bits need to be connected in order to complete the flow back and forth between client and upstream. When linking REST offramps and onramps together it is important to take care that any error that might happen on the way is reported back to the REST onramp http_in as otherwise clients would not receive any response. Luckily with Linked Transports we can connect all error outputs easily in our binding and thus will receive proper error messages as HTTP responses. Again, we do it in config.yaml : binding : - id : main links : \"/onramp/http_in/{instance}/out\" : [ \"/pipeline/request_handling/{instance}/in\" ] # connect the three pipeline outputs to the offramps to our upstream servers \"/pipeline/request_handling/{instance}/ws01\" : [ \"/offramp/upstream01/{instance}/in\" ] \"/pipeline/request_handling/{instance}/ws02\" : [ \"/offramp/upstream02/{instance}/in\" ] \"/pipeline/request_handling/{instance}/ws03\" : [ \"/offramp/upstream03/{instance}/in\" ] # send responses from upstreams through the response handling pipeline \"/offramp/upstream01/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] \"/offramp/upstream02/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] \"/offramp/upstream03/{instance}/out\" : [ \"/pipeline/response_handling/{instance}/in\" ] # send responses back to the http_in onramp \"/pipeline/response_handling/{instance}/out\" : [ \"/onramp/http_in/{instance}/in\" ] # error handling - send errors back to the http_in onramp \"/pipeline/request_handling/{instance}/err\" : [ \"/onramp/http_in/{instance}/in\" ] \"/pipeline/response_handling/{instance}/err\" : [ \"/onramp/http_in/{instance}/in\" ] \"/offramp/upstream01/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream02/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/offramp/upstream03/{instance}/err\" : [ \"/pipeline/internal_error_processing/{instance}/in\" ] \"/pipeline/internal_error_processing/{instance}/out\" : [ \"onramp/http_in/{instance}/in\" ] \"/pipeline/internal_error_processing/{instance}/err\" : [ \"onramp/http_in/{instance}/in\" ] mapping : /binding/main/01 : instance : \"01\"","title":"Setting up Tremor as a reverse proxy"},{"location":"workshop/examples/35_reverse_proxy_load_balancing/#start-the-reverse-proxy-and-test-it","text":"We set up 3 upstream servers and tremor itself in the docker-compose.yml . Starting them is straight-forward: $ docker compose up In another shell, we fire up curl and send requests through our reverse proxy: $ curl -v -XGET http://localhost:65535/anything -H 'Content-Type: appliaction/json' -d '{\"snot\": \"badger\"}' * Trying ::1... * TCP_NODELAY set * Connected to localhost ( ::1 ) port 65535 ( #0) > GET /anything HTTP/1.1 > Host: localhost:65535 > User-Agent: curl/7.64.1 > Accept: */* > Content-Type: application/json > Content-Length: 18 > * upload completely sent off: 18 out of 18 bytes < HTTP/1.1 200 OK < content-length: 549 < access-control-allow-origin: * < content-type: application/json < connection: keep-alive < server: gunicorn/19.9.0 < date: Tue, 06 Oct 2020 15 :05:22 GMT < access-control-allow-credentials: true < via: 1 .1 789e85f38adc/tremor < * Connection #0 to host localhost left intact { \"args\" : {} , \"data\" : \"{\\\"snot\\\":\\\"badger\\\",\\\"forwarded\\\":\\\"by=localhost:65535;host=localhost:65535;proto=http\\\"}\" , \"files\" : {} , \"form\" : {} , \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"deflate, gzip\" , \"Content-Length\" : \"82\" , \"Content-Type\" : \"application/json\" , \"Expect\" : \"100-continue\" , \"Forwarded\" : \"by=localhost:65535;host=localhost:65535;proto=http\" , \"Host\" : \"webserver01\" , \"User-Agent\" : \"curl/7.64.1\" } , \"json\" : { \"forwarded\" : \"by=localhost:65535;host=localhost:65535;proto=http\" , \"snot\" : \"badger\" } , \"method\" : \"GET\" , \"origin\" : \"172.19.0.5\" , \"url\" : \"http://webserver01/anything\" } The tremor reverse-proxy added the forwarded field and header to the request (See the data amd headers fields of the response body) and passed through the response body from the upstream. In the case of an upstream failing, the qos::backpressure operators will kick in and discard events for the failed upstream. Here is an example response for the case an upstream is not reachable: { \"error\" : \"ConnectFailed: failed to connect to the server\" , \"event_id\" : \"1: 4\" }","title":"Start the Reverse Proxy and test it"},{"location":"workshop/examples/36_quota_service/","text":"Quota Service \u00b6 Demo quota service, geared for a log collection usecase (but can be easily applied for similar needs elsewhere too). This is a HTTP application running alongside an example log receiver setup (logs in via TCP -> tremor -> elastic out), that allows operators to easily (and quickly) update the throttling rates for various classes of logs. It also allows retrieving the current quotas for all classes, which can potentially be used by logging clients to pre-throttle on their end, even before transmitting the logs to tremor (or the information can be used to always provide up-to-date quota limits to folks using this log platform). The application itself is written on top of tremor, utilizing the linked transports feature introduced as part of tremor v0.9. Setup \u00b6 Note All the application code here is available from the docs git repository . # needed for elasticsearch container to start successfully # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_set_vm_max_map_count_to_at_least_262144 sudo sysctl -w vm.max_map_count = 262144 # start everything (tremor/elasticsearch/kibana) docker-compose up Following services should now be accessible: Quota Service: (Tremor) http://locahost:8139 Log receiver (Tremor): tcp://localhost:12202 Elasticsearch: http://locahost:9200 Kibana: http://localhost:5601/ Log Ingestion \u00b6 # send logs periodically to the tcp log receiver port (powered by tremor) ./logger.sh | nc localhost 12202 You should now see an elasticsearch index ( test_logs ) being created with the logs. Example: $ curl localhost:9200/_cat/indices yellow open test_logs xtdBODqGQ7Gub1PfUlcaWA 5 1 788 0 139kb 139kb You can also inspect/visualize the logs from kibana. Create an index pattern here (eg: test_logs* ), choosing ingestion_timestamp as the time field -- this allows timeseries navigation in the Discover page as the logs are ingested (useful since we are replaying old log data dump). Using the Quota Service \u00b6 List routes $ curl http://localhost:8139 Welcome to the Logging Quota Service! Available routes: GET /quotas POST /quotas POST /flush HEAD /ping GET /stats /echo Get quotas # populated from file etc/tremor/data/quotas.json on startup # output is keyed by class name (the details of the classification are defined in etc/tremor/config/logs.trickle) $ curl -XGET localhost:8139/quotas { \"host_default\" :500, \"logger_default\" :50, \"index_default\" :100, \"tremolo\" :100, \"application_default\" :100 } Set quotas # set rate for tremolo class of logs to 1 message per second # returns the overall quotas state on success $ curl -XPOST -H 'Content-Type: application/json' http://localhost:8139/quotas -d '{\"tremolo\": 1}' { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :1, \"index_default\" :100, \"application_default\" :100 } # If you are monitoring the tremolo logs from kibana, you should now see the (debug) field `tremor_class_rate` change to 1 (from 100). # More importantly, the log volume visible there should have also decreased. # to remove the rate override for tremolo (i.e. switch to the default, hard-coded rate for it) # curl -XPOST -H'Content-Type: application/json' http://localhost:8139/quotas -d'{\"tremolo\": null}' Example kibana view demonstrating the change in number of tremolo logs, after setting the quota dynamically from the above api: Flush quotas # force save of quotas to disk as well as publishing of quotas to any other pipelines present. # both actions are handled on quota updates as well -- this path is there in case manual flush is needed. # returns 202 if the request was succesfully accepted, along with overall quotas state. $ curl -XPOST http://localhost:8139/flush { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :1, \"index_default\" :100, \"application_default\" :100 } # to load the saved quotas during tremor restart, we need to do the following before tremor restarts right now: sudo cp etc/tremor/data/quotas.json.save etc/tremor/data/quotas.json # and after startup, run the flush once more Debug request $ curl -XPOST localhost:8139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"user-agent\" : [ \"curl/7.65.3\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:8139\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}}","title":"Quota Service"},{"location":"workshop/examples/36_quota_service/#quota-service","text":"Demo quota service, geared for a log collection usecase (but can be easily applied for similar needs elsewhere too). This is a HTTP application running alongside an example log receiver setup (logs in via TCP -> tremor -> elastic out), that allows operators to easily (and quickly) update the throttling rates for various classes of logs. It also allows retrieving the current quotas for all classes, which can potentially be used by logging clients to pre-throttle on their end, even before transmitting the logs to tremor (or the information can be used to always provide up-to-date quota limits to folks using this log platform). The application itself is written on top of tremor, utilizing the linked transports feature introduced as part of tremor v0.9.","title":"Quota Service"},{"location":"workshop/examples/36_quota_service/#setup","text":"Note All the application code here is available from the docs git repository . # needed for elasticsearch container to start successfully # https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html#_set_vm_max_map_count_to_at_least_262144 sudo sysctl -w vm.max_map_count = 262144 # start everything (tremor/elasticsearch/kibana) docker-compose up Following services should now be accessible: Quota Service: (Tremor) http://locahost:8139 Log receiver (Tremor): tcp://localhost:12202 Elasticsearch: http://locahost:9200 Kibana: http://localhost:5601/","title":"Setup"},{"location":"workshop/examples/36_quota_service/#log-ingestion","text":"# send logs periodically to the tcp log receiver port (powered by tremor) ./logger.sh | nc localhost 12202 You should now see an elasticsearch index ( test_logs ) being created with the logs. Example: $ curl localhost:9200/_cat/indices yellow open test_logs xtdBODqGQ7Gub1PfUlcaWA 5 1 788 0 139kb 139kb You can also inspect/visualize the logs from kibana. Create an index pattern here (eg: test_logs* ), choosing ingestion_timestamp as the time field -- this allows timeseries navigation in the Discover page as the logs are ingested (useful since we are replaying old log data dump).","title":"Log Ingestion"},{"location":"workshop/examples/36_quota_service/#using-the-quota-service","text":"List routes $ curl http://localhost:8139 Welcome to the Logging Quota Service! Available routes: GET /quotas POST /quotas POST /flush HEAD /ping GET /stats /echo Get quotas # populated from file etc/tremor/data/quotas.json on startup # output is keyed by class name (the details of the classification are defined in etc/tremor/config/logs.trickle) $ curl -XGET localhost:8139/quotas { \"host_default\" :500, \"logger_default\" :50, \"index_default\" :100, \"tremolo\" :100, \"application_default\" :100 } Set quotas # set rate for tremolo class of logs to 1 message per second # returns the overall quotas state on success $ curl -XPOST -H 'Content-Type: application/json' http://localhost:8139/quotas -d '{\"tremolo\": 1}' { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :1, \"index_default\" :100, \"application_default\" :100 } # If you are monitoring the tremolo logs from kibana, you should now see the (debug) field `tremor_class_rate` change to 1 (from 100). # More importantly, the log volume visible there should have also decreased. # to remove the rate override for tremolo (i.e. switch to the default, hard-coded rate for it) # curl -XPOST -H'Content-Type: application/json' http://localhost:8139/quotas -d'{\"tremolo\": null}' Example kibana view demonstrating the change in number of tremolo logs, after setting the quota dynamically from the above api: Flush quotas # force save of quotas to disk as well as publishing of quotas to any other pipelines present. # both actions are handled on quota updates as well -- this path is there in case manual flush is needed. # returns 202 if the request was succesfully accepted, along with overall quotas state. $ curl -XPOST http://localhost:8139/flush { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :1, \"index_default\" :100, \"application_default\" :100 } # to load the saved quotas during tremor restart, we need to do the following before tremor restarts right now: sudo cp etc/tremor/data/quotas.json.save etc/tremor/data/quotas.json # and after startup, run the flush once more Debug request $ curl -XPOST localhost:8139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"user-agent\" : [ \"curl/7.65.3\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:8139\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :8139, \"path\" : \"/echo\" }}}","title":"Using the Quota Service"},{"location":"workshop/examples/37_configurator/","text":"Configurator \u00b6 An application built using tremor (and the new linked transports feature new in 0.9) allowing for centralized configuration across services and their component nodes. This is an exploration project meant to push what we can do with the current tremor feature set and as such, there are/will be rough edges. Setup \u00b6 Note All the application code here is available from the docs git repository . # start everything docker-compose up Following services should be now accessible: Configurator: http://localhost:9139 Quota Service Node 1: http://localhost:8139 Quota Service Node 2: http://localhost:8140 Using the Configurator \u00b6 List routes $ curl http://localhost:9139 Welcome to the Configurator! Available routes: GET /services POST /service/<id> HEAD /ping GET /stats /echo List services $ curl http://localhost:9139/services [ \"quota_service] Set service configuration # change quotas for the quota service $ curl -XPOST -H 'Content-Type: application/json' http://localhost:9139/service/quota_service -d '{\"application_default\": 11}' # should have now applied to all the nodes in the quota service. # if the delivery fails on a node (eg: it's down or there's network issues), it will be retried until it's successful # (this works even if the configurator gets restarted during the process, since the undelivered updates are stored on disk) $ curl http://localhost:8139/quotas { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :100, \"index_default\" :100, \"application_default\" :11 } $ curl http://localhost:8140/quotas { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :100, \"index_default\" :100, \"application_default\" :11 } Debug request $ curl -XPOST localhost:9139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"user-agent\" : [ \"curl/7.65.3\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:9139\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :9139, \"path\" : \"/echo\" }}} TODO \u00b6 aggregate responses from service constituents (from tremor-script for now) explore pull model for configuration sync generate per-service config routes from openapi specs templatize new service addition/boilerplate In the context of configuring tremor nodes, problems around configuration sync/distribution will be easier to solve after tremor becomes truly clustered, but for now, we can try and tackle it with what we have and see how far we go (eg: via guaranteed delivery or periodic updates).","title":"Configurator"},{"location":"workshop/examples/37_configurator/#configurator","text":"An application built using tremor (and the new linked transports feature new in 0.9) allowing for centralized configuration across services and their component nodes. This is an exploration project meant to push what we can do with the current tremor feature set and as such, there are/will be rough edges.","title":"Configurator"},{"location":"workshop/examples/37_configurator/#setup","text":"Note All the application code here is available from the docs git repository . # start everything docker-compose up Following services should be now accessible: Configurator: http://localhost:9139 Quota Service Node 1: http://localhost:8139 Quota Service Node 2: http://localhost:8140","title":"Setup"},{"location":"workshop/examples/37_configurator/#using-the-configurator","text":"List routes $ curl http://localhost:9139 Welcome to the Configurator! Available routes: GET /services POST /service/<id> HEAD /ping GET /stats /echo List services $ curl http://localhost:9139/services [ \"quota_service] Set service configuration # change quotas for the quota service $ curl -XPOST -H 'Content-Type: application/json' http://localhost:9139/service/quota_service -d '{\"application_default\": 11}' # should have now applied to all the nodes in the quota service. # if the delivery fails on a node (eg: it's down or there's network issues), it will be retried until it's successful # (this works even if the configurator gets restarted during the process, since the undelivered updates are stored on disk) $ curl http://localhost:8139/quotas { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :100, \"index_default\" :100, \"application_default\" :11 } $ curl http://localhost:8140/quotas { \"host_default\" :500, \"logger_default\" :50, \"tremolo\" :100, \"index_default\" :100, \"application_default\" :11 } Debug request $ curl -XPOST localhost:9139/echo -d '{\"snot\": \"badger\"}' { \"body\" : \"{\\\"snot\\\": \\\"badger\\\"}\" , \"meta\" : { \"method\" : \"POST\" , \"headers\" : { \"content-length\" : [ \"18\" ] , \"content-type\" : [ \"application/x-www-form-urlencoded\" ] , \"user-agent\" : [ \"curl/7.65.3\" ] , \"accept\" : [ \"*/*\" ] , \"host\" : [ \"localhost:9139\" ]} , \"url\" : { \"scheme\" : \"http\" , \"host\" : \"localhost\" , \"port\" :9139, \"path\" : \"/echo\" }}}","title":"Using the Configurator"},{"location":"workshop/examples/37_configurator/#todo","text":"aggregate responses from service constituents (from tremor-script for now) explore pull model for configuration sync generate per-service config routes from openapi specs templatize new service addition/boilerplate In the context of configuring tremor nodes, problems around configuration sync/distribution will be easier to solve after tremor becomes truly clustered, but for now, we can try and tackle it with what we have and see how far we go (eg: via guaranteed delivery or periodic updates).","title":"TODO"},{"location":"workshop/examples/38_polling_alerts/","text":"Polling \u00b6 Note All the application code here is available from the docs git repository . This example demonstrates using Tremor to periodically poll a data source (we use influx as it can quickly generate data) and make decisions based on the results - in our case alert us on low CPU or memory. We will not dive deep into the query used or the alerts defined as they're only supporting elements to the story we're trying to tell here: periodic, reactive workflows. To this end, we leverage a good bit of the configuration introduced in the influx example . Environment \u00b6 As mentioned above, we reuse a lot of the influx logic, so we ignore the following artifacts: onramp: udp-input offramp: influx-output query: ingress binding: ingress mapping: ingress Also there are two new pipelines: poll - translates a tick into a query alert - translates the result and evaluates if an alert should be triggered We also have a new onramp ( tick ) and offramp ( influx-query ). For the sake of not repeating the privious workshop we will focus on those new parts exclusively. Business Logic \u00b6 Polling \u00b6 This section deals with polling, in our case we want to query influxdb on a periodic interval. To this end we use a metronome onramp that fires an event every 10s. We send the events into poll.trickle where we create a influx request out of the metronom event. The poll pipeline then connects to the linked influx offramp to run the query. # poll.trickle # This file is for for turning ticks into queries # this turns the `metronom` tick into a query define script query with host = \"\" , db = \"\" script use std :: url ; # we define the query to gather data # this is the original, for the sake of dockerizing it we ignore the host in the final query since we don't know what it will be # let query = \"SELECT mean(\\\"usage_idle\\\") AS \\\"cpu_idle\\\", mean(\\\"active\\\") AS \\\"mem_active\\\" FROM \\\"tremor\\\".\\\"autogen\\\".\\\"cpu\\\", \\\"tremor\\\".\\\"autogen\\\".\\\"mem\\\" WHERE time > now() - 1h AND time < now() AND \\\"host\\\"='#{ args.host }' GROUP BY time(1h) FILL(null)\"; let query = \"SELECT mean( \\\" usage_idle \\\" ) AS \\\" cpu_idle \\\" , mean( \\\" active \\\" ) AS \\\" mem_active \\\" FROM \\\" tremor \\\" . \\\" autogen \\\" . \\\" cpu \\\" , \\\" tremor \\\" . \\\" autogen \\\" . \\\" mem \\\" WHERE time > now() - 1h AND time < now() GROUP BY time(1h) FILL(null)\" ; # we encode this to a rest offramp query parameter using `url::encode` let $ endpoint . query = \"db= #{ args . db } &epock=ms&q= #{ url :: encode ( query ) } \" ; let event . meta = $; # we can end this script event end ; # we create a script for a given host create script query with host = \"d111f17774f7\" end ; # we wire it all up select event from in into query ; select event from query into out ; Alerting \u00b6 The alert.trickle pipeline takes the reply from Influx and alert if the values we see are above a given limit. Since the influx reply uses a unique datamodle, we need to unscramble the results, this sadly is a trail and error process based on what influx returns. Once we have extracted the data we can pass it into an alerting script that checks a few conditions in a given order. The first condition that is met will trigger the coresponding alert. You can adopt the alert conditions in the with section of the script. # This script takes the responses and turns them into alerts # The script that does all the logic, we define our alerts here define script alert with cpu_limit = 100 , mem_limit = 19518531180 script match event of case % {cpu_idle < args . cpu_limit , mem_active > args . mem_limit} => emit \"EVERYTHING IS ON FIRE\" case % {cpu_idle < args . cpu_limit} => match event of case % {cpu_system > 50 } => emit \"OS BROKEN\" default => emit \"CPU BUSY\" end case % {mem_active > args . mem_limit } => emit \"MEM LOW\" default => drop end end ; create script alert ; # Since the influx reply is hard to work with we santize it here so we can write our alerts # in a cleaner fashipn # # example result: # ``` # {\"results\":[{\"statement_id\":0,\"series\":[{\"columns\":[\"time\",\"cpu_idle1\",\"mem_active\"],\"values\":[[\"2021-03-02T15:00:00Z\",98.856058199546,null],[\"2021-03-02T16:00:00Z\",97.09260215835516,null]],\"name\":\"cpu\"},{\"columns\":[\"time\",\"cpu_idle1\",\"mem_active\"],\"values\":[[\"2021-03-02T15:00:00Z\",null,19519109501.023254],[\"2021-03-02T16:00:00Z\",null,19959332287.756653]],\"name\":\"mem\"}]}]} # ``` create stream extracted ; select { \" #{ event . results [ 0 ]. series [ 0 ]. columns [ 1 ] } \" : event . results [ 0 ]. series [ 0 ]. values [ 1 ][ 1 ], \" #{ event . results [ 0 ]. series [ 1 ]. columns [ 2 ] } \" : event . results [ 0 ]. series [ 1 ]. values [ 1 ][ 2 ], } from in into extracted ; # we wire it all up select event from extracted into alert ; select event from alert into out ; # we could use this for debugging # select event from in into out; Command line testing during logic development \u00b6 $ docker-compose up ... lots of logs ... Then watch alerts on stdout from docker-compose .","title":"Polling"},{"location":"workshop/examples/38_polling_alerts/#polling","text":"Note All the application code here is available from the docs git repository . This example demonstrates using Tremor to periodically poll a data source (we use influx as it can quickly generate data) and make decisions based on the results - in our case alert us on low CPU or memory. We will not dive deep into the query used or the alerts defined as they're only supporting elements to the story we're trying to tell here: periodic, reactive workflows. To this end, we leverage a good bit of the configuration introduced in the influx example .","title":"Polling"},{"location":"workshop/examples/38_polling_alerts/#environment","text":"As mentioned above, we reuse a lot of the influx logic, so we ignore the following artifacts: onramp: udp-input offramp: influx-output query: ingress binding: ingress mapping: ingress Also there are two new pipelines: poll - translates a tick into a query alert - translates the result and evaluates if an alert should be triggered We also have a new onramp ( tick ) and offramp ( influx-query ). For the sake of not repeating the privious workshop we will focus on those new parts exclusively.","title":"Environment"},{"location":"workshop/examples/38_polling_alerts/#business-logic","text":"","title":"Business Logic"},{"location":"workshop/examples/38_polling_alerts/#polling_1","text":"This section deals with polling, in our case we want to query influxdb on a periodic interval. To this end we use a metronome onramp that fires an event every 10s. We send the events into poll.trickle where we create a influx request out of the metronom event. The poll pipeline then connects to the linked influx offramp to run the query. # poll.trickle # This file is for for turning ticks into queries # this turns the `metronom` tick into a query define script query with host = \"\" , db = \"\" script use std :: url ; # we define the query to gather data # this is the original, for the sake of dockerizing it we ignore the host in the final query since we don't know what it will be # let query = \"SELECT mean(\\\"usage_idle\\\") AS \\\"cpu_idle\\\", mean(\\\"active\\\") AS \\\"mem_active\\\" FROM \\\"tremor\\\".\\\"autogen\\\".\\\"cpu\\\", \\\"tremor\\\".\\\"autogen\\\".\\\"mem\\\" WHERE time > now() - 1h AND time < now() AND \\\"host\\\"='#{ args.host }' GROUP BY time(1h) FILL(null)\"; let query = \"SELECT mean( \\\" usage_idle \\\" ) AS \\\" cpu_idle \\\" , mean( \\\" active \\\" ) AS \\\" mem_active \\\" FROM \\\" tremor \\\" . \\\" autogen \\\" . \\\" cpu \\\" , \\\" tremor \\\" . \\\" autogen \\\" . \\\" mem \\\" WHERE time > now() - 1h AND time < now() GROUP BY time(1h) FILL(null)\" ; # we encode this to a rest offramp query parameter using `url::encode` let $ endpoint . query = \"db= #{ args . db } &epock=ms&q= #{ url :: encode ( query ) } \" ; let event . meta = $; # we can end this script event end ; # we create a script for a given host create script query with host = \"d111f17774f7\" end ; # we wire it all up select event from in into query ; select event from query into out ;","title":"Polling"},{"location":"workshop/examples/38_polling_alerts/#alerting","text":"The alert.trickle pipeline takes the reply from Influx and alert if the values we see are above a given limit. Since the influx reply uses a unique datamodle, we need to unscramble the results, this sadly is a trail and error process based on what influx returns. Once we have extracted the data we can pass it into an alerting script that checks a few conditions in a given order. The first condition that is met will trigger the coresponding alert. You can adopt the alert conditions in the with section of the script. # This script takes the responses and turns them into alerts # The script that does all the logic, we define our alerts here define script alert with cpu_limit = 100 , mem_limit = 19518531180 script match event of case % {cpu_idle < args . cpu_limit , mem_active > args . mem_limit} => emit \"EVERYTHING IS ON FIRE\" case % {cpu_idle < args . cpu_limit} => match event of case % {cpu_system > 50 } => emit \"OS BROKEN\" default => emit \"CPU BUSY\" end case % {mem_active > args . mem_limit } => emit \"MEM LOW\" default => drop end end ; create script alert ; # Since the influx reply is hard to work with we santize it here so we can write our alerts # in a cleaner fashipn # # example result: # ``` # {\"results\":[{\"statement_id\":0,\"series\":[{\"columns\":[\"time\",\"cpu_idle1\",\"mem_active\"],\"values\":[[\"2021-03-02T15:00:00Z\",98.856058199546,null],[\"2021-03-02T16:00:00Z\",97.09260215835516,null]],\"name\":\"cpu\"},{\"columns\":[\"time\",\"cpu_idle1\",\"mem_active\"],\"values\":[[\"2021-03-02T15:00:00Z\",null,19519109501.023254],[\"2021-03-02T16:00:00Z\",null,19959332287.756653]],\"name\":\"mem\"}]}]} # ``` create stream extracted ; select { \" #{ event . results [ 0 ]. series [ 0 ]. columns [ 1 ] } \" : event . results [ 0 ]. series [ 0 ]. values [ 1 ][ 1 ], \" #{ event . results [ 0 ]. series [ 1 ]. columns [ 2 ] } \" : event . results [ 0 ]. series [ 1 ]. values [ 1 ][ 2 ], } from in into extracted ; # we wire it all up select event from extracted into alert ; select event from alert into out ; # we could use this for debugging # select event from in into out;","title":"Alerting"},{"location":"workshop/examples/38_polling_alerts/#command-line-testing-during-logic-development","text":"$ docker-compose up ... lots of logs ... Then watch alerts on stdout from docker-compose .","title":"Command line testing during logic development"},{"location":"workshop/examples/40_otel_passthrough/","text":"CNCF OpenTelemetry Passthrough \u00b6 This example is the simplest possible configuration of tremor with support for CNCF OpenTelemetry interception and distribution. It shows the very basic building blocks: * CNCF OpenTelemetry Onramp * CNCF OpenTelemetry Offramp * Deployment configuration file External open telemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector. Environment \u00b6 The onramp we use is the otel CNCF OpenTeletry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. select event from in into out ; We connect the passthrough output events into an OpenTelemetry sink which distributes them to a downstream OpenTelemetry service. offramp : - id : otlp type : otel codec : json # Jsn is the only supported value config : host : \"0.0.0.0\" port : 4317 The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' - '/offramp/otlp/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\" Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 . Docker \u00b6 For convenience, use the provided docker-compose.yaml to start and stop tremor and the opentelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose stop","title":"OpenTelemetry Passthrough"},{"location":"workshop/examples/40_otel_passthrough/#cncf-opentelemetry-passthrough","text":"This example is the simplest possible configuration of tremor with support for CNCF OpenTelemetry interception and distribution. It shows the very basic building blocks: * CNCF OpenTelemetry Onramp * CNCF OpenTelemetry Offramp * Deployment configuration file External open telemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector.","title":"CNCF OpenTelemetry Passthrough"},{"location":"workshop/examples/40_otel_passthrough/#environment","text":"The onramp we use is the otel CNCF OpenTeletry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. select event from in into out ; We connect the passthrough output events into an OpenTelemetry sink which distributes them to a downstream OpenTelemetry service. offramp : - id : otlp type : otel codec : json # Jsn is the only supported value config : host : \"0.0.0.0\" port : 4317 The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' - '/offramp/otlp/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\"","title":"Environment"},{"location":"workshop/examples/40_otel_passthrough/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/40_otel_passthrough/#command-line-testing-during-logic-development","text":"Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 .","title":"Command line testing during logic development"},{"location":"workshop/examples/40_otel_passthrough/#docker","text":"For convenience, use the provided docker-compose.yaml to start and stop tremor and the opentelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose stop","title":"Docker"},{"location":"workshop/examples/41_otel_zipkin/","text":"CNCF OpenTelemetry Zipkin Interorking \u00b6 This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Zipkin as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Zipkin can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Zipkin service CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External open telemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector. Environment \u00b6 The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. select event from in into out ; We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\" Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 . Docker \u00b6 For convenience, use the provided docker-compose.yaml to start and stop tremor and the opentelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down Zipkin client \u00b6 We use an existing Zipkin client for demonstration purposes. Fetch the standard zipkin php client as follows: # Clone the git repo $ git clone https://github.com/openzipkin/zipkin-php-example # Cd into the repo root $ cd zipkin-php-example # Install dependent php libraries $ composer install And, assuming you have PHP composer, run the front and backend in two separate terminal windows: # Spin up the PHP backend on `locahost:9000` composer -run run-frontend # Spin up the PHP frontend on `localhost:8081` composer -run run-backend Hit the frontend via curl ( in another terminal ) # Generate trace spand via curl curl -o - http://locahost:8081/ Verify that our frontend has issued some spans in its terminal output # Output from our frontend composer terminal > php -S 'localhost:8081' frontend.php [ Tue Apr 6 18 :53:56 2021 ] PHP 7 .4.10 Development Server ( http://localhost:8081 ) started [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50812 Accepted [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50812 Closing Verify that our PHP backend has issued some spans in its terminal output # Output from our backend composer terminal > php -S 'localhost:9000' backend.php [ Tue Apr 6 18 :50:34 2021 ] PHP 7 .4.10 Development Server ( http://localhost:9000 ) started [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50813 Accepted [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50813 Closing Verify our spans reached the Zipkin UI deployed in docker via pointing our browser to http://localhost:9412 by searching for traces: Note that we expose the Zipkin UI on a non-standard port via docker so that our Zipkin traffic actually gets routed via the opentelemetry collector to tremor and to the Zipkin service and ui. In this way the opentelemetry collector and tremor . From the perspective of the Zipkin PHP client - this is a plain vanilla Zipkin service. In practice, this is the opentelemetry-collector which is forwarding to both tremor and to the zipkin-ui in this demo. Advanced \u00b6 Rather than run multiple sidecars, tremor could be configured to transform Zipkin traffic directly to the OpenTelemetry format. Given transformation logic as follows ### Transform zipkin b3 ( http/json ) to otel use cncf :: otel ; use tremor :: system ; use std :: record ; fn transform_span ( span ) with # A simple transient event counter let count = match state of case null => let state = 0 default => let state = state + 1 end ; match span of case % { present id , # span id # present parentId, # span parent id present traceId , # trace id present annotations , present name , # name #present kind, # CLIENT # present remoteEndpoint, present timestamp , present tags , present duration , present localEndpoint , } => { \"resource\" : { \"attributes\" : merge span . tags of { \"tremor.ingest_ns\" : system :: ingest_ns () } end , \"dropped_attributes_count\" : 0 , } , \"instrumentation_library_spans\" : [ { \"instrumentation_library\" : { \"name\" : \"tremor\" , \"version\" : system :: version (), } , \"spans\" : [ { \"start_time_unix_nano\" : ( span . timestamp * 1000 ), \"end_time_unix_nano\" : ( span . timestamp * 1000 ) + ( span . duration * 1000 ), \"name\" : \" #{span . name} - #{count} \" , \"attributes\" : record :: from_array ( for span . annotations of case ( i , e ) => [ \"zipkin.annotation. #{e . value} \" , e . timestamp * 1000 ] # convert ts micros -> nanos end ), \"dropped_attributes_count\" : 0 , \"kind\" : match span of case % { present kind } => match span . kind of case \"CLIENT\" => otel :: trace :: spankind :: client case \"SERVER\" => otel :: trace :: spankind :: server case \"PRODUCER\" => otel :: trace :: spankind :: server case \"CONSUMER\" => otel :: trace :: spankind :: server default => otel :: trace :: spankind :: client end default => otel :: trace :: spankind :: client end , \"trace_state\" : \"\" , \"parent_span_id\" : match span of case % { present parentId , } => span . parentId default => \"\" # no parent span end , \"span_id\" : span . id , \"trace_id\" : span . traceId , \"status\" : otel :: trace :: status :: ok (), \"events\" : [], \"links\" : [], \"dropped_events_count\" : 0 , \"dropped_links_count\" : 0 , } ] } ] } default => { \"drop\" : span } end end ; And a simple tremor query as follows: # # Process zipkin b3 [span] to [otel resource span] # define script to_zipkin script use zipkin_to_otel ; for event . trace of case ( i , span ) => merge zipkin_to_otel :: transform_span ( span ) of { \"resource\" : { \"attributes\" : { \"http.url.path\" : event . request . url . path , \"http.url.host\" : event . request . url . host , \"http.url.port\" : event . request . url . port , \"http.url.scheme\" : event . request . url . scheme , \"http.headers.user-agent\" : event . request . headers . user - agent [ 0 ], \"http.headers.b3\" : event . request . headers . b3 [ 0 ], \"http.method\" : event . request . method , } } } end end end ; create script to_zipkin ; # Push zipkin-b3/http [trace] into transformer capturing http request metadata select { \"request\" : $ request , \"trace\" : event } from in into to_zipkin ; # Wrap resource spans as a trace event compatible with tremor otel sink select { \"trace\" : event } from to_zipkin into out ; Removing the zipkin-all-in-one container from this walkthrough's docker-compose.yaml and removing the OpenTelemetry collector configuration and container should be sufficient to produce a basic working environment based solely on tremor, and the Zipkin PHP clients with minor adjustments to the script and query files above. However, the CNCF OpenTelemetry Collector has excellent support for legacy observability frameworks and formats. Tremor does not. The Zipkin UI will be familiar to users who have experience of observability through the Zipkin project. Tremor does not have a UI at all. We provide the example to illustrate a more complete example of how tremor is typically configured in production environments and to illustrate how existing trace and span information can relatively easily be adapted to CNCF OpenTelemetry using tremor's scripting and query language support.","title":"Zipkin Interworking"},{"location":"workshop/examples/41_otel_zipkin/#cncf-opentelemetry-zipkin-interorking","text":"This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Zipkin as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Zipkin can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Zipkin service CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External open telemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector.","title":"CNCF OpenTelemetry Zipkin Interorking"},{"location":"workshop/examples/41_otel_zipkin/#environment","text":"The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. select event from in into out ; We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\"","title":"Environment"},{"location":"workshop/examples/41_otel_zipkin/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/41_otel_zipkin/#command-line-testing-during-logic-development","text":"Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 .","title":"Command line testing during logic development"},{"location":"workshop/examples/41_otel_zipkin/#docker","text":"For convenience, use the provided docker-compose.yaml to start and stop tremor and the opentelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down","title":"Docker"},{"location":"workshop/examples/41_otel_zipkin/#zipkin-client","text":"We use an existing Zipkin client for demonstration purposes. Fetch the standard zipkin php client as follows: # Clone the git repo $ git clone https://github.com/openzipkin/zipkin-php-example # Cd into the repo root $ cd zipkin-php-example # Install dependent php libraries $ composer install And, assuming you have PHP composer, run the front and backend in two separate terminal windows: # Spin up the PHP backend on `locahost:9000` composer -run run-frontend # Spin up the PHP frontend on `localhost:8081` composer -run run-backend Hit the frontend via curl ( in another terminal ) # Generate trace spand via curl curl -o - http://locahost:8081/ Verify that our frontend has issued some spans in its terminal output # Output from our frontend composer terminal > php -S 'localhost:8081' frontend.php [ Tue Apr 6 18 :53:56 2021 ] PHP 7 .4.10 Development Server ( http://localhost:8081 ) started [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50812 Accepted [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50812 Closing Verify that our PHP backend has issued some spans in its terminal output # Output from our backend composer terminal > php -S 'localhost:9000' backend.php [ Tue Apr 6 18 :50:34 2021 ] PHP 7 .4.10 Development Server ( http://localhost:9000 ) started [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50813 Accepted [ Tue Apr 6 18 :54:03 2021 ] [ ::1 ] :50813 Closing Verify our spans reached the Zipkin UI deployed in docker via pointing our browser to http://localhost:9412 by searching for traces: Note that we expose the Zipkin UI on a non-standard port via docker so that our Zipkin traffic actually gets routed via the opentelemetry collector to tremor and to the Zipkin service and ui. In this way the opentelemetry collector and tremor . From the perspective of the Zipkin PHP client - this is a plain vanilla Zipkin service. In practice, this is the opentelemetry-collector which is forwarding to both tremor and to the zipkin-ui in this demo.","title":"Zipkin client"},{"location":"workshop/examples/41_otel_zipkin/#advanced","text":"Rather than run multiple sidecars, tremor could be configured to transform Zipkin traffic directly to the OpenTelemetry format. Given transformation logic as follows ### Transform zipkin b3 ( http/json ) to otel use cncf :: otel ; use tremor :: system ; use std :: record ; fn transform_span ( span ) with # A simple transient event counter let count = match state of case null => let state = 0 default => let state = state + 1 end ; match span of case % { present id , # span id # present parentId, # span parent id present traceId , # trace id present annotations , present name , # name #present kind, # CLIENT # present remoteEndpoint, present timestamp , present tags , present duration , present localEndpoint , } => { \"resource\" : { \"attributes\" : merge span . tags of { \"tremor.ingest_ns\" : system :: ingest_ns () } end , \"dropped_attributes_count\" : 0 , } , \"instrumentation_library_spans\" : [ { \"instrumentation_library\" : { \"name\" : \"tremor\" , \"version\" : system :: version (), } , \"spans\" : [ { \"start_time_unix_nano\" : ( span . timestamp * 1000 ), \"end_time_unix_nano\" : ( span . timestamp * 1000 ) + ( span . duration * 1000 ), \"name\" : \" #{span . name} - #{count} \" , \"attributes\" : record :: from_array ( for span . annotations of case ( i , e ) => [ \"zipkin.annotation. #{e . value} \" , e . timestamp * 1000 ] # convert ts micros -> nanos end ), \"dropped_attributes_count\" : 0 , \"kind\" : match span of case % { present kind } => match span . kind of case \"CLIENT\" => otel :: trace :: spankind :: client case \"SERVER\" => otel :: trace :: spankind :: server case \"PRODUCER\" => otel :: trace :: spankind :: server case \"CONSUMER\" => otel :: trace :: spankind :: server default => otel :: trace :: spankind :: client end default => otel :: trace :: spankind :: client end , \"trace_state\" : \"\" , \"parent_span_id\" : match span of case % { present parentId , } => span . parentId default => \"\" # no parent span end , \"span_id\" : span . id , \"trace_id\" : span . traceId , \"status\" : otel :: trace :: status :: ok (), \"events\" : [], \"links\" : [], \"dropped_events_count\" : 0 , \"dropped_links_count\" : 0 , } ] } ] } default => { \"drop\" : span } end end ; And a simple tremor query as follows: # # Process zipkin b3 [span] to [otel resource span] # define script to_zipkin script use zipkin_to_otel ; for event . trace of case ( i , span ) => merge zipkin_to_otel :: transform_span ( span ) of { \"resource\" : { \"attributes\" : { \"http.url.path\" : event . request . url . path , \"http.url.host\" : event . request . url . host , \"http.url.port\" : event . request . url . port , \"http.url.scheme\" : event . request . url . scheme , \"http.headers.user-agent\" : event . request . headers . user - agent [ 0 ], \"http.headers.b3\" : event . request . headers . b3 [ 0 ], \"http.method\" : event . request . method , } } } end end end ; create script to_zipkin ; # Push zipkin-b3/http [trace] into transformer capturing http request metadata select { \"request\" : $ request , \"trace\" : event } from in into to_zipkin ; # Wrap resource spans as a trace event compatible with tremor otel sink select { \"trace\" : event } from to_zipkin into out ; Removing the zipkin-all-in-one container from this walkthrough's docker-compose.yaml and removing the OpenTelemetry collector configuration and container should be sufficient to produce a basic working environment based solely on tremor, and the Zipkin PHP clients with minor adjustments to the script and query files above. However, the CNCF OpenTelemetry Collector has excellent support for legacy observability frameworks and formats. Tremor does not. The Zipkin UI will be familiar to users who have experience of observability through the Zipkin project. Tremor does not have a UI at all. We provide the example to illustrate a more complete example of how tremor is typically configured in production environments and to illustrate how existing trace and span information can relatively easily be adapted to CNCF OpenTelemetry using tremor's scripting and query language support.","title":"Advanced"},{"location":"workshop/examples/42_otel_jaeger/","text":"CNCF OpenTelemetry Jaeger Interworking \u00b6 !! note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Jaeger as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Jaeger can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Jaeger service CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector. Environment \u00b6 The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\" Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 . Docker \u00b6 For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down Jaeger client \u00b6 We use an existing Jaeger client for demonstration purposes. Fetch the standard C++ Jaeger client as follows: # Clone the git repo $ git clone https://github.com/jaegertracing/jaeger-client-cpp # Change directory into the repo root $ cd jaeger-client-cpp # Build ( make a cup of tea, this takes a while! ) $ mkdir build $ cd build $ cmake .. $ make Configure the example jaeger application to point at our dockerized service disabled : false reporter : logSpans : true endpoint : http://localhost:14268/api/traces sampler : type : const param : 1 Run the jeager client to generate trace spans # From the `build` directory $ ./app ../examples/config.yml INFO: Initializing logging reporter INFO: Reporting span f05fbb51006ac531:f062508e8013067f:f05fbb51006ac531:1 INFO: Reporting span f05fbb51006ac531:f05fbb51006ac531:0000000000000000:1 Verify our spans reached the Jaeger UI deployed in docker via pointing our browser to http://localhost:16686 by searching for traces: Verify that the OpenTelemetry Collector and tremor have processed our trace spans. In this configuration we use the OpenTelemetry Collector to forward to Jaeger and to forward to tremor.","title":"Jaeger Interworking"},{"location":"workshop/examples/42_otel_jaeger/#cncf-opentelemetry-jaeger-interworking","text":"!! note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Jaeger as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Jaeger can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Jaeger service CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector.","title":"CNCF OpenTelemetry Jaeger Interworking"},{"location":"workshop/examples/42_otel_jaeger/#environment","text":"The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\"","title":"Environment"},{"location":"workshop/examples/42_otel_jaeger/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/42_otel_jaeger/#command-line-testing-during-logic-development","text":"Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 .","title":"Command line testing during logic development"},{"location":"workshop/examples/42_otel_jaeger/#docker","text":"For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down","title":"Docker"},{"location":"workshop/examples/42_otel_jaeger/#jaeger-client","text":"We use an existing Jaeger client for demonstration purposes. Fetch the standard C++ Jaeger client as follows: # Clone the git repo $ git clone https://github.com/jaegertracing/jaeger-client-cpp # Change directory into the repo root $ cd jaeger-client-cpp # Build ( make a cup of tea, this takes a while! ) $ mkdir build $ cd build $ cmake .. $ make Configure the example jaeger application to point at our dockerized service disabled : false reporter : logSpans : true endpoint : http://localhost:14268/api/traces sampler : type : const param : 1 Run the jeager client to generate trace spans # From the `build` directory $ ./app ../examples/config.yml INFO: Initializing logging reporter INFO: Reporting span f05fbb51006ac531:f062508e8013067f:f05fbb51006ac531:1 INFO: Reporting span f05fbb51006ac531:f05fbb51006ac531:0000000000000000:1 Verify our spans reached the Jaeger UI deployed in docker via pointing our browser to http://localhost:16686 by searching for traces: Verify that the OpenTelemetry Collector and tremor have processed our trace spans. In this configuration we use the OpenTelemetry Collector to forward to Jaeger and to forward to tremor.","title":"Jaeger client"},{"location":"workshop/examples/43_otel_prometheus/","text":"CNCF OpenTelemetry Prometheus Interworking \u00b6 !! note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Prometheus as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Prometheus can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Prometheus service and the Prometheus Push Gateway CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector. Environment \u00b6 The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\" Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 . Docker \u00b6 For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down Prometheus client \u00b6 We provide a simple prometheus client implemented in rust use device_query :: { DeviceState , Keycode }; use prometheus :: { labels , register_counter }; use std :: { error :: Error , thread , time }; fn main () -> Result < (), Box < dyn Error >> { let ten_ms = time :: Duration :: from_millis ( 100 ); let johnny_five = DeviceState :: new (); let counter = register_counter ! ( \"iterations\" , \"Number of badgers in snot green situations\" ) ? ; let mut done = false ; thread :: sleep ( time :: Duration :: from_secs ( 1 )); // Delay at start in case user still has keys pressed println! ( \"Press any key to stop ...\" ); ' main : loop { if done { println! ( \"Done\" ); break ; } // Terminate if any input on stdin let keymap = johnny_five . query_keymap (); for keycode in keymap { match keycode { Keycode :: Right | Keycode :: Left | Keycode :: Up | Keycode :: Down => (), _any_other_key => { done = true ; continue 'main ; } } } let metric_families = prometheus :: gather (); println! ( \"Sending metrics: {}\" , counter . get ()); prometheus :: push_metrics ( \"example_push\" , labels ! { \"instance\" . to_owned () => \"HAL-9000\" . to_owned (),}, \"0.0.0.0:9091\" , // This refers to our prometheus push gateway in the docker-compose metric_families , None , // No authentication ) ? ; counter . inc (); thread :: sleep ( ten_ms ); } Ok (()) } Build and run the rust prometheus client: $ cd prometheus-rust-client $ cargo build $ cargo run Hit any non-cursor key to stop the client from injecting spans ( 10 per second ). Verify our metrics reached the Prometheus UI deployed in docker via pointing our browser to http://localhost:9090 by searching for metrics: Verify that the OpenTelemetry Collector and tremor have processed our trace spans. In this configuration we use the OpenTelemetry Collector to forward to Prometheus and to forward to tremor.","title":"Prometheus Interworking"},{"location":"workshop/examples/43_otel_prometheus/#cncf-opentelemetry-prometheus-interworking","text":"!! note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Prometheus as a receiver and exporter in the OpenTelemetry Collector. It shows how legacy observability frameworks such as Prometheus can be ingested into OpenTelemetry based services directly, or via the OpenTelemetry collector into tremor for specialized processing. Prometheus service and the Prometheus Push Gateway CNCF OpenTelemetry Collector service CNCF OpenTelemetry Onramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector.","title":"CNCF OpenTelemetry Prometheus Interworking"},{"location":"workshop/examples/43_otel_prometheus/#environment","text":"The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\"","title":"Environment"},{"location":"workshop/examples/43_otel_prometheus/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/43_otel_prometheus/#command-line-testing-during-logic-development","text":"Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 .","title":"Command line testing during logic development"},{"location":"workshop/examples/43_otel_prometheus/#docker","text":"For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down","title":"Docker"},{"location":"workshop/examples/43_otel_prometheus/#prometheus-client","text":"We provide a simple prometheus client implemented in rust use device_query :: { DeviceState , Keycode }; use prometheus :: { labels , register_counter }; use std :: { error :: Error , thread , time }; fn main () -> Result < (), Box < dyn Error >> { let ten_ms = time :: Duration :: from_millis ( 100 ); let johnny_five = DeviceState :: new (); let counter = register_counter ! ( \"iterations\" , \"Number of badgers in snot green situations\" ) ? ; let mut done = false ; thread :: sleep ( time :: Duration :: from_secs ( 1 )); // Delay at start in case user still has keys pressed println! ( \"Press any key to stop ...\" ); ' main : loop { if done { println! ( \"Done\" ); break ; } // Terminate if any input on stdin let keymap = johnny_five . query_keymap (); for keycode in keymap { match keycode { Keycode :: Right | Keycode :: Left | Keycode :: Up | Keycode :: Down => (), _any_other_key => { done = true ; continue 'main ; } } } let metric_families = prometheus :: gather (); println! ( \"Sending metrics: {}\" , counter . get ()); prometheus :: push_metrics ( \"example_push\" , labels ! { \"instance\" . to_owned () => \"HAL-9000\" . to_owned (),}, \"0.0.0.0:9091\" , // This refers to our prometheus push gateway in the docker-compose metric_families , None , // No authentication ) ? ; counter . inc (); thread :: sleep ( ten_ms ); } Ok (()) } Build and run the rust prometheus client: $ cd prometheus-rust-client $ cargo build $ cargo run Hit any non-cursor key to stop the client from injecting spans ( 10 per second ). Verify our metrics reached the Prometheus UI deployed in docker via pointing our browser to http://localhost:9090 by searching for metrics: Verify that the OpenTelemetry Collector and tremor have processed our trace spans. In this configuration we use the OpenTelemetry Collector to forward to Prometheus and to forward to tremor.","title":"Prometheus client"},{"location":"workshop/examples/44_otel_elastic_apm/","text":"CNCF OpenTelemetry Elastic APM Interworking \u00b6 Note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Elastic APM as an exporter in the OpenTelemetry Collector. It shows how Elastic APM can be used with OpenTelemetry based services directly, or via tremor for specialized processing. The Elastic Suite including search, Kibana and the APM server CNCF OpenTelemetry Onramp and Offramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector downstream - which is an instance of the Elastic APM Server in this case. Environment \u00b6 The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) offramp : - id : otlp type : otel # Use the OpenTelemetry gRPC client codec : json # Json is the only supported value config : port : 8200 # The TCP port to distributed to host : \"apm-server\" # The Elastic APM server we're distributing to It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' - '/offramp/otlp/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\" Business Logic \u00b6 select event from in into out Command line testing during logic development \u00b6 Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 . Docker \u00b6 For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down Post-installation steps \u00b6 Once the environment is running we can log into kibana and select the APM service from the menu system. Using any OpenTelemetry client, send metrics or traces ( not logs as Elastic APM does not support log shipping yet ) to tremor on its OpenTelemetry port. We should see them log to console. Console output from docker: We should also be able to see metrics and spans in the APM service section of elastic. Advanced \u00b6 It should be relatively simple to configure the ElasticSearch offramp in tremor for log shipping to elastic whilst exposing a pure OpenTelemetry interface to the outside world. This would allow upstream clients to use the OpenTelemetry protocol exclusively. A simple tremor algorithm can batch and convert OpenTelemetry log formatted messages to the form that the ElasticSearch APIs prefer for bulk log shipping.","title":"Elastic APM Interworking"},{"location":"workshop/examples/44_otel_elastic_apm/#cncf-opentelemetry-elastic-apm-interworking","text":"Note All the application code here is available from the docs git repository . This example builds on the simple passthrough CNCF OpenTelemetry configuration but configures Elastic APM as an exporter in the OpenTelemetry Collector. It shows how Elastic APM can be used with OpenTelemetry based services directly, or via tremor for specialized processing. The Elastic Suite including search, Kibana and the APM server CNCF OpenTelemetry Onramp and Offramp deployed into tremor Deployment configuration file External OpenTelemetry clients can use port 4316 to send OpenTelemetry logs, traces and metrics through tremor. Tremor prints the json mapping to standard out and forwards the events to the OpenTelemetry collector downstream - which is an instance of the Elastic APM Server in this case.","title":"CNCF OpenTelemetry Elastic APM Interworking"},{"location":"workshop/examples/44_otel_elastic_apm/#environment","text":"The onramp we use is the otel CNCF OpenTelemetry onramp listening on a non-standard CNCF OpenTelemetry port 4316 , it receives protocol buffer messages over gRPC on this port. The log, metric and trace events received are converted to tremor's value system and passed through a passthrough pipeline to the CNCF OpenTelemetry sink. The sink will try to connect to a downstream CNCF OpenTelemetry endpoint. In this workshop we will use the well known OpenTelemetry port of 4317 for our sink and run the standard OpenTelemetry collector on this port using its a simple collector configuration . onramp : - id : otlp type : otel # Use the OpenTelemetry gRPC listener source codec : json # Json is the only supported value config : port : 4316 # The TCP port to listen on host : \"0.0.0.0\" # The IP address to bind on ( all interfaces in this case ) offramp : - id : otlp type : otel # Use the OpenTelemetry gRPC client codec : json # Json is the only supported value config : port : 8200 # The TCP port to distributed to host : \"apm-server\" # The Elastic APM server we're distributing to It connects to a simple passthrough pipeline. This pipeline forwards any received observability events downstream unchanged. We connect the passthrough output events into a standard output sink. The binding expresses these relations and gives deployment connectivity graph. binding : - id : example links : '/onramp/otlp/{instance}/out' : - '/pipeline/example/{instance}/in' '/pipeline/example/{instance}/out' : - '/offramp/stdout/{instance}/in' - '/offramp/otlp/{instance}/in' Finally the mapping instanciates the binding with the given name and instance variable to activate the elements of the binding. mapping : /binding/example/passthrough : instance : \"passthrough\"","title":"Environment"},{"location":"workshop/examples/44_otel_elastic_apm/#business-logic","text":"select event from in into out","title":"Business Logic"},{"location":"workshop/examples/44_otel_elastic_apm/#command-line-testing-during-logic-development","text":"Use any compliant OpenTelemetry instrumented application and configure the server to our source on port 4316 instead of the default 4317 .","title":"Command line testing during logic development"},{"location":"workshop/examples/44_otel_elastic_apm/#docker","text":"For convenience, use the provided docker-compose.yaml to start and stop tremor and the OpenTelemetry collector as follows: # Start $ docker-compose up # Stop $ docker-compose down","title":"Docker"},{"location":"workshop/examples/44_otel_elastic_apm/#post-installation-steps","text":"Once the environment is running we can log into kibana and select the APM service from the menu system. Using any OpenTelemetry client, send metrics or traces ( not logs as Elastic APM does not support log shipping yet ) to tremor on its OpenTelemetry port. We should see them log to console. Console output from docker: We should also be able to see metrics and spans in the APM service section of elastic.","title":"Post-installation steps"},{"location":"workshop/examples/44_otel_elastic_apm/#advanced","text":"It should be relatively simple to configure the ElasticSearch offramp in tremor for log shipping to elastic whilst exposing a pure OpenTelemetry interface to the outside world. This would allow upstream clients to use the OpenTelemetry protocol exclusively. A simple tremor algorithm can batch and convert OpenTelemetry log formatted messages to the form that the ElasticSearch APIs prefer for bulk log shipping.","title":"Advanced"}]}